{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>Welcome to my personal knowledge base \u2014 a curated collection of notes, explanations, and resources I\u2019ve written while learning and revising various technical topics.</p> <p>This site is built with MkDocs Material and serves as both a reference and a learning log \u2014 a place where I organize thoughts, code snippets, and insights gathered over time.</p> <p>To run this site locally: </p><pre><code>pip install -r requirements.txt\nmkdocs serve\n</code></pre><p></p>"},{"location":"Courses/","title":"Courses","text":""},{"location":"Courses/#courses","title":"Courses","text":"Title Description References Tags Fundamentals of Database Engineering Learn about ACID properties, Database Indexes, Database Engines, Database Internals and more.  notes course Fundamentals of Operating System Learn how OS Kernel works under the hood to have a better understanding on management of different hardware done on our behalf. notes course Fundamentals of Computer Networking Learn about different computer network protocol which makes it possible to communicate over internet. notes course"},{"location":"Courses/focn/","title":"Fundamentals of Computer Networking","text":""},{"location":"Courses/focn/#fundamentals-of-computer-networking","title":"Fundamentals of Computer Networking","text":"<ul> <li> Fundamentals of Networking</li> <li> Internet Protocol</li> <li> User Datagram Protocol</li> <li> Transmission Control Protocol</li> <li> Other Popular Protocols</li> <li> Network Performance</li> <li> Network Routing</li> <li> Extra</li> </ul>"},{"location":"Courses/focn/basic/","title":"Basics of Networking","text":""},{"location":"Courses/focn/basic/#basics-of-networking","title":"Basics of Networking","text":"Brief History of Internet <p>Early computers were huge, expensive machines so much that an entire labs would only have single machine to serve their needs. The key requirement was to allow multiple people to access this system at same time, which lead to development of ideas to share processing power, storage, printers and terminals across connection. This lead to development of projects like <code>ARPANET</code> which was the early ancestor of the internet. At this point, multiple computers within a network could communicate with each other. But as more networks appeared, each used their own implementation for communication which proved as a hurdle to communicate across different networks. This required standardization in communication  protocol which could be implemented independent of underlying hardware. This solution was provided by TCP/IP protocol which did two critical jobs:</p> <ul> <li>IP moves packets across different networks</li> <li>TCP ensures those packets arrive reliably and in the correct order</li> </ul> <p>This marked as the starting point for development of internet which further evolved using key technologies like  DNS, WWW, Browsers, etc.  </p> <p>Due to lack of standardized way of communication, computer vendors invented their own standards which caused huge problems when integrating different systems . OSI Model was an attempt to provide common standards which could be implemented by any hardware regardless of its vendor. Its idea was to break down networking in different layers,  where each layer handled specific tasks. But by the time OSI protocols were ready, TCP/IP was already working in real networks and had huge adoption. Due to this reason OSI protocols never took off, however its conceptual model survived because it explains networking much clearly.</p>"},{"location":"Courses/focn/basic/#osi-model","title":"OSI Model","text":"<p>OSI (Open System Interconnection) model uses 7 layers of abstraction where each layer represents a networking component.</p> <ul> <li>Layer 7/Application Layer, handles protocol like application protocols like HTTP, FTP, gRPC. Usually these     protocols are mostly implemented using standard libraries of respective programming languages, so that you won't    have to go into implementation specific. </li> <li>Layer 6/Presentation Layer, handles serialization and deserialization of programming objects into byte strings     as per the encoding provided. This may include encrypting/decrypting (SSL/TLS), formatting or encoding/decoding data. </li> <li>Layer 5/Session Layer, handles sessions/connections b/w application. Usually involves around opening/closing     connections, keeping sessions active and synchronizing data. </li> <li>Layer 4/Transport Layer, handles end-to-end communication b/w two systems using protocols like TCP/UDP.     It involves segmenting data into chunks (packets/datagrams), assigning them ports to allow sending/reading them by    other processes (on same/different computer across network). </li> <li>Layer 3/Network Layer, handles routing packets across networks to destined party using IP address. </li> <li>Layer 2/Data Link, creates a reliable link between two devices on the same network using MAC address. </li> <li> <p>Layer 1/Physical, moves raw bits (0s and 1s) across wired/wireless signals by translating/assembling digital     signal into/from analog signals(like (1)) .</p> <ol> <li>like electric, light or radio waves.</li> </ol> </li> </ul> <p>For better understanding, follow below sequence diagrams for journey of POST request from both sender and receiver perspective. </p> <pre><code>sequenceDiagram\n    participant L7 as Layer 7&lt;br/&gt;(Application)\n    participant L6 as Layer 6&lt;br/&gt;(Presentation)\n    participant L5 as Layer 5&lt;br/&gt;(Session)\n    participant L4 as Layer 4&lt;br/&gt;(Transport)\n    participant L3 as Layer 3&lt;br/&gt;(Network)\n    participant L2 as Layer 2&lt;br/&gt;(Data Link)\n    participant L1 as Layer 1&lt;br/&gt;(Physical)\n\n    %%Sender\n    Note over L7,L6: From Senders point of view\n    L7-&gt;&gt;L6: Send POST request&lt;br/&gt; with JSON payload\n    L6-&gt;&gt;L5: Serialize JSON &lt;br/&gt;into byte strings\n    L5-&gt;&gt;L4: Establish/Identify&lt;br/&gt; connection\n    L4-&gt;&gt;L3: Segment bytes into segments&lt;br/&gt;(add source &amp; destination ports)\n    L3-&gt;&gt;L2: Encapsulate segments into IP packets&lt;br/&gt;(add source &amp; destination IPs)\n    L2-&gt;&gt;L1: Encapsulate packet into frames&lt;br/&gt;(add source &amp; destination MACs)\n    Note over L1: -&gt; Convert frames into analog&lt;br/&gt; signal and send over network\n\n    %%Receiver\n    Note over L1: &lt;- Convert analog signals into digital bits\n    L1-&gt;&gt;L2: Assemble bits into frames\n    L2-&gt;&gt;L3: Extract IP packets from frames\n    L3-&gt;&gt;L4: Reassemble IP packets into TCP segments\n    L4-&gt;&gt;L5: Establish/identify&lt;br/&gt; connection\n    L5-&gt;&gt;L6: Deserialize bytes into&lt;br/&gt; JSON data\n    L6-&gt;&gt;L7: Trigger \u201crequest received\u201d &lt;br/&gt;event for app\n    Note over L7, L6: From Receivers point of view\n\n</code></pre>  Use mouse to pan and zoom  <p></p>"},{"location":"Courses/focn/basic/#basic-network-routing","title":"Basic network routing","text":"<p>When your data is transmitted over network, it goes through different devices which helps to rout it towards the fastest and appropriate direction. This redirection requires peeking information within our request like IP addresses, MAC addresses.</p> <ul> <li>The request is first send to Switch which allows you to communicate within your LAN. For this it needs to look up   the MAC address of data (L2), to move data b/w right device on LAN. However, switch doesn't have any information about   external networks.</li> <li>To communicate with external networks, you've to use a Router (also known as default gateway) which has routing   information required to send the data towards appropriate network. This requires looking up the destination IP    address (L3). </li> <li>From there, data travel through ISP's access network which aggregate multiple data frames (L2) before moving them to   their routers where they can be forwarded across cities, countries or continents. Across these WAN, data just hops   from one router to another. </li> <li>Finally, when data reaches the destination router, it moves the data to LAN switch which looks up the MAC address   and forwards the request to correct host.</li> </ul> <p>This is a brief overview of how multiple networking components interact to allow host to host communication over world. In following chapters, we'll dive deep into each of these components and understand their HOWS and WHYS.</p>"},{"location":"Courses/focn/ip/","title":"Internet Protocol","text":""},{"location":"Courses/focn/ip/#internet-protocol","title":"Internet Protocol","text":"<p>IP is a L3 (network) protocol whose main responsibility is to help data move towards its destination host through a graph of hosts (or network). To understand IP, we need to understand IP Address which is used to uniquely  identify a host while also providing efficient routing. </p>"},{"location":"Courses/focn/ip/#ip-address","title":"IP Address","text":"<p>IP Address helps solve IDing hosts and inefficient routing issues by organizing the address into <code>xxx.xxx.xxx.xxx</code>  format, where each number can range from <code>0-255</code> (essentially using 4 bytes, i.e. IPv4).  To help routing data faster, this continuous address is divided smaller, manageable segments, called subnets so that you can just traverse these subnets to avoid the majority of network graph while moving towards destined IP address. This is done by dividing the address into two parts:</p> <ol> <li>Network portion, which uses some starting bits in the address to identify the subnet.</li> <li>Host portion, which uses the remaining bits to identify the host in subnet. </li> </ol> <p>This split is represented using CIDR (Classless Inter-Domain Routing) notation, which defines subnet using  <code>a.b.c.d/x</code> format. Here <code>x</code> bits in <code>a.b.c.d</code> belongs to network and rest to identify hosts within subnet. But this notation is for human-readability, internally computers store this information separately, where network = <code>a.b.c.d</code>  and <code>/x</code> is stored using subnet mask formed using 1s in first <code>x</code> bits of 32 bits. This way computers can AND this mask with any IP address to determine if the address belongs to network. For example, <code>192.168.254.9/24</code> indicates </p> <ul> <li>first 24 bits belong to subnet -&gt; Network = <code>192.168.254.0</code></li> <li>Subnet Mask = <code>11111111 11111111 11111111 00000000</code> -&gt; <code>255.255.255.0</code> </li> <li> <p>Reserved Broadcast address(1) -&gt; <code>192.168.1.255</code></p> <ol> <li>used as signal for broadcasting the request on subnet, like ARP Request discussed     below</li> </ol> </li> <li> <p>Hosts can use IP from <code>192.168.254.1</code> -&gt; <code>192.168.254.254</code>.</p> </li> </ul> Example, if an IP belongs to subnet <pre><code>Subnet = 10.1.32.0/20 -&gt; mask = 11111111 11111111 11110000 00000000\nIncomingIP = 10.1.38.77 -&gt;  AND 00001010 00000001 00100110 01001101\n                                -----------------------------------\n                                00001010 00000001 00100000 00000000\n  same as network address    -&gt;     10         1        32     0 \n  IP belongs to subnet\n</code></pre> <p>Creating subnets simplifies a lot of networking:</p> <ul> <li>Routers use subnet information to make faster decisions, forwarding packets only to the necessary subnet, not the   whole internet. Now, routers only need to know how to reach specific subnets, making routing tables smaller and more   efficient.</li> <li>Subnets can be aggregated into larger blocks (supernets), further reducing routing table complexity for internet routers. </li> <li>Subnets organize devices by location, department, or function, allowing for targeted routing policies and easier   management.</li> </ul> How to Divide Subnet into Smaller Subnets? <p>If you want to divide this subnet into further smaller subnets for better organization, you can extend the bits used by network. For example, </p><pre><code>/24 -&gt; /26\n\n                     192.168.254.0/26\n192.168.254.9/24 -&gt;  192.168.254.64/26\n                     192.168.254.128/26\n                     192.168.254.192/26\n</code></pre><p></p> <p>With this information, we can start explaining the journey of IP packets when moving through network. Once the IP packets are assembled at Network Layer, sender checks if the IP address belong in its subnet or not.</p> <ul> <li>If the destination IP belong same subnet, it'll follow the    Intra-network Communication pathway.</li> <li>If the destination IP is outside its subnet, it'll follow the   Inter-network Communication pathway.</li> </ul>"},{"location":"Courses/focn/ip/#intra-network-communication-using-arp","title":"Intra-network communication using ARP","text":"<p>When destination belongs within same subnet, the communication entirely happens within LAN using just frames at  Data Link Layer (L2). To develop frames, you'll need receiver's MAC address (1) to identify the host reliably, but sender only know IP address of receiver (it got through DNS resolution). To get receiver's MAC address, it uses ARP (Address Resolution Protocol). To understand ARP, let's take following example. Device A wants to send an  IP packet to Device B within same network.</p> <ol> <li>which is a permanent address associated to your network hardware on device (like NIC)</li> </ol> <pre><code>sequenceDiagram\n    participant A as Device A\n    participant LAN as LAN (Broadcast Domain)\n    participant B as Device B\n\n    A-&gt;&gt;A: Check if destination IP is in same LAN\n    A-&gt;&gt;A: Check ARP cache for IP \u2192 MAC mapping\n\n    alt MAC found in ARP cache\n        A-&gt;&gt;A: Use cached MAC to build Ethernet frame\n    else MAC not found\n        A-&gt;&gt;LAN: Broadcast ARP Request (Who has IP B?)\n        LAN-&gt;&gt;B: ARP Request received\n        B-&gt;&gt;A: Unicast ARP Reply (IP B is at MAC B)\n        A-&gt;&gt;A: Store MAC in ARP cache\n        A-&gt;&gt;A: Build Ethernet frame using MAC B\n    end\n</code></pre>  Use mouse to pan and zoom  ARP Poisoning <p>Since ARP request is broadcasted to every device in a subnet, bad host can respond with their own MAC address before the real host which leads to redirect of traffic to bad players. This attack is called ARP Poisoning. Modern systems uses strategies like dynamic ARP inspection, switch port security or keeping static ARP entries in  critical systems to avoid such attacks.</p> <p>This entire networking infrastructure is handled by Switch (1) which stores each device's MAC address, and map it to  respective physical port to forward the frames and Ethernet Cables which moves the frames from one device to  another. In case of Wireless devices (like Wi-Fi), devices communicate through Access Point (AP) which then connects to a switch.</p> <ol> <li>different from Router</li> </ol> <p>This model of communication is simple and works fine for small LANs, but scale for WANs due to congestion caused by  broadcasts. It's also easy to spoof (ARP poisoning), which makes it vulnerable for communicating with unknown device  on internet. That's why when a device wants to communicate with hosts outside the LAN, it's done using a different protocol called IP, which is used for inter-network communication. </p>"},{"location":"Courses/focn/ip/#inter-network-communication-using-ip","title":"Inter-network communication using IP","text":"<p>When destination belongs outside sender's subnet, it'll send the request to a special device within its subnet called Default gateway (usually your router) (1). Routers are another core infrastructure in networking, whose main responsibility is forwarding packets into right direction. This is done by looking at the destination IP address of packet (L3, need to strip L2 headers) and searching it against Routing Table(2) to find best match(3) which will be  either the next hop of packet towards destination or the destination subnet itself.</p> <ol> <li>It's named gateway literally based on its functioning, i.e. it acts as a gateway for subnet devices to help them     communicate with devices on other subnets.</li> <li>data structure maintained across all routers, which stores routing information like destination network, interface and    next hop in a table.</li> <li>the longest prefix match.</li> </ol> <p>A key entry in routing table is <code>0.0.0.0/0</code>, this is used when router doesn\u2019t recognize the destination network,  so it send the packet to its ISP router (default route when no match is found).</p> <p>This terminology of \"default gateway\" is relative to each device, where if the device (router/computer) doesn't know where to send the request next, it'll send it to default gateway device upstream having knowledge of wider network. For PCs/phones this is default gateway/router, for your router this is default route to ISP's router in routing table, for ISP's router this is some upstream router (like (1)). This way you can divide the internet across different levels  of routers and every router always know where to send the packet next without knowing the whole internet. </p> <ol> <li>Regional / National ISP routers for connecting local ISP to larger ISPs which defaults to tier-1 ISPs which forms backbone of     internet like Tata, AT&amp;T.</li> </ol> <p>But before your router sends your IP packet outside subnet, it performs another key operation called NAT.</p>"},{"location":"Courses/focn/ip/#nat","title":"NAT","text":"<p>IPv4 address only allows the system to address ~\\(4.3\\) billion (\\(2^{32}\\)) hosts which is insufficient at this time due to high usage of devices like PCs, phones, servers, IoT. To solve this issue, there were two  proposals:</p> <ul> <li>Use IPv6 address which can support upto ~\\(340\\) undecillion (\\(2^{128}\\)) devices.</li> <li>Hiding many private devices behind one public IPv4 address using NAT (Network Address Translation). </li> </ul> IPv6 adoption <p>IPv6 was the right solution, but it came too late for the crisis. IPv4 exhaustion became serious in the  mid-1990s while IPv6 specs were finalized around 1998. The immediate response to avert the crisis was NAT, which could be deployed immediately without waiting for support from OS, routers, applications. It worked with existing IPv4 and by just changing single router you could fix many devices. Since NAT was cheaper and faster backward-compatible option, it was adopted immediately. While IPv6 is still being implemented slowly even after two decades (since it's the right choice technically) but its adoption was delayed due to huge success of NAT.</p> <p>NAT is implemented on routers, and it rewrites IP addresses (and usually ports) in packets so that multiple  private devices can share one or a few public IP addresses. To differentiate b/w each of its private host, it uses NAT Table which maps devices privateIP (and port) to the publicIP (and port). This entries in table are created only when private host initiates an internet request, and usually times out when unused for some time.  For example,</p> <pre><code>sequenceDiagram\n    participant A as Device A (192.168.1.10)\n    participant R as NAT Router\n    participant S as Internet Server (142.250.72.14)\n\n    A-&gt;&gt;R: TCP Packet&lt;br/&gt;Src: 192.168.1.10:52344&lt;br/&gt;Dst: 142.250.72.14:443\n    R-&gt;&gt;R: Create NAT entry&lt;br/&gt;203.0.113.5:40001 \u2194 192.168.1.10:52344\n    R-&gt;&gt;S: Translated Packet&lt;br/&gt;Src: 203.0.113.5:40001&lt;br/&gt;Dst: 142.250.72.14:443\n\n    S-&gt;&gt;R: TCP Reply&lt;br/&gt;Src: 142.250.72.14:443&lt;br/&gt;Dst: 203.0.113.5:40001\n    R-&gt;&gt;R: Lookup NAT table&lt;br/&gt;Find internal mapping\n    R-&gt;&gt;A: Restored Packet&lt;br/&gt;Src: 142.250.72.14:443&lt;br/&gt;Dst: 192.168.1.10:52344\n</code></pre>  Use mouse to pan and zoom  <p>The above example is one type of NAT, also known as PAT (Port Address Translation) since it also uses ports to map  requests. Since single router can have \\(1000\\)s of ports, PAT allows the router to translate many devices making it most commonly used. Other commonly used NAT implementation is Static NAT which maps public to private IP (1:1), usually used for  servers.</p> <p>With NAT, your devices could now be assigned IP address which would never be used on internet. This means, we can  use same IP address for two devices on different private network without conflict. This idea leads to reserving  chunks of specific IP address (1) for internal usage in private network (RFC 1918). Additionally, to make it more secure,  routers would immediately drop any packet using private IP address.</p> <ol> <li>10.0.0.0/8, 172.16.0.0/12, and 192.168.0.0/16</li> </ol> <p>NAT also have many other applications apart from public to private translations. Some of them are</p> <ul> <li>Port forwarding which can be used to expose local webservers publicly</li> <li>Load balancing at Layer 4, used by proxies like HAProxy which replaces the destIP to one of the servers grouped to    this destIP (reverse proxying).</li> </ul> <p>With this, we can explain how out inter-network communication moves through different devices using IP protocol. </p> End-To-End Overview of Internet Request <pre><code>sequenceDiagram\n    participant H as Host (192.168.1.10)\n    participant R as Home NAT Router\n    participant ISP as ISP Edge Router\n    participant UP as Upstream / Backbone Router\n    participant SISP as Destination ISP Router\n    participant S as Internet Server (Public IP)\n\n    H-&gt;&gt;R: Packet&lt;br/&gt;Src: 192.168.1.10:52344&lt;br/&gt;Dst: ServerIP:443\n    R-&gt;&gt;R: NAT (PAT)&lt;br/&gt;192.168.1.10:52344 \u2192 203.0.113.5:40001\n    R-&gt;&gt;ISP: Translated Packet&lt;br/&gt;Src: 203.0.113.5:40001\n\n    ISP-&gt;&gt;UP: Route using BGP&lt;br/&gt;(next hop decision)\n    UP-&gt;&gt;SISP: Forward packet\n    SISP-&gt;&gt;S: Deliver to server\n\n    S-&gt;&gt;SISP: Reply&lt;br/&gt;Src: ServerIP:443&lt;br/&gt;Dst: 203.0.113.5:40001\n    SISP-&gt;&gt;UP: Forward reply\n    UP-&gt;&gt;ISP: Forward reply\n    ISP-&gt;&gt;R: Reply arrives\n\n    R-&gt;&gt;R: NAT Table Lookup&lt;br/&gt;203.0.113.5:40001 \u2192 192.168.1.10:52344\n    R-&gt;&gt;H: Restored Packet&lt;br/&gt;Dst: 192.168.1.10:52344\n</code></pre>  Use mouse to pan and zoom"},{"location":"Courses/focn/ip/#anatomy-of-ip-packet","title":"Anatomy of IP Packet","text":"<p>IP Packet (IPv4 specifically) is the unit of data used for networking at Network Layer (L3) as per IP specification  (also known as PDU (1)). Until now, we've only discussed its routing aspect through IP address, but there are few other required fields defined by protocol. To understand each of them, let's go through the anatomy of IP packets.</p> <ol> <li>Protocol Data Units</li> </ol> <p>An IP packet is continuous block of bytes organized into two major section:</p> <ol> <li>Header, which stores metadata about IP Packet using first 20-60 bytes of packet. </li> <li>Body, which stores the actual segmented data (datagrams/segments) from Transport layer.</li> </ol> <pre><code>            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25004 bytes\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           \u2502Version\u2502  IHL  \u2502  DSCP \u2502 ECN \u2502         Total Length          \u2502\n\u2502           \u2502 4 b   \u2502  4 b  \u2502  6 b  \u2502 2 b \u2502            16 b               \u2502\n\u2502           \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n20 bytes    \u2502        Identification       \u2502Flags\u2502   Fragment Offset       \u2502\nRequired    \u2502            16 b             \u2502 3 b \u2502        13 b             \u2502\n\u2502           \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502           \u2502  TTL          \u2502   Protocol  \u2502        Header Checksum        \u2502\n\u2502           \u2502 8 b           \u2502     8 b     \u2502            16 b               \u2502\n\u2502           \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502           \u2502                     Source IP Address                       \u2502\n\u2502           \u2502                         32 b                                \u2502\n\u2502           \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502           \u2502                  Destination IP Address                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502                         32 b                                \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n40 bytes    \u2502                    Options (if any) + Padding               \u2502\nOptional    \u2502                  Variable (0\u201340 bytes)                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502                    Data                                     \u2502\n            \u2502                                                             \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ul> <li>Version: First 4 bits are used to determine the Version of packet, if its IPv4 or IPv6.</li> <li>IHL (Internet Header Length): defines length of header in 4 byte lines. Default is 5 which reads 20 (\\(5 \\times 4\\)) bits    for required headers.</li> <li>DSCP (Differentiated Service Code Point): defines the forwarding priority of packets. This is required because   not all packets are equally important. For example, VoIP (Voice Over IP) packets are given more priority since they're   used for voice call which is sensitive to delay. With the help of DSCP, you can mark your packets at the edge    (host, router, firewall) based on your policy and prioritize traffic as per it.</li> <li>ECN (Explicit Congestion Notification): used by L3 devices to notify upper layers if the network is facing   congestion. We'll look into it when talking about Congestion Control in TCP protocol (L4).</li> <li>Total Length: Total length of packet (header+body) in bytes. Since it uses 16 bits, packets size range can range   from \\(0\\) - \\(2^{16}\\) bytes (~64kB).</li> <li> <p>Next line of header is used by IP fragmentation.</p> IP Fragmentation <p>Every network link is associate with an MTU (Maximum Transmission Unit) (1). A host may send a perfectly  valid IP packet that fits its outgoing interface (using TCP segmentation (2)) but might be too large for a  downstream link. This is why routers can't assume same MTU everywhere and that the sender knows the path MTU.  To help with this, Fragmentation was introduced which allow routers to split large IP packet into smaller  pieces (fragments) so it can traverse network links with a smaller MTU, with the fragments later reassembled at  the destination host using respective IP headers.</p> <ul> <li>Identification: ID to reassemble fragments belonging to the same original IP packet.</li> <li>Flags: three 1-bit flags,<ul> <li>bit 0: reserved for future compatability, must be set to 0.</li> <li>bit 1: DF (Don't Fragment) flag, set this bit to tell routers to not fragment the packet and drop if it's too big.    This is used in Path MTU Discovery.</li> <li>bit 2: MF (More Fragment) flag, set this bit to tell receiver about more incoming packets. Last fragment will have    MF = 0</li> </ul> </li> <li>Fragment Offset: Offset where this fragmented packet belongs in whole packet</li> </ul> <ol> <li>size of largest data packet link can handle without fragmentation</li> <li>This will be discussed more in TCP chapter.</li> </ol> </li> <li> <p>TTL (Time to Live): defines maximum number of hops a packet can survive before its discarded. This is done to    avoid packet roaming around the network infinitely (due to cycles). At each hop, the router must decrement this field   and when any router encounters an IP packet where TTL=0, it'll discard the packet and return an ICMP(1) message stating    the reason back to client. </p> <ol> <li>disused below</li> </ol> </li> <li> <p>Protocol: identifies which upper-layer protocol (like ICMP, TCP, UDP) should receive the payload. Without this    field, the receiver wouldn\u2019t know how to interpret the payload. </p> </li> <li>Header Checksum: protects header fields from corruption and tampering. This is required because headers like TTL   are modified by routers inflight.   </li> <li>Options: providing future compatability without redefining the protocol. Sender can use this space to attach extra   instructions or metadata to an IP packet.</li> </ul>"},{"location":"Courses/focn/ip/#icmp","title":"ICMP","text":"<p>ICMP (Internet Control Message Protocol) is another important Network Layer (L3) protocol which is used by network  devices to communicate errors/operational information about IP packet delivery. ICMP was designed because IP doesn't define a way to handle errors like delivery failures, routing problems or reachability and timing info back to sender. ICMP message have the following format:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     Type 8 b      \u2502     Code 8 b     \u2502   Checksum 16 b      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                Message-Specific Data 32 b                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        Original IP Header + First 8 Bytes of Payload        \u2502\n\u2502                 for reporting as logs                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ul> <li>Type: defines the general category of ICMP message. For example, 8 - Echo Request, 0 - Echo Reply, 3 - Destination   Unreachable, 11 - Time Exceeded.</li> <li>Code: adds more specific detail about the Type. For example, Type 3 (Destination Unreachable) can have Code 0 -   network unreachable, 1 - Host unreachable, 3 - Port unreachable, 4 - Fragmentation needed.</li> <li>Checksum: ensures integrity of the ICMP message (header + data).</li> <li>Message-Specific Data to make ICMP flexible for different message type. For example, Destination Unreachable can    send the MTU value required for fragmentation.</li> <li>Original IP Header + First 8 Bytes of Payload, to allow sender to match the ICMP message to specific packet which    caused the error</li> </ul> <p>To prevent boardcast storms ICMP errors aren't send for other ICMP errors or for broadcast message. </p> <p>The most common use case of ICMP are:</p> <ol> <li>Error reporting: Routers and hosts send ICMP messages when a packet can\u2019t be delivered.</li> <li>Network diagnostics: You can build tools on ICMP to provide diagnostics on network. For example, <code>ping</code> uses ICMP    Echo Request / Reply to ping an ip and <code>traceroute</code> uses ICMP Time Exceeded message to provide packet path taken from    source to destination.</li> <li>Path MTU Discovery (PMTUD): Start sending large packets with set DF flag. On the way, router would drop the oversized    packet and sends ICMP \"Fragmentation Needed\". You can reduce the packet size and repeat the process until it reaches     destination. This way you can adjust your packet size while avoiding fragmentation.</li> </ol>"},{"location":"Courses/focn/udp/","title":"TCP and UDP","text":""},{"location":"Courses/focn/udp/#tcp-and-udp","title":"TCP and UDP","text":""},{"location":"Courses/fode/","title":"Fundamentals of Database Engineering","text":""},{"location":"Courses/fode/#fundamentals-of-database-engineering","title":"Fundamentals of Database Engineering","text":"<ul> <li> ACID</li> <li> Understanding Database Internals</li> <li> Database Indexing</li> <li> B-Trees vs B+-Trees</li> <li> Database Partitioning</li> <li> Database Sharding</li> <li> Concurrency Control</li> <li> Replication</li> <li> Database Engines</li> <li> Database Cursor</li> <li> NoSQL Architecture</li> <li> Database Security</li> <li>Extra</li> </ul>"},{"location":"Courses/fode/acid/","title":"ACID","text":""},{"location":"Courses/fode/acid/#acid","title":"ACID","text":"<p>ACID, which stands for Atomicity, Consistency, Isolation and Durability - are fundamental properties desirable across all database systems.</p> <p>To understand these properties individually, you should know about Transactions.</p> Transactions briefly <p>A transaction is a collection of DML(1) queries treated as one unit of work at application logic. </p> <p>For Example, money transfer between accounts requires multiple operations (check balance, debit one account, credit another) to succeed or fail together. However, this is performed using a bunch DML queries and failure in any single of them could result in bugs like money being debited even if there's no sufficient balance, money being debited without crediting, etc. That's is why all these operations should be wrapped in an Transaction which ensures they either run all or none.</p> <p>Transaction lifecycle involves keywords to start (<code>BEGIN</code>) , save changes (<code>COMMIT</code>) and discard changes (<code>ROLLBACK</code>). Each of these commands are implemented differently across different DBMS, like COMMIT either flushes changes made in memory to disk in one go, or it saves individual changes separately. This is due to the tradeoffs involved with such decision, which makes each DB unique and optimized for their  own specific use case and no general DB system can handle it all. </p> <p>Often transactions are mostly used in writing data, but you can also have read-only transactions. For example, transactions for generating consistent reports by providing a time-based snapshot of data.</p> <ol> <li>Data Manipulation Language</li> </ol>"},{"location":"Courses/fode/acid/#atomicity","title":"Atomicity","text":"<p>Every transaction should be treated as indivisible unit of work (either all queries within it succeed or fail, no partials). </p> <p>This helps DB to remain in a consistent state by guaranteeing that all failed transactions are rolled back which helps  to prevent data corruption and maintain consistency. It also simplifies error handling where developers don't need to  handle rollbacks.</p> <p>There are different ways to implement atomicity, few of which are:</p> <ul> <li>Logging: before writing changes to disk, they're written to undo/redo logs and only applied when commit is successful.</li> <li>Shadow Copy: changes are applied to a copy of original page. When the transaction is successful,    the pointers to data are updated to apply the changes.</li> <li>Two-Phase Commits: used in distributed systems, ensuring all peers commit or abort the transaction together.</li> </ul>"},{"location":"Courses/fode/acid/#isolation","title":"Isolation","text":"<p>Isolation property in transaction helps prevent concurrent operations from interfering with each other, ensuring each transaction appears to run on its own. It's managed through different isolation levels, which control how transactions interact with the concurrency anomalies encountered. </p> <p>Concurrency anomalies (or Read Phenomenas) are undesirable side effects of running multiple transactions at same  time. Few of these includes:</p> <ul> <li>Dirty Reads: when a transaction reads uncommitted changes made by another concurrent transaction,    which is rolled back. From DB point of view this change was never present in data as it was never committed    essentially making our read dirty.  </li> <li>Non-Repeatable Reads: when you read same entry more than once in a transaction, and it yields different values.    For example, you read a value directly in first query and then collect sum in second query.    If the value is changed when collecting sum, this will result in inconsistent sum w.r.t data in first query.   That\u2019s why it's called non-repeatable as in you can't read repeated value in same transaction. </li> <li> <p>Phantom Reads: when re-reading a range of rows, a new row appears due to write by other transaction.    The reason it's different from Non-Repeatable Reads is due to the way Repeatable Read (1) isolation level is implemented.</p> <ol> <li>Most DBs implement Repeatable Reads by keeping a version of rows being used in the transaction. This approach     doesn't help with Phantom Reads, as you can't version non-existent rows.</li> </ol> </li> <li> <p>Lost Updates: when two or more concurrent transactions read the same data, both make a modification based    on that data, and the second transaction's update overwrites the first one, effectively erasing its changes.    This leads to inconsistency as work updated by one transaction is lost due to overwrite from others.</p> </li> </ul> <p>To prevent these anomalies, Isolation property provide different levels of control. Below are few commonly implemented Isolation levels listed from lowest to highest Isolation.</p> <ol> <li>Read committed: transactions will only see committed changes. This solves dirty read as you\u2019re sure the changes    read are committed.</li> <li>Repeatable Read: with this isolation level you can repeat reads consistently within your transaction, solving     non-repeatable and dirty read anomalies.</li> <li>Serializable: concurrent transactions are executed as if they're being run one after another, essentially solving    all concurrency anomalies.</li> <li>Snapshot: allows transaction to read from a consistent snapshot of database without blocking writers.</li> </ol> Table Anomalies/Isolation Isolation Level Dirty Reads Non-repeatable Reads Phantom Reads Lost Updates Read Committed Repeatable Read Snapshot Serializable <p>There\u2019s mainly two different approaches for implementing isolation:</p> <ul> <li>Pessimistic approaches by using locks. These maybe on either row, table or page level. </li> <li>Optimistic approaches keep track of transactions and fails one of them when they overstep each others isolation levels.    This approaches reduces the significant lock management overhead on DB, but requires additional handling for retries.  </li> </ul> <p>NOTE</p> <p>Postgres implements Repeatable Read as a Snapshot isolation level and as such you don\u2019t get Phantom reads there but this might not be true for other DB system which implements Repeatable Read by maintaining version of rows.</p> <p>The choice of isolation level balances data consistency with performance, as higher isolation provides more consistency but can decrease performance.</p>"},{"location":"Courses/fode/acid/#consistency","title":"Consistency","text":"<p>Consistency ensures that a database remains in a valid state both before and after a transaction by guaranteeing  adherence to all predefined rules, constraints, and triggers.</p> <p>When defining Consistency across DB system, it can mean two different things:</p> <ol> <li>Consistency in Data - consistent data w.r.t to the defined data model.     For example, having integrity across defined constraints (like primary key, foreign keys, data type),     cleaning orphaned references as per defined rules and constraints.</li> <li> <p>Consistency in Read/Write - consistently reading data across different instances of DB.</p> Read More here <p>Read consistency ensures a transaction sees the most recent committed changes immediately.  This consistency challenge is introduced due to Replication, specifically when data written to primary isn't  synced yet to replicas. This is usually done to optimize for performance. For example, Eventual consistency within a system provide higher performance but the application may  temporarily show stale data before eventually reflecting correct values. While Synchronous replication  offers stronger consistency at the cost of slower performance compared to asynchronous approach.</p> </li> </ol> <p>Consistency acts as a safeguard, ensuring that data integrity is maintained and preventing the database from entering  an invalid or corrupted state due to incomplete or erroneous transactions.</p>"},{"location":"Courses/fode/acid/#durability","title":"Durability","text":"<p>Durability ensures changes from a committed transaction are permanently stored on non-volatile storage (e.g., SSD, HDD) \u2014 even if the system losses its power or crashes.</p> <p>DB systems play around with this concept to optimize their performance since writing to disk is slower, and instead you can write to memory first and then flush the changes to disk in bulk. But this may compromise the durability under uncertain conditions, so in addition DBs writes these changes in a compact format to a log file (<code>WAL</code> (1) ) on disk so that even if we  lose the data in memory - the record can be replayed to recover the lost data. This is better because the changes  written are compact and appended at the end of file.</p> <ol> <li>Write Ahead Log</li> </ol> <p>NOTE</p> <p>The standard <code>write()</code> operation in OS caches writes in file system for better performance. If the system crashes during this time, the data in the cache is lost. Instead, DBs use the <code>fsync</code> operation to immediately write to disk, ensuring durability but at a performance cost.</p> <p>For mission-critical systems, strong durability is non-negotiable; for less critical data, eventual durability may be acceptable.</p>"},{"location":"Courses/fode/concurrency_control/","title":"Concurrency Control","text":""},{"location":"Courses/fode/concurrency_control/#concurrency-control","title":"Concurrency Control","text":"<p>Consider the given scenario, you've a ticket booking platform which allows multiple users to book ticket at same time. In one particular case, two users decides to book tickets for same seat at same time. Ideally the querying logic for this would be to</p> <ul> <li>Check the availability of seat in DB -&gt; <code>SELECT is_booked FROM seats WHERE id ='xyz'</code></li> <li>If available book the seat and update booked to <code>true</code> -&gt; <code>UPDATE seats SET is_booked=1 WHERE id = 'xyz'</code></li> </ul> <p>This logic in our case would book the tickets and send the confirmation to both users which shouldn't be happening. Such scenarios exist all over applications which need to check the inventory and reserve the item for end users. You could prevent this mishap if you'd control over who could access the selected seat at start of any transaction. Such features are provided under Concurrency control category and locking is one of the most commonly used implementations. </p>"},{"location":"Courses/fode/concurrency_control/#locks","title":"Locks","text":"<p>Locks are logical constructs which could be placed over a resource to prevents access of certain operations over it from other parties. Based on access pattern, locks can be divided into 2 major category:</p> <ul> <li>Exclusive Locks: Can be acquired by 1 transaction exclusively allowing it to read-write over the data while    preventing read-writes from others. These locks are primarily used to allow a transaction modify data without external   conflict.  </li> <li>Shared Locks: Can be shared between multiple transaction for reading but prevents writes from any. This allows   multiple transaction to read same data concurrently while maintaining the integrity of data. </li> </ul> <p>Usage of locks should be managed with care as it could easily lead to issues like Dead Lock where 2 transactions are waiting indefinitely to acquire lock obtained by each other. Since neither of transaction will release the  lock before committing the changes, they\u2019ll both keep waiting forever. Such scenarios must be handled by the database and depending on implementation rollback and fail one of the transactions. One way to prevent Dead locks is by using Two Phase Locking.</p> <p>Two Phase Locking ensures transaction are ordered during execution by controlling how they acquire and release locks. It works in two phases</p> <ol> <li>Growing phase where locks are acquired.</li> <li>Shrinking phase where locks are released.</li> </ol> <p>For example, in Double Booking Problem discussed at the start, if we\u2019d acquired the exclusive lock during the reading phase we could avoid the double booking situation as other transactions can\u2019t read this value currently. Only after the first transaction commits does this lock gets released upon which it can see that the seat  is already booked, so it\u2019ll return immediately.</p>"},{"location":"Courses/fode/concurrency_control/#locking-in-postgres","title":"Locking in Postgres","text":"<p>While the concept of locks on top remains as discussed above, their implementation could vary over different DB systems, to provide more granular control. For example, we'll discuss the kinds of locks provide by Postgres. For more information, can view the original postgres docs Reference: Postgres Docs</p> <p>Postgres categorizes its locks into 5 categories</p>"},{"location":"Courses/fode/concurrency_control/#tabel-level-locks","title":"Tabel Level Locks","text":"<p>Postgres provides 8 different type of table locks and transactions can have multiple locks on same table. Some of these lock can conflict, others don\u2019t.</p> <ul> <li><code>ACCESS EXCLUSIVE</code>: conflicts will all other table locks and as such completely locks the table for other transactions.</li> <li><code>ACCESS SHARE</code>: Generally acquired by queries which only reads from table like <code>SELECT</code>.</li> <li><code>EXCLUSIVE</code>: similar to <code>ACCESS EXCLUSIVE</code> except it doesn\u2019t conflict <code>ACCESS SHARE</code> locks for reading.   It\u2019s only used by <code>REFRESH MATERIALIZED VIEW CONCURRENTLY</code> command, which seems it was added so that users   can refresh their materialized view while also reading from it.</li> <li><code>ROW SHARE</code>: designed for <code>SELECT FOR...</code> commands like <code>SELECT FOR UPDATE</code>, <code>SELECT FOR SHARE</code> which works on   row level. These commands obtain two kinds of locks \u2014 a Row lock and ROW SHARE table lock.</li> <li><code>ROW EXCLUSIVE</code>: this lock mainly impacts write latency in system since it's used by write commands like <code>UPDATE</code>,   <code>DELETE</code>, <code>INSERT</code>, <code>MERGE</code>, <code>COPY FROM</code>.</li> <li><code>SHARE ROW EXCLUSIVE</code>: prevents table against concurrent data changes, and is self-exclusive so that only one session    can hold it at a time. Acquired by some <code>ALTER TABLE</code> command and <code>CREATE TRIGGER</code>.</li> <li><code>SHARE</code>: protects a table against concurrent data changes but isn't self exclusive. Used by <code>CREATE INDEX</code> so that    you can create multiple indexes concurrently but the data isn\u2019t allowed to change.</li> <li><code>SHARE UPDATE EXCLUSIVE</code>: allows concurrent writes and reads but prevents schema changes and VACCUM runs.</li> </ul> <p>Below table will summarize which locks would conflict with one another.</p> <p></p>"},{"location":"Courses/fode/concurrency_control/#row-level-locks","title":"Row Level Locks","text":"<p>Row locks are critical to prevent lost updates. New tuples don\u2019t require locks as they\u2019re only visible to current transaction in which they were created. That\u2019s why postgres doesn\u2019t support READ UNCOMMITTED isolation.  These locks are limited to <code>DELETE</code> , <code>UPDATE (no key)</code> , <code>UPDATE (key)</code>, and all <code>SELECT FOR</code>s. (key/no key refers if the column has unique index on that column or not) </p> <ul> <li><code>FOR UPDATE</code>: highest row lock, you can\u2019t delete, update on the row when this lock is acquired by other transaction.    It\u2019s self conflicting, so you can\u2019t use two <code>FOR UPDATE</code> concurrently. You can still read it through normal <code>SELECT</code>.   It\u2019s obtained by <code>DELETE</code>, <code>UPDATE (key)</code>, <code>SELECT</code> commands.</li> <li><code>FOR NO KEY UPDATE</code>: acquired for updates to column without a unique index. It\u2019s weaker <code>FOR UPDATE</code> as it allows   <code>SELECT FOR KEY SHARE</code>.</li> <li><code>FOR SHARE</code>: true shared lock, it can be acquired by multiple transactions. When acquired, it blocks modification to row</li> <li><code>FOR KEY SHARE</code>: like <code>FOR SHARE</code> but allows update to column without unique index.</li> </ul> <p>To view conflict among above locks, refer to below table </p> <p>Postgres stores table locks in memory because they\u2019re coarse but row locks are stored alongside table in <code>xmax</code> system field, which saves memory but costs disk write. </p>"},{"location":"Courses/fode/concurrency_control/#page-level-locks","title":"Page Level Locks","text":"<p>Postgres page are of size 8KB and stores tuples for table and indexes. Since these pages loaded in shared buffer pool and Postgres being process based backend, multiple process  can access these pages in shared buffer memory which could lead to inconsistent read or conflicting writes. To avoid such cases, postgres provides page-level share/exclusive locks to control read/write access to table pages  in the shared buffer pool.</p>"},{"location":"Courses/fode/concurrency_control/#dead-locks","title":"Dead Locks","text":"<p>Using explicit locking can increase the likelihood of deadlocks among transactions in DB. Postgres detects such conditions and kills one of the transaction to avoid blocking forever. So long as no deadlock situation is detected, a transaction seeking either a table-level or row-level lock will wait indefinitely for conflicting locks to be released. This means it\u2019s a bad idea for applications to hold transactions open for long periods of time</p>"},{"location":"Courses/fode/concurrency_control/#advisory-locks","title":"Advisory Locks","text":"<p>Sometimes application requirements aren\u2019t satisfied by postgres built in locks.  To help with this, postgres provides application based locks which are managed by application. However, these locks still live in databases. These are of two types: - session lock: obtained with <code>pg_advisory_lock()</code> , are kept for the length of session - transaction lock: obtained with <code>pg_advisory_xact_lock()</code>, are kept for length of current running transaction</p>"},{"location":"Courses/fode/concurrency_control/#optimistic-concurrency-control","title":"Optimistic Concurrency Control","text":"<p>Locking as discussed above is categorized under Pessimistic Concurrency Control, as in you don\u2019t trust others (Pessimistic) while updating a row, that you lock it and when anyone comes to mess with it while you\u2019re updating  the row, you can tell them whatever you want to.</p> <p>Optimistic Concurrency Control in contrast allow transactions to freely (Optimistic) operate during transaction but validates conflicts during COMMIT. If there's any conflict found, it'll abort the transaction and return an error to user. This kind of concurrency control is mostly used in read heavy workload where you get conflicts rarely as data isn't modified as frequently.</p>"},{"location":"Courses/fode/concurrency_control/#multi-version-concurrency-control-mvcc","title":"Multi-Version Concurrency Control (MVCC)","text":"<p>Instead of locking rows for reads, the DB keeps multiple versions of rows. Readers see a consistent snapshot without blocking writers. Postgres uses MVCC which you can read more about in their  docs</p>"},{"location":"Courses/fode/db_cursor/","title":"Database Cursor","text":""},{"location":"Courses/fode/db_cursor/#database-cursor","title":"Database Cursor","text":"<p>Suppose your SQL query is fetching lots of rows, and it takes some time before getting back the result.  This is because the query needs to do plan query execution, gather all records, then move this data over to TCP and transmit it over network and finally collect all the data on client side. All these steps become increasingly slower as we fetch more and more rows. Also, sometimes few of the clients won't have the required memory to store all the results from such huge queries.</p> <p>This scenario can be avoided using server side Cursor, which allow you to encapsulate a query into a token and fetch few entries at a time from the query result using that token. This allows you to have immediate result for few queried entries allowing the UI to partially load data which wouldn't affect the user workflow. You can save  memory usage on client side by processing few rows at a time. You can also cancel the query midway, if you're satisfied with your results. And all these can be streamed through websocket allowing smooth workflow on client. But this shifts the load of managing state of cursor to backend/server, which may causes resource starvation if not managed properly. Also, long-running cursors involving transactions would continue to block writes causing bad write performance.</p> <p>The above-mentioned scenarios are applicable for Server Side Cursors, where the server is responsible for managing the state of cursor. You can also use Client-Side Cursors, which provided limited sets of queried entries by sorting and filtering data in batches (using keywords like <code>offset</code>, <code>limit</code>, <code>sort by</code> in SQL).</p>"},{"location":"Courses/fode/db_cursor/#cursors-in-sql-server","title":"Cursors in SQL Server","text":"<p>To understand more about how cursors are implemented in real production DBs, let's look at the implementation of cursors in Microsoft SQL Server. Most of the information is derived from their docs referenced  here.</p> <p>They define cursors as an extension over results sets to provide processing like positioning the cursor to  a specific row in result, retrieve/modify one or many rows from current position, allowing different visibility to changes made by others and finally allowing access to stored procedures and triggers. SQL Server supports four cursor types.</p> <ol> <li>Forward-Only: is a forward-only and read-only cursor which doesn't support scrolling. They only allow you to     read data serially from start to end. Rows are retrieved from DB as they're fetched, which allows it to read     modification made by other users even after its declaration. This simplicity allows it to fetch result quickly    while keeping low memory footprint.</li> <li>Static: Builds the complete result set in a temporary DB (<code>tempdb</code>) when this cursor is opened allowing it    to hide modification made by other users after cursor declaration.  Due to the static copy, you can scroll through    result set quickly, but storing the entire result set uses more memory.</li> <li>Keyset: controlled by a set of unique identifiers, or keys, known as the <code>keyset</code>, stores the key and respective    row in temporary DB (<code>tempdb</code>). When the result is fetches, DB will fetch the related row specified by key from table    as such it only detects modification like updates and deletes but not inserts since the key for new row wouldn't    be present in <code>tempdb</code>. Since only keys are stored in <code>tempdb</code>, it uses lesser memory than <code>STATIC</code> cursor  but    fetching respective rows requires additional lookup to underlying table making scrolling slower.</li> <li>Dynamic: reflects all committed changes made to result set when scrolling over the cursor. This allows your    cursor to view the most recent changes, beneficial displaying real-time updates. But since the ordering    of data isn't constant you can use positioning reliably. </li> </ol>"},{"location":"Courses/fode/db_engines/","title":"Database Engines","text":""},{"location":"Courses/fode/db_engines/#database-engines","title":"Database Engines","text":"<p>Database Engines or Storage Engines are binaries which provides low level disk storage operations like storing and retrieving data from disk, providing compression over stored data, handling crash recovery, indexing over data, etc. When you run an DML query, the parse and optimizer translates the query into these low level operation which are then executed by engine.</p> <p>Other benefits of designing these engine is you don't have to start from scratch to build an DB system, which  supports your specific use case. You can build new features on top of existing engines. Some DB systems (like MySQL and MariaDB) even allows you to switch engines. Below are the most popular DB engines developed over time as requirement for applications evolved.</p>"},{"location":"Courses/fode/db_engines/#myisam","title":"MyISAM","text":"<p>MyISAM (which stands for Indexed Sequential Access Method) was default storage engine for MySQL in  its earlier days (before version 5.5). It became popular during early internet days due to its  simplicity (lightweight) and faster reads. But it didn't support essential features which are required by modern application like - No transactions, ACID making data storage unsafe and inconsistent. - Write operations locked entire table resulting in poor write performance in concurrent environmnet. - Didn't provide support for foreign key</p>"},{"location":"Courses/fode/db_engines/#innodb","title":"InnoDB","text":"<p>Developed to address the problems encountered by MyISAM, InnoDB became the default storage engine for MySQL since version 5.5 by providing essential features like ACID transactions, row level locking, crash recovery, foreign key constraints and more. It stores data in a B+ Tree Clustered Index around primary key.  </p>"},{"location":"Courses/fode/db_engines/#sqlite","title":"SQLite","text":"<p>A lightweight, serverless, self-contained SQL engine which became popular for being extremely lightweight, allowing it to be embedded into applications directly. There's no separate process running the DB in background, it's just your application process which reads and writes data from the file directly. Additionally, you don't need to configure or setup any additional step, just include the SQLite file. Since whole database is stored in single file, copying, versioning, deploying is very fast and simple. It's ACID compliant, works across  different platforms (like Windows, Linux, macOS). It's use across various popular apps like browsers, mobile apps, and IoT devices. However, due to this simplicity its not ideal for large datasets or high throughput writes.</p>"},{"location":"Courses/fode/db_engines/#berkeleydb","title":"BerkeleyDB","text":"<p>An embedded key\u2013value storage engine that provides ACID transactions, multiple indexing methods, and configurable concurrency control. It's extremely fast, reliable, and flexible, making it perfect for embedded systems, but it doesn\u2019t provide SQL and requires the application to handle schema and data logic.</p>"},{"location":"Courses/fode/db_engines/#leveldb","title":"LevelDB","text":"<p>LevelDB is a fast, embedded, single-threaded, key\u2013value storage engine using an LSM-tree design which allows fast sequential writes, high compression, lower write amplification and efficient storage layout.  Range or prefix queries over sorted key are much faster. Writes are optimized using LSM design while Reads are optimized using a combination of Memtable, SSTable and Bloom Filters. It's popularly used across application as Blockchain, Caching layers, and Web Browsers (much lighter than SQLite).</p>"},{"location":"Courses/fode/db_engines/#rocksdb","title":"RocksDB","text":"<p>Built as a fork from LevelDB, RocksDB introduced enhancements to provide low latency operation, higher write throughput optimized for SSDs and large scale production workloads. It's popularly used across systems like Kafka streams, Blockchains, Cockroach DB. You can check out the summary of major enhancements over its ancestor in following table.</p> Feature LevelDB RocksDB Concurrency Single-writer, limited readers Multi-threaded, many writers, parallel compaction Performance Good for small apps Extremely high throughput for large workloads Compaction Simple, single-threaded Multi-threaded, advanced, tunable compactions Transactions No transactions Full ACID transactions via WriteBatch + WAL Column families Not supported Supported (namespaces like MySQL tables) Tuning options Very few Hundreds of tuning knobs for memory, IO, compaction Memory optimization Basic Block cache, compressed cache, rate-limiting Storage types Disk only Optimized for SSD / flash, persistent memory Backup &amp; restore Manual Built-in APIs for backup, checkpoints, replication Use cases Small embedded apps Large-scale server apps, distributed systems"},{"location":"Courses/fode/db_index/","title":"B-Tree Index","text":""},{"location":"Courses/fode/db_index/#b-tree-index","title":"B-Tree Index","text":"<p>Indexes in DBs are used to speed up your read queries.</p> <p>Without Indexes, when you query for some records on your table, the DB would to sequentially search all the table pages and gather records matching your query. This internally involves many steps like loading the page from disk into memory and filtering the tuples, which is very inefficient and the time took grows linearly with the dataset. To optimize this, you can maintain a sorted order in your table which allows you to use Binary Search to cut the time taken to \\(O(log n)\\). But this sort of algorithm operates very poorly at disk level, since it requires you to load random pages causing lots of random I/O which under utilizes page buffer pool in memory. To minimize random disk I/O, computer scientists invented a balanced tree data structure where each node (DB page) stored multiple keys along  with pointers to multiple child nodes similar to below diagram.</p> <pre><code>graph TD\n\n    %% Level 0 (root)\n    R[\"[20 | 40]\"]\n\n    %% Level 1\n    A[\"[10]\"]\n    B[\"[30]\"]\n    C[\"[50 | 60]\"]\n\n    R --&gt; A\n    R --&gt; B\n    R --&gt; C\n\n    %% Level 2\n    A1[\"[5]\"]\n    A2[\"[12 | 18]\"]\n\n    B1[\"[22 | 25]\"]\n    B2[\"[32 | 35]\"]\n\n    C1[\"[45 | 48]\"]\n    C2[\"[55]\"]\n    C3[\"[65 | 70]\"]\n\n    A --&gt; A1\n    A --&gt; A2\n\n    B --&gt; B1\n    B --&gt; B2\n\n    C --&gt; C1\n    C --&gt; C2\n    C --&gt; C3</code></pre>  Use mouse to pan and zoom  <p>The tree consists of root node which is the starting point for search, branch nodes consisting of  keys and pointers to next level of nodes, and leaf nodes which consists of actual keys and respective data. With this you can store the root and branch nodes in memory most of the time as they're required for navigating through the tree and only evict the leaf nodes as required, essentially reducing disk I/Os required to find a key. The data structure is named as B+ Tree or commonly known as B Tree index. Additionally, the leaf nodes also have reference to their respective neighbouring leaf node which saves you the cost of traversing the whole tree in ordered to fetch a range of key.</p> <p>Let's look into how DBs use this B-Tree structure to perform different CRUD operations.</p> <ol> <li>Search: The search begins from the root node to respective leaf node by repeatedly comparing the search key    with keys in current node and moving to appropriate child node. For example, if search key is between two keys in    a node, move to the child pointer between them. Since the keys in each node are ordered, we can use binary search    to determine the key location in \\(O(logk)\\) time(k -&gt; number of keys in each node or the degree of B-Tree).</li> <li> <p>Insert: Find the leaf node responsible for holding the new key and place the key into the leaf node. If the node    is overfilled a page split occurs.</p> <ul> <li>Divide the leaf node into two leaf nodes.</li> <li>Add the middle key from leaf node into parent node.</li> <li>Update the pointers to child nodes in parent node.</li> </ul> <p>This process is cascading as page split in low level nodes can move upto higher level nodes which result in   lots of movement of data on disk than required, causing write amplifications and spikes in disk usage.  You can minimize this effect by using sequential index keys where the index is filled from left to right.</p> </li> <li> <p>Delete: Find the node containing the key to be deleted and removing the key. If the node has too few keys after    deletion, it must be restructured by either Borrowing a key from sibling node or Merging with a sibling node     which can cascade to higher level nodes potentially reducing the height of tree. </p> </li> <li>Update: It usually involves a search for the key and then performing a delete operation, followed by an insert     for the new value.</li> </ol> <p>To test it out and visualize these operation, you can use this  website.  </p>"},{"location":"Courses/fode/db_index/#secondary-index","title":"Secondary Index","text":"<p>But sometimes the table would also be queried using other fields which aren't primary key. In such cases you can develop a Secondary Index on such field, which basically creates same B-Tree Data structure with the  leaf nodes pointing to primary key or tupleId (in case of Postgres).  Few things to keep in mind when using Secondary Index</p> <ul> <li>Some DBs (like InnoDB), Secondary Indexes use primary key internally to locate the row. In such case, the size   of <code>pk</code> needs to be kept in check and choosing large <code>pk</code> can lead to larger secondary index essentially   degrading its performance.</li> <li>Each additional secondary index causes more write amplification, as the DB now have to update all these structures   which related incoming change in table. Postgres handles this a little differently by lazily updating the pointers   with a regular cleanup process, and in the meantime it'll just mark the pointed tuple data with respective change so that   DB can make the right decision.</li> <li>While <code>pk</code> is guaranteed to be unique, secondary index keys can be duplicate. Indexes perform best when the indexed    keys are selective, i.e. they map to small set of tuples. If a non-selective key is queried using index, the DB would   have to query all the pointers which are scattered across pages which could potentially lead to more I/Os. In such   cases, DB query optimizers decides to choose a less efficient execution plan like full table scans. Writes are similarly   influenced by non-selective indexes, as DB have to maintain the large list of pointers for each such operation.</li> </ul>"},{"location":"Courses/fode/db_index/#composite-indexes","title":"Composite Indexes","text":"<p>Queries which include multiple AND filters for search can hugely benefit from  Composite Indexes. Composite Indexes allows you to include more one keys in same index which are internally sorted and stored by concatenating them together from left to right. For example, index on <code>(A, B, C)</code> would store the key as <code>A_B_C</code>. The order in which columns are provided when creating composite index matters since you can still use the index to filter tuples by selectively using left most columns in provided order but same can't be done for rest of columns.</p>"},{"location":"Courses/fode/db_index/#uuid-in-b-tree-indexes","title":"UUID in B-Tree Indexes","text":"<p>UUID4 are completely random identifiers, two UUID4 Ids generated consecutively can never be ordered one after another. Using such completely random (like UUID4) values in B-Tree index are disastrous and should be avoided as they negatively impact both reads and writes.</p> <ul> <li>Writing random keys in B-Tree would result in frequent page split and fragmentation of data. This could be avoided   if we had a sequential key for index which would fill up the index entries sequentially from left to right.</li> <li>Reading requires loading whole pages into a shared buffer pool memory. If the buffer is full, DB will eliminate the    oldest page to load up the current page. And in case of UUIDs, we\u2019ll end up loading and removing page randomly    because we\u2019ve no ordering. Instead, if we had sequential key for index, we\u2019d have related pages which loaded up   into memory, so if let\u2019s say there's a surge in read for a category of products, they\u2019ll be present in same nearby   pages because their id are sequential. </li> </ul>"},{"location":"Courses/fode/db_index/#long-running-transactions-in-postgres","title":"Long-running transactions in Postgres","text":"<p>In Postgres, any DML transaction touching a row creates a new version of that row.  If the row is referenced in indexes, those need to be updated with the new tuple id as well.  There are exceptions with optimization such as heap only tuples (HOT) where all the index doesn\u2019t need to be  updated immediately but that only happens if the page where the row lives have enough space (fill factor &lt; 100%)</p> <p>If a long transaction that\u2019s updated millions of rows rolls back, then the new row versions created by this transaction (millions in my case) are now invalid and shouldn\u2019t be read by any new transaction.  You have many ways to address this, </p> <ul> <li>do you clean all dead rows eagerly on transaction rollback?</li> <li>Or do you do it lazily as a post-process?</li> <li>Or do you lock the table and clean those up until the database fully restarts?</li> </ul> <p>Postgres does the lazy approach, using <code>VACCUM</code> command which is called periodically to remove dead rows and free up space on the page.</p> <p>What's the harm of leaving those dead rows in?  It's not really correctness issues at all, in fact, transactions know not to read those dead rows by checking the state of the transaction that created them. This is however an expensive check, the check to see if the transaction that created this row is committed or rolled back. Also, the fact that those dead rows live in disk pages with alive rows makes an IO inefficient as the database has to filter out dead rows. For example, a page may have contained 1000 rows, but only 1 live row and 999 dead rows, the database will make that IO but only will get a single row of it. Repeat that and you end up making more IOs. More IOs = slower performance.</p> <p>Other databases do the eager approach and won\u2019t let you even start the database before rolling back completely, using undo logs. Both approaches have their pros and cons and at the end it really upto your workload which approach suits you best.</p>"},{"location":"Courses/fode/db_internals/","title":"Database Internals","text":""},{"location":"Courses/fode/db_internals/#database-internals","title":"Database Internals","text":"<p>At a high level, DB systems involves storing vast amount of data and providing an API for querying over the data efficiently. To solve this DBs uses various strategies and data structure, the majority of which revolves around these two data structure: Table and Index.</p>"},{"location":"Courses/fode/db_internals/#table","title":"Table","text":"<p>Table is a logical structure which defines how data is modeled and stored on disk. From outside, table is collection of rows(1) and columns(2) - usually representing an entity in your application. Internally, table is a collection of tuples (rows) organized across pages on one or more data files.</p> <ol> <li>unique record or instance of entity referenced by the table </li> <li>specifies the attribute or field of referenced row </li> </ol>"},{"location":"Courses/fode/db_internals/#tuple","title":"Tuple","text":"<p>Tuple is a structured block of bytes stored inside a page, the physical realization of a row. It's composed of header and data field, with following format roughly,</p> <p><code>| tuple header (visibility, length, flags, etc.) | col_1 value | col_2 value...|</code></p> <p>Within the page, it's paired with a line pointer which is generated by the byte offset of Tuple relative to the start of page, similar to following:</p> <pre><code>[Page]\n...\n \u251c\u2500\u2500 Line pointer \u2192 Tuple #1 (offset 40)\n \u251c\u2500\u2500 Line pointer \u2192 Tuple #2 (offset 120)\n |\n...\n |\n [ ... Tuple Data Area ... ]\n</code></pre> <p>With this, DB can globally identify each tuple physically within a table using following combination known as  Tuple ID -&gt; <code>(PageNumber, LinePointer)</code>. For example, <code>TID -&gt; (PageNumber=42, LinePointer=2)</code> would read the page number \\(42\\) into memory, lookup the value of line pointer \\(2\\) and fetch the data pointed by line pointer. TID allows DBs to reference rows efficiently and uniquely across various data structures like indexes.</p> <p>When designing a DB system, there are two broad choices based on how this TID mapping to physical location is utilized:</p> <ol> <li> <p>DBs (like Postgres) where table is physically stored as a heap (1) uses <code>(PageNumber, LinePointer)</code> directly    since the rows are unordered. </p> <ol> <li>Unordered pages of tuple</li> </ol> </li> <li> <p>DBs (like MySQL InnoDB) which uses clustered indexes (1), lays the physical location of tuple using primary key.    Due to this, the  TID mapping mentioned above can't be utilized directly.     You've to use primary key to get the physical location, which internally uses mapping similar to TID managed by      storage layer.</p> <ol> <li>Tuples are stored inside the index, whose key determines the physical and logical location of tuple.</li> </ol> </li> </ol> Tradeoff <p>You get the following summarized tradeoff based on this decision choice:</p> Aspect PostgreSQL (Internal TID) MySQL/InnoDB (External RowID) Row addressing Physical (page, slot) Logical (primary key) Update behavior (MVCC) New version = new TID (row moves) Row stays; old versions kept in undo log Index maintenance Indexes point to TIDs \u2192 need update when TIDs change Indexes point to PKs \u2192 stable, fewer updates Lookup cost (secondary index) One hop (index \u2192 heap) Two hops (secondary \u2192 primary \u2192 data) Insert performance Fast (append to heap) Slower (must maintain clustered order) Range scan performance Slower (heap unordered) Faster (rows ordered by PK) VACUUM / cleanup Required to reclaim old tuples Handled via purge of undo logs Storage flexibility Simple, flexible heap More rigid due to clustering"},{"location":"Courses/fode/db_internals/#page","title":"Page","text":"<p>All data structures in DBs (like Tables, collections, rows, columns, indexes, sequences, documents)  end up as bytes in a page. This model allows you to decouple the storage engine from the DB frontend which is  responsible for formating the data and provide an API over it.  A DB page is the fundamental unit of I/O and storage inside a DB engine. It\u2019s the smallest chunk of data the DB reads from or writes to disk or caches in memory.</p> <p>But why do DBs use fixed size blocks for its read/write operations? This is due to the way disk storage (HDDs/SDDs) work and how they're different from memory (RAM). </p> <p>Note</p> <p>Physical Disk operates at the level of sectors (HDD) or blocks (SDD) which allow you to persist fixed chunks of data even in absence of power. Working with disk is abstracted by OS using LBA API and then File System API, which defines fixed size chunks known as Pages (usually \\(4\\) KB) as smallest unit to read and write from disk (defined as an I/O).</p> <p>So working with disk requires you to use fixed size blocks of storage known as pages. To be not confused with OS Pages, DBs uses their own logical page definition. This approach offers several key advantages which are requirements for DB systems:</p> <ol> <li>Separation of Concern, OS is designed to manage generic files and memory, while DBs needs exact control on    how data is laid out, cached, logged, and recovered. Using its own Page abstraction would allow DB system    to use all such optimize optimization.</li> <li> <p>OS Page size defaults to \\(4\\) KB which works for many applications, but is inefficient of DBs    since the space acquired by metadata would out weigh the useful information available. Today, DBs uses    page size ranging from (\\(8\\)-\\(16\\) KB) optimized for their workload.</p> Small vs Large DB Pages <ul> <li>Small pages are faster to read and write especially if the page size is closer to the media block size.    However, the overhead cost of the page header metadata compare to useful data can get higher for smaller pages.</li> <li>Larger pages can minimize metadata overhead and page splits but at the cost of higher cold read/write.</li> </ul> </li> <li> <p>This allows DBs to implement their own page cache/buffer pool as they already know which pages are \"hot\"(1) and want to     manage them efficiently. </p> <ol> <li>When the required page is already present in memory/cache.</li> </ol> </li> <li>You can consistently port your data to different OS platform, which make replication and backup operations simpler.</li> </ol> <p>A simplified page structure on disk looks like as follows:</p> <pre><code>+----------------------------------------------------+\n| Page Header (metadata)                             |\n|----------------------------------------------------|\n| Line Pointer Array (Item IDs / Slot Directory)     |\n|----------------------------------------------------|\n|                    Free Space                      |\n|----------------------------------------------------|\n| Tuple Data Area (actual rows/tuples)               |\n|----------------------------------------------------|\n| Special Space (optional, for indexes)              |\n+----------------------------------------------------+\n</code></pre> <ul> <li>Page Header is a fixed size bytes which stores metadata like Page LSN (Log Sequence Number) -&gt; for WAL consistency,   checksums/CRC -&gt; corruption detection, and various flags and pointers.</li> <li>Line pointer array stores offset of tuple within the page along with other metadata like tuple length or whether   the tuple is dead or redirected, etc. These pointers grow downward from the header, as such the deleted/updated   tuples are marks respectively without moving other tuples immediately.</li> <li>Tuple Data Area, where actual data is stored. It consists of tuple headers (like visibility info, transaction IDs) and    column data. Tuple Data grows upward from bottom of page.</li> </ul> <p>Note</p> <p>Space occupied by dead/redirected tuples are reclaimed later by cleanup operations like <code>VACCUM</code> or compaction.  </p> Role of Page Layout in forming different DB domains <p>The internal page layout and tuple organization fundamentally define what kind of database it is:</p> <ul> <li>Row based DBs stores complete tuples (rows) in each page. This gives you access to all columns of a row in same page,   which is ideal for OLTP workloads involving frequent read/writes over entire rows.</li> <li>Columnar DBs organizes each page to store data for individual columns across many rows. Since columns are   contiguously stored, such DBs are ideal for OLAP workload which involves reading few columns across large number of rows.</li> <li>Document DBs organizes their data in self-contained object (JSON/BSON) of variable length known as documents. This   model provides flexible schema and faster document level read/writes.  </li> <li>Graph DBs models their data as nodes and relationships (edges). Each page stores either nodes, relationship or property   records, where the relationship records has direct pointers to start and end nodes, making sequence traversal fast.</li> <li>Key Value Stores (Sorted Key Pages) like RocksDB, LevelDB stores sorted key-value pairs.    The storage engine (often an LSM tree) manages multiple sorted runs which makes it ideal for efficient for range    scans and sequential writes.</li> </ul>"},{"location":"Courses/fode/db_internals/#index","title":"Index","text":"<p>Another important data structure which plays crucial role in working of DBs is Index. Logically index is a data structure which allows database to quickly locate rows without scanning the entire table. Physically, they\u2019re just files made up of fixed size pages consisting of index entries. The structure of these entries (and how pages are connected) defines the index type. Among which the most common type is B-Tree (B+Tree), which almost all general-purpose databases (Postgres, MySQL, Oracle, SQL Server, SQLite) use as standard indexes.</p> <p>Note</p> <p>To be not confused with B-Tree and B+Tree, modern DBs entirely uses B+Tree implementation, but you'll still find people using B-Tree naming convention from place to place.</p> <p>At a high level, B+Tree index is a hierarchy of pages consisting of Root, Internal and Leaf pages (or nodes). The Root node is entry point for lookup, points to Internal nodes consisting of indexed keys -&gt; child node mapping. At the bottom, we've Leaf nodes which consists of the index key -&gt; TupleID mapping. Each node stores sorted list of keys and pointer which allows you to perform Binary Search on it to find the  respective key in \\(O(logn)\\) time. Also, the leaf nodes are doubly linked to adjacent nodes, which allows you faster range queries.</p> <p>Some DBs (like InnoDB) decided to store the data tuple within index itself instead of using TID. Such indexes are known as Clustered Index, as they cluster the table within index itself. Since data is stored directly in index, the I/O cost for lookup is usually 1-page read which is faster compared other its counterpart. You also get faster range queries since the rows would live within same page if the keys are sequential. But this design is severally impacted inserts, since you must keep rows physically ordered by key which can cause page splits and  random I/O if the indexing key is random. Also, secondary Indexes point to primary key in clustered index. If you primary key is modified, all your secondary index needs to be updated. You also need to be aware of the primary key size, which can impact the secondary index size and performance.</p> <p>To optimize index performance, try to keep the index size as small as possible, since we need to load the index pages into memory before working with them. Smaller index entries would allow us to pack more information per index page, and if such information is user managed -&gt; just be mindful about the size of user defined field stored in such entries.</p>"},{"location":"Courses/fode/db_partition/","title":"Database Partitioning","text":""},{"location":"Courses/fode/db_partition/#database-partitioning","title":"Database Partitioning","text":"<p>As your table size grows, querying data from it becomes more and more slower. Even the index would grow larger,  making its navigation slower. At this point, the best way to optimize your queries is by dividing your table into smaller datasets which are operable independently. One of the way to do this is using Partitioning, where you divide your table into smaller tables (partition) based on a partitioning key, which map the rows to their respective partition.  </p> <p>For example, we to execute the following query -&gt; <code>SELECT name FROM Customer WHERE Id=655970</code> . In original table, DB have to sequentially scan the whole table to fetch this row. But if we partition our table on <code>Id</code> as following figure </p> <pre><code>flowchart LR\n    A[Customer Table] --&gt; B[Partition 1&lt;br/&gt;Id: 1 - 200,000]\n    A --&gt; C[Partition 2&lt;br/&gt;Id: 200,001 - 400,000]\n    A --&gt; D[Partition 3&lt;br/&gt;Id: 400,001 - 600,000]\n    A --&gt; E[Partition 4&lt;br/&gt;Id: 600,001 - 800,000]\n    A --&gt; F[Partition 5&lt;br/&gt;Id: 800,001 - 1,000,000]\n\n    class B,C,D,E,F partition;</code></pre>  Use mouse to pan and zoom   The DB would directly jump to Partition 4 since it knows that <code>Id</code> -&gt; \\((600k,800k)\\) belongs to that partition and here it\u2019ll only have to scan through 200k rows at max. <p></p> <p>The above example is known as Horizontal Partitioning where we slice table along the rows. There\u2019s a lesser popular version of partition known as  Vertical Partitioning which slices table along column,  which you can use to slice column which are larger and lesser frequently accessed (like a blob). This allows you keep access to rest of the columns quicker, as same page could now fit more rows.</p> <p>However, efficiently utilizing partitions might not be as straight forward as implementing it. You need to have an understanding over you data and queries, like frequently accessed data which could cause  hot partitions, and determining queries which are crucial for performance. And depending on these factors, decide the right partitioning strategy like:</p> <ul> <li>partitioning on a range of keys, (like dates or IDs) which is used ideal for data with natural ordering</li> <li>partitioning on a list of keys, (like region, category) which is ideal for discrete predefined data.</li> <li>partitioning on a hash of keys, to evenly distribute the data, reducing hotspots and improving performance</li> </ul> <p>Along with it, you need to set up plans for maintenance and evolving partitions like automating partitioning creating in case of range partitioning, and monitoring the performance and usage of each partition to identify the imbalance and address them effectively. Following table briefly summarizes the pros and cons of using partitioning.</p> Advantage Disadvantage Improves query performance when accessing a single partition as it\u2019ll have lot less rows then original. Updates which moves row from one partition to another are slower. Improves Sequential Scan and Index Scan as both underlying data structure are a lot less in size compared to original. Inefficient Queries could scan all partitions if not used properly. This is a lot more slower Easy to bulk import data by attaching partition Schema changes can be challenging. Archieve old data into seperate partition which can use cheaper storage."},{"location":"Courses/fode/db_security/","title":"Database Security","text":""},{"location":"Courses/fode/db_security/#database-security","title":"Database Security","text":"<p>Data is a critical part of every business which should be handled responsibly for successfully running a business. Security plays a vital role in managing such data by using set of policies and controls to protect data from unauthorized access, misuse, alteration or destruction ensuring only the right user can access the right data. Key goals of security in DB involves:</p> <ul> <li>Keeping data confidential, by protecting it from unauthorized access</li> <li>Maintaining data integrity, by controlling access so that it can't be altered by just anybody.</li> <li>Ensuring data is available whenever needed, it shouldn't be lost on accidents or crashes.</li> </ul> <p>Few components which helps DB achieve these goals</p> <ul> <li>Access Control: Control who can access what data and how they can use it by using proper authentication and   authorization. For example, only admins should've access to creating and deleting tables.</li> <li>Encryption: Data should be protected from 3rd parties by using safe encryption at both rest and transit to    keep them confidential.</li> <li>Backup &amp; Recovery: Keeping regular backups and redundant copies can prevent data loss due to failure, or crashes.</li> </ul> <p>REST apps sometimes requires database tables to be present, and it\u2019s usually covered with the startup  of app to create the table if not present. This is a bad practice and should be avoided, because when your app users are interacting with your database \u2014 they\u2019ll have full privilege to your  DB which can cause serious harm like SQL Injections or XSS attack to drop the table.  Instead, you should keep separate users for creating tables, schema, etc. and have separate users for read/write permissions. You can also maintain separate connection pools in client side for each of these  read/update/delete operations.</p>"},{"location":"Courses/fode/db_security/#homomorphic-encryption","title":"Homomorphic Encryption","text":"<p>Encryption is transforming data into random text which doesn\u2019t make sense when looked at.  To make the sense out of it, you\u2019ll have to decrypt the random text.  You can do this in two different way, by using a common key (symmetric encryption) for both encryption and decryption or by using separate keys (asymmetric encryption) for encryption and decryption.</p> <p>In database systems, we can\u2019t simply encrypt the data because of few reasons - queries need plain data to perform their work. To work with encrypted data, it\u2019ll have to decrypt it first    everytime which isn\u2019t optimal. - Analysis of data, indexing, tuning the data needs plain text - application needs recognizable data to process it. - Layer 7 reverse proxies terminates TLS connections so that it can read the traffic and apply   routing rules on it.</p> <p>Homomorphic Encryption allows you to do all these operations on encrypted data by allowing  Arithmetic operations on encrypted data. You can use indexes and query on encrypted data using these Arithmetics because at low level each of these operations are simply performed by comparison, shifting bits, adding, etc. However, it\u2019s still in PoC as actual querying is too slow for production system.</p>"},{"location":"Courses/fode/db_shard/","title":"Database Sharding","text":""},{"location":"Courses/fode/db_shard/#database-sharding","title":"Database Sharding","text":"<p>Another way to divide you dataset into smaller chunks is using Shards. It divides the table into smaller tables  similar to partitioning, but the key difference is each table lives in a separate DB instance with sharding. Using separate DB instance would provide u additional benefits like more resources (CPU, memory) for each sharded table, or network advantage by geographically locating the shard closer to client, or provide specific security standards for specific group of users. </p> <p>You can create shards based on keys like, zipcode which represent an area geographically, or using range of values on some number field. But what if your have to shard on some random text? Mostly shard keys which can\u2019t be distinguished into groups are grouped using Consistent Hashing.</p> Consistent Hashing <p>Consistent hashing is an algorithm which distributes different shards as points on a ring like data structure and the range of values which falls on a pie of the ring belong to a single shard. To map values to ring, you\u2019ve different hash function which evenly distributes the shard keys so that no  single shard is overused.</p> <p>Following are few key advantages and disadvantages of sharding summarized briefly.</p> Advantage Disadvantage You get scalability in data, memory, CPU, etc Makes the client complex as it needs to be aware of each shard. You get smaller tables and as such smaller indexes Transaction across shard wouldn\u2019t be atomic anymore. You get security as data can live in separate database instances for users which require most secure storage. Rollbacks would be expensive Schema changes are hard The query must know which shard to hit, otherwise the client would\u2019ve to search every database. <p>So when should you use sharding to optimize your DB performance? Sharding should be your last option for optimizing your database performances due to the complications involved  which most of the time are unnecessary. There are many other optimization trick you can do before ultimately using sharding</p> <ul> <li>You can look into horizontal partitioning before it which allows you to divide your table into smaller   chunks and provide smaller index for all these partitions.</li> <li>Then, if your reads are slow, you can look into replication \u2014 where you can employ multiple read replicas    to distribute the load over a single server. </li> <li>If you\u2019re facing slow writes, maybe try to separate servers based on regions.</li> </ul> <p>Sharding does provide you with scaled writes and read, but you\u2019ll have to leave behind features like ACID transactions. Also, the client coupling to database is strong since client needs to be aware of each shard. And resharding or changing business logic also becomes complicated. So avoid using it until its absolutely  necessary.</p> <p>Vitess</p> <p>To avoid coupling you can transfer sharding client logic to a backend app which will handle which shard  should the query be directed to. One such backend app is Vitess.</p>"},{"location":"Courses/fode/extra/","title":"Extra Topics","text":""},{"location":"Courses/fode/extra/#extra-topics","title":"Extra Topics","text":""},{"location":"Courses/fode/extra/#index-selectivity","title":"Index Selectivity","text":"<p>You want to index column which provides as few rows when filtered through.  For example, we index a column which stores Gender which can have 3 values,  and filtering any gender would result in massive result set. So instead database would go for heap scan.  </p>"},{"location":"Courses/fode/extra/#postgres-tupleid","title":"Postgres TupleId","text":"<p>All indexes in Postgres point to a tupleId which is used as the key to cluster around the table. Whenever you make any update, a new tupleId will be generated for the same row. Due to this Postgres needs to update all the indexes pointing to this row with old tupleId to new tupleId,  which isn\u2019t optimal when we\u2019ve a lots of indexes. Still it tries to optimize this by using  Heap Only Tuple (HOT) optimization where it\u2019ll immediately update the tupleId in index of the updated column. But this could still cause issue for index pointing to old tupleId. This is resolved by storing some metadata on old tupleId page, which points to the latest tupleId of this row only if the new tupleId is on same page.  This can be used to advantage by using the fill factor configuration which tells the limit upto which a page can be filled to leave some space for updates and inserts. </p>"},{"location":"Courses/fode/extra/#wal-redo-and-undo-logs","title":"WAL, Redo and Undo Logs","text":"<p>Logs help DBMS to ensure durability and crash recoverability. Whenever you commit writes, it must be persisted  by database. It basically means the data must be present even after shutting of the DBMS.  One way to do this is to directly flush the changes to disk after each commit, but this way your commit operation will become slower as writing to disk is heavy (because writes to disk are in whole pages,  not individual bytes). Instead, databases, keep changes in memory and marks the pages as dirty to indicate the  page has been updated. This approach is fast but can compromise with the persistence of data. To be 100% sure with persistence, database instead maintains logs contains each of these changes as a tiny delta which are  appended to the end of the log file and can be replayed to update the state of database. This log is called Write Ahead Log (WAL). WAL can\u2019t grow infinitely large, so after it grew to some size we\u2019ll flush the changes in WAL to disk and clear our WAL since it no longer needs to maintain the logs. And we\u2019ll restart again. This flushing is of changes to disk is called checkpointing. Checkpointing operation are very heavy  operations as it includes a lots of IOs and compute operations which can cause spike in systems resource usage and impact its performance. To make this tolerable we can make checkpoints smaller so that we can flush to disk frequently, but not smaller enough to impact the writes operation.  </p>"},{"location":"Courses/fode/extra/#endurance-of-ssds","title":"Endurance of SSDs","text":"<p>SSDs store data within page present in fixed blocks. There\u2019s no mechanical apparatus like hard disk which makes them faster compared to them. However, SSDs can update certain page until a limit after which the bytes are no longer usable essentially reducing the size of SSD. Due to this workload involving updates aren\u2019t considered good for SSDs. For example, B-Tree indexes which restructure themselves are considered bad for SSDs but index like LSM-Tree which only appends entries are considered optimal for SSDs.</p>"},{"location":"Courses/fode/extra/#postgres-architecture","title":"Postgres Architecture","text":"<p>Postgres is a SQL row-based database which follows MVCC storage model where each row can have multiple  physical version on disk with the last version is the latest. Basically every insert/update/delete operation create a newer tupleId of row indicating the current version (lookup pros and cons of this decision). Then it uses processes instead of threads for work. Let\u2019s discuss all the process below: </p> <ul> <li>Post master: first process to spawn on startup, acts a parent process for all other processes,   works at listener to connect external application over network. Every other process is forked from this process.</li> <li>Backend Processes: each client connection receives its own backend processes to receive and process the    request. This is a bad choice as processes requires more memory and CPU context switching which impacts   the performance. But postgres avoid this by offloading most of the work outside these processes.</li> <li>Shared Memory: Or shared buffer pool where most of the data which is shared among processes are present like   WAL records, pages, etc.</li> <li>Background workers: backend processes uses these workers to outsource most of its work like querying   based on the generated plan. If a parallel plan is needed, these background workers will be picked up and   assigned respective work.</li> <li>Auxiliary Processes: </li> <li><code>bw</code> (background writer) wakes up periodically,and write pages from shared memory to disk to free up the memory.</li> <li><code>cp</code> (checkpointer) directly flushes the WAL records and pages to disk and creates a checkpoint which      indicates that data uptil this point is consistent. </li> <li><code>lg</code> (logger) is used to writes logs. </li> <li><code>avl</code> (Auto vacuum launcher) launches autovacuum workers. </li> <li><code>wa</code>(WAL archiver) responsible for backing up WAL records</li> <li><code>wr</code> (WAL receiver) runs on replica to replicate data from WAL records</li> <li><code>ww</code> (WAL writer) writes record to WAL and flush them to disk.</li> <li><code>st</code> (startup process) which is actually the first process to start whose role is to check if the pages     are consistent with WAL records, if not mark them as dirty pages. As this needs to be done before any     client connects to database, it should be the first process to start.</li> <li>AutoVacuum Workers: Periodically Vacuums the database which essentially means it frees up old tupleId   which are no longer required by any transactions. Vacuum includes much other stuff you can explore online.</li> <li>WAL Senders: responsible for sending WAL records from client to replicas.</li> </ul>"},{"location":"Courses/fode/extra/#table-joins-using-hash-tables","title":"Table Joins using Hash Tables","text":"<p>In simple terms one relation is mapped to another using key and value as respective column and foreign key. Usually the column with less value is picked up as key, as the hashtable will be smaller than. To map the value to foreign key \u2014 you fetch the foreign key row, and the row from source table with same foreign key, map it in hash table and finally u can use it to join the tables. </p>"},{"location":"Courses/fode/extra/#storage-of-null-value-in-database-systems","title":"Storage of NULL value in database systems","text":"<p>NULL in database system indicate that the given place isn\u2019t allowed to store data. Postgres uses a null bitmap in front of every row to indicate if the given column is null. The size of this bitmap starts with 8bits for 8 column each and with more columns it increments with 8 bytes  in size (so 9-64 column would use another 8 bytes). This helps you save space on disk, as you no longer have to persist the column for null value and allows you to fit more rows within same page. However, be careful when working with NULLs as they\u2019re widely inconsistent:</p> <ul> <li><code>SELECT COUNT(FIELD)</code> would ignore counts of row having null value in given field but    <code>SELECT COUNT(*)</code> would give you correct count.</li> <li>You can\u2019t compare null value, you can just check if It's null or not null. You can\u2019t use <code>T in [NULL]</code></li> <li>Not all database support NULLs in indexes, check if beforehand.</li> </ul>"},{"location":"Courses/fode/extra/#write-amplifications","title":"Write Amplifications","text":"<p>Basically when the actual work done for write is much more than the logical work required.  For example, postgres creates new row even for every update/delete (for versioning). Now this new row with new tupleId must be updated in all existing index on this table.  This is somewhat optimized by updating the index of columns whose value had been changes and rest of untouched column index can be updated in background. Those old rows will now also point to this latest version known as heap only tuple (HOT), because these version must be present in same page which is managed by using fill factor configuration. Then you\u2019ve WAL writes to achieve durability. All these amplification happens at database level where the database system is responsible. </p> <p>SSDs Disks/Storage also causes write amplifications. SSDs uses charge entrapment where electrons are trapped in different levels on a atom. The way these electrons are trapped can be used as a way to store information in them and this configuration of electrons isn\u2019t lost even in absence of electricity.  These cells are then arranged into rows which are then arranged into pages and pages into blocks. For Database Systems, we just need to be aware of page and block level. </p> <ul> <li>Writes to disk are on page level, and for SSDs you can simply write data to new pages easily.</li> <li>When you want to update the data on a already existing page, you\u2019ll write the data to a new page and mark the existing page as stale.</li> <li>Finally, you can\u2019t clean single pages in SSDs, you\u2019ve to clean the whole block to free up space.</li> </ul> <p>To clean up blocks with both stale and active pages, SSDs have a garbage collection program which moves active data to new block and then clean the block to free up storage. All these processes due to update requires additional work in SSDs causing write amplification.</p>"},{"location":"Courses/fode/indexing/","title":"Working with Indexes","text":""},{"location":"Courses/fode/indexing/#working-with-indexes","title":"Working with Indexes","text":"<p>This page will focus on how indexes works in DBs by demonstration using Postgres.  We'll be using following Employees table with given schema</p> <pre><code>\\d employees\n                            Table \"public.employees\"\n Column |  Type   | Collation | Nullable |                Default                \n--------+---------+-----------+----------+---------------------------------------\n id     | integer |           | not null | nextval('employees_id_seq'::regclass)\n name   | text    |           |          | \nIndexes:\n    \"employees_pkey\" PRIMARY KEY, btree (id)\n</code></pre> <p>By default, Postgres builds B-Tree index around <code>pk</code>. To look around how a query performs,  you also get <code>EXPLAIN ANALYZE</code> command which provides the query plan along with costs and other important information.</p> EXPLAIN ANALYZE <p>Postgres provides you two commands, <code>EXPLAIN</code> and <code>EXPLAIN ANALYZE</code> to understand how the query planner executes  (or intends to execute) a SQL query. It shows info like predicted plan steps (e.g., Seq Scan, Index Scan, Hash Join) estimated costs (<code>cost=...</code>), row counts, and row widths. </p> <ul> <li><code>EXPLAIN</code> shows the query execution plan without actually running the query.</li> <li><code>EXPLAIN ANALYZE</code> executes the query and shows the actual query plan, execution time, and other actual figures.</li> </ul> <p>Let\u2019s look around how different queries behave when working with index in Postgres. The actual figures might vary when executing same query due to optimizations in between like caching.</p> <ol> <li> <p><code>SELECT</code> \u2192 indexed field, <code>WHERE</code> \u2192 indexed field (ignore the example query, it's just for demo purpose) </p> <p></p><pre><code>explain analyze select id from employees where id = 2000;\n                                                          QUERY PLAN                                                           \n-------------------------------------------------------------------------------------------------------------------------------\n Index Only Scan using employees_pkey on employees  (cost=0.42..4.44 rows=1 width=4) (actual time=0.031..0.033 rows=1 loops=1)\n   Index Cond: (id = 2000)\n   Heap Fetches: 0\n Planning Time: 0.332 ms\n Execution Time: 0.072 ms\n(5 rows)\n</code></pre> DB decides to use Index Only Scan because we\u2019ve an index on <code>id</code> which is used for filtering.    Heap Fetch are 0 because the field we\u2019re fetching is present in index itself, so we didn\u2019t have to go to heap.<p></p> </li> <li> <p><code>SELECT</code> \u2192 non-indexed field, <code>WHERE</code> \u2192 indexed field</p> <p></p><pre><code>explain analyze select name from employees where id = 50000;\n                                                        QUERY PLAN                                                        \n--------------------------------------------------------------------------------------------------------------------------\n Index Scan using employees_pkey on employees  (cost=0.42..8.44 rows=1 width=6) (actual time=0.042..0.044 rows=1 loops=1)\n   Index Cond: (id = 50000)\n Planning Time: 0.092 ms\n Execution Time: 0.152 ms\n(4 rows)\n</code></pre> <code>Index Scan</code> \u2192 for identifying the rows, then we\u2019ve to go to heap for fetching the <code>name</code> field in <code>SELECT</code>.<p></p> </li> <li> <p><code>SELECT</code> \u2192 indexed field, <code>WHERE</code> \u2192 non-indexed field</p> <pre><code>explain analyze select id from employees where name = 'P7o';\n                                                       QUERY PLAN                                                       \n------------------------------------------------------------------------------------------------------------------------\n Gather  (cost=1000.00..11310.94 rows=6 width=4) (actual time=0.729..52.861 rows=1 loops=1)\n   Workers Planned: 2\n   Workers Launched: 2\n   -&gt;  Parallel Seq Scan on employees  (cost=0.00..10310.34 rows=2 width=4) (actual time=11.900..27.233 rows=0 loops=3)\n         Filter: (name = 'P7o'::text)\n         Rows Removed by Filter: 333333\n Planning Time: 0.105 ms\n Execution Time: 52.886 ms\n(8 rows)\n</code></pre> <p>Postgres will check if we\u2019ve an index on the <code>WHERE</code> clause, if not we\u2019ve to perform a Parallel Seq Scan    (Full Table Scan). Still it tries to optimize this by using multiple worker.</p> </li> <li> <p>Let\u2019s index our name field and filter using a pattern match</p> <p></p><pre><code>create index employees_name on employees(name);\nexplain analyze select id from employees where name like '%P7o%';\n                                                       QUERY PLAN                                                       \n------------------------------------------------------------------------------------------------------------------------\n Gather  (cost=1000.00..11319.34 rows=90 width=4) (actual time=0.350..73.237 rows=11 loops=1)\n   Workers Planned: 2\n   Workers Launched: 2\n   -&gt;  Parallel Seq Scan on employees  (cost=0.00..10310.34 rows=38 width=4) (actual time=6.337..42.018 rows=4 loops=3)\n         Filter: (name ~~ '%P7o%'::text)\n         Rows Removed by Filter: 333330\n Planning Time: 0.344 ms\n Execution Time: 73.324 ms\n(8 rows)\n</code></pre> Even though we\u2019d an index on <code>name</code> the DB couldn\u2019t use it because we\u2019re filtering for a pattern and not exact value. Since the pattern could fit multiple value, DB decides its more efficient to perform sequential scan.<p></p> </li> </ol>"},{"location":"Courses/fode/indexing/#different-execution-plans-in-postgres","title":"Different Execution Plans in Postgres","text":"<ul> <li> <p>Sequential Scan (or Full Table Scan): When Postgres goes directly to the heap to fetches the query results.    This can be normally identified when query is using no filtering or filtering using field without index.   But an unexpected scenario where this could happen is when it expects the query to bring a lots of rows even   if we\u2019ve filtering using indexed field. For example, <code>id!=10</code> will result only in single row so why go to   index to fetch <code>id!=10</code> when its more efficient to just fetch the rows from heap and discard <code>id=10</code> </p> <pre><code>explain select name from grades where id!=10; \n                        QUERY PLAN                        \n----------------------------------------------------------\n Seq Scan on grades  (cost=0.00..10.26 rows=500 width=15)\n   Filter: (id &lt;&gt; 10)\n(2 rows) \n</code></pre> </li> <li> <p>Bitmap Index Scan: Postgres first creates a bitmap (bits where the position of bit indicates the page    number) for pages in heap and then scans the index to set bits which satisfies the condition.    After completing this, all the pages with set a bit in bitmap are fetched in one go. This is usually performed   when we don\u2019t have lots of rows which proves sequential scan efficient but enough rows that requires fetching   in bulk.     </p><pre><code>explain select name from grades where g = 25;\n                              QUERY PLAN                               \n-----------------------------------------------------------------------\n Bitmap Heap Scan on grades  (cost=4.18..8.43 rows=4 width=15)\n   Recheck Cond: (g = 25)\n   -&gt;  Bitmap Index Scan on grades_g  (cost=0.00..4.18 rows=4 width=0)\n         Index Cond: (g = 25)\n(4 rows)   \n</code></pre>   Fetching rows from page is performed in Bitmap Heap Scan which discard rows which doesn\u2019t satisfy the    query condition. Similar to this, there\u2019s BitmapAnd and BitmapOr Scan which are used when using   more than 2 indexed fields for filtering. In which case, Postgres would develop Bitmap for both the   filters separately and then merge them into one using AND/OR operation depending on filtering condition.<p></p> </li> <li> <p>Index Scan: Postgres mostly uses index whenever we\u2019ve a filtering criteria which uses indexed field.   But it also depends on the amount of data we\u2019re fetching, like previously discussed in Full Table Scans.   If we\u2019ve lots of rows its usually inefficient to fetch results from Index and then fetch the rows from heap.</p> </li> <li> <p>Index Only Scan: Postgres performs this when it can fetch the information asked by query from index itself   (without going to heap). This is more efficient than index scans and can be made useful by adding   a non-key column to the index. For example,     </p><pre><code>create index grades_g on grades(g) include (name);\n</code></pre>     We\u2019ve included column <code>name</code> as non-key to index <code>grades_g</code> where <code>g</code> is the key column.   So all the filtering will be performed on key column, but we can also fetch non-key column from index directly.    However, beware of including a non-key column to index, as it\u2019ll increase the size of index and    will certainly impact on the cost for querying the index (as we\u2019ve to load more pages for same index).<p></p> </li> </ul>"},{"location":"Courses/fode/indexing/#when-does-db-use-index","title":"When does DB use Index?","text":"<p>Suppose we\u2019ve a table <code>T</code> with index on column <code>t1</code> (<code>idx_t1</code>) and index on column <code>t2</code> (<code>idx_t2</code>). How will database plan for following query: <code>select * from T where t1=1 and t2=4</code> ?</p> <ul> <li>If we\u2019ve lots and lots of rows, the optimizer will go ahead with full table scans and filter out data from there.</li> <li>If we\u2019ve very few rows, the optimizer will use a single index to fetch the intermediate rows and   filter out from them based on the other columns condition. Which index is used to fetch intermediate rows?   The one which yields lesser rows in case of AND operation.</li> <li>If we\u2019ve good enough rows (not too few or too many), we\u2019ll develop bitmaps from both the indexes and   BitmapAnd then to get final bitmap to scan the heap.</li> </ul> <p>This decision about how many rows will turn up in a query is estimated based on statistics  which are precalculated by DB for each table. So always remember to update stats on tables before performing any critical operation.</p> <p>You can also force database to use certain index by hinting it in query.  For example, <code>select * from T where t1=1 and t2=4 /*+ index[t1 idx_t1] */</code></p> <p>Above case study also hints on what kind of column you should create index on to have maximum efficiency. For example, if we\u2019ve a column <code>state</code> and most of the rows use a same value for <code>state</code> creating an index on it won\u2019t help with searching your query because you\u2019ll have so many rows with same state that its much more efficient to just perform a sequential scan. </p> <p>Create Index Concurrently</p> <p>Most databases blocks writes when creating an index and this could impact live production system.  To solve this issue, postgres provide this feature to create index concurrently without stopping write in b/w the process. Command: <code>create index concurrently &lt;index-name&gt; on &lt;table&gt;(&lt;column&gt;)</code> It\u2019d essentially create index sequentially and before exiting it\u2019d wait for all ongoing transactions to complete so that they\u2019ve been accounted for within the index.   </p>"},{"location":"Courses/fode/indexing/#bloom-filters","title":"Bloom Filters","text":"<p>Take a case where we need to query username to check if it\u2019s been already taken by another user or not.</p> <p>Directly querying the DB for presence of username is very slow if we\u2019ve a lots of users signing up. Instead, we can use intermediate cache to store username already taken up and query from these, but this approach doubles our memory footprint.</p> <p>To resolve this issue, we can use bloom filter which is essentially a fixed size bitmap on which set bit indicates the possibility presence of that bit number and unset indicates its absence  (the index of bit we need to check for a username can be found by using a hashing function % size of bitmap). With this we can easily redirect most of absent usernames but to confirm the presence of one,  we\u2019ll have to query our database since the bit might be set by other username which collided on same index of our bitmap. </p> <p>If all bits as set on our bloom filter, it'll become useless and if we\u2019re always increasing the size of our bloom filter we\u2019re moving toward more memory footprint. The actual implementation of bloom filter accounts for this pretty well, making it essentially works like above.</p>"},{"location":"Courses/fode/nosql/","title":"NoSQL Overview","text":""},{"location":"Courses/fode/nosql/#nosql-overview","title":"NoSQL Overview","text":"<p>DB Systems can be fundamentally divided into two parts based on their architecture:</p> <ul> <li>Frontend: communication layer which implements the APIs exposes internal functionality to clients and decides   the format of data used for this communication. The most popular format of communication with DB was using table,   which SQL excelled in but as internet evolved various other data structures like JSON became commonly used across   different places. This change became a catalyst to development of DBs using different data formats for communication,   like documents data which used JSON like structure for encoding its data. All these data format were designed because   of the need of performance in their specific use case. </li> <li>Storage Engine: primarily focuses on storing data on disk efficiently, which involves working with bytes so   format of data is irrelevant here. Other responsibility of storage engine involves working with indexes to    store/fetch data, managing data files, using compression to save storage space, providing crash recovery,   and other features like ACID.</li> </ul> <p>The major difference in SQL and NoSQL is between Frontend where the data format is changed to document from rows, and the API format which changed from SQL to simple getter and setter commands.</p>"},{"location":"Courses/fode/nosql/#mongodb-architecture","title":"MongoDB Architecture","text":"<p>MongoDB is a document based NoSQL database popular for its schemaless tables.</p> <p>MongoDB version &lt;4.2: This initial version used Memory Map Index on <code>_id</code> which is a B-Tree index where the leaf node contained a 64-bit pointer to the document. The 64-bit was composed of filename (32-bit) and offset (32-bit) to locate the document in that file, using which the OS can directly jump to the document location and retrieve it. The downside of this design was that any update in page size or data files could  mess up the whole offset based index. Also, it only supported collection level lock for concurrent transactions.</p> <p>MongoDB version 4.2-5.2: The storage engine was replaced with WiredTiger which solve the problem of collection level locks by allowing document level locks. It introduced compression which allowed mongo to fit more documents within a page.  The storage model now included a hidden clustered index (B+ Tree) on a field recordId. Any indexed field would reference this recordId which would then in turn point you to respective page on the clustered index. The problem now what that primary index on <code>_id</code> became slower since we\u2019ve to do two lookups (find recordId \u2192 find page).</p> <p>MongoDB version &gt; 5.3: Introduced clustered collection, which basically built a clustered index around <code>_id</code> field (avoiding recordId). The problem with this was that <code>_id</code> is 12 bytes long, due to which secondary indexes grew much larger.</p>"},{"location":"Courses/fode/nosql/#memcached-architecture","title":"MemCached Architecture","text":"<p>Memcached is a high-performance, distributed, in-memory key-value cache used to speed up dynamic web applications by reducing database load. Its architecture is intentionally simple and optimized for speed. It uses a client-server architecture where server are responsible for storing data in RAM as key-value pair, and client are libraries in apps which decides how to fetch the key within the cache.</p> <p>Distributed</p> <p>The cluster is logically distributed, but coordination is handled entirely by the clients. This makes the system highly scalable and avoids complex distributed consensus.</p> <p>Memory Management: It organizes memory allocation in slabs where each slab holds item of same size.  As new items are added, they\u2019re written to a pre-allocated page serially. The page is divided into equal fixed  size chunks whose determined by the assigned slab class. Each item uses whole chunk/s to stored their information, as such you might have unused memory within each chunk. This is minimized by using the most appropriate slab class for each item. The Slab class vary from class 1 (chunk size of 72 bytes) to class 43 (chunk size of 1MB). This is done to avoid memory fragmentation while keeping allocation fast and  predicatable to ensure high throughput for caching. </p> <p>Fragmentation</p> <p>When storing data sequentially without any strategy, freeing unused memory leaves small gaps of free memory scattered across the physical memory, this problem is known as fragmentation. Fragmentation makes it difficult to get a continuous block of memory large enough to store your  new data item even though there\u2019s more than enough memory present. OS overcomes this problem using  virtual memory, which basically gives us a continuous block but behind the scene is mapped to  multiple small area on physical memory. This still isn\u2019t optimal because to fetch a single block OS will have to fetch multiple pages and reassemble the memory fragments, which is why it's always better to avoid memory fragmentation.  </p> <ul> <li>Threading: Memcached used TCP transport as default to connect to remote clients.     The listener thread creates a TCP socket on port 11211. After accepting the connection,     it's distributed among a pool of worker threads which is responsible for the requested read/write</li> <li>LRU: Memcached use LRU eviction policy when it can\u2019t find any space for new keys.    Even if you put a TTL on a key that it can\u2019t expire before given time, the eviction policy can still   remove this key. LRU is implemented as a linked list where each node is a key-value pair in linked list,   and each slab has its own linked list. When an item is accessed, its move to the head of Linked List.   As result, unused items are pushed down to the tail and can be removed when needed.    One of the disadvantage of using Linked List LRU approach, you need to lock the entire linked list before   any update which serializes write operations and the multithreaded model wasn\u2019t effective.    This model was then updated to have LRU Linked List per slab, which reduced the locking to per slab.   Later on, LRU updates were made once every 60 second to reduce locking further. In 2018, the model was    completely redesigned by breaking LRU into subclass based on temperature but the problem of lock still   persists in keys belonging to same temperature.</li> <li>Reads and Writes: It uses hash to index the key to a memory location where its value is present.   For reads, it\u2019ll look up the key on the linked list at designated memory location and update the key\u2019s   position to head. For writes, if the memory location is free, it\u2019ll create a new pointer and a slab class   is assigned otherwise it\u2019ll handle Collision using chaining.  If the chain becomes too large to impact    read, Memcached resizes the hash and shifts everything to flattens the structure.</li> </ul>"},{"location":"Courses/fode/nosql/#redis-architecture","title":"Redis Architecture","text":"<p>Redis is an in-memory data structure store that supports caching, message queuing, real-time analytics, and more. Its architecture is more feature-rich than Memcached while still being extremely fast.</p> <p>It uses a single threaded event loop model for all its operation. To allow processing of multiple clients in  parallel, it uses I/O multiplexer (epoll/kqueue/select) over the single thread. </p> <p>One of the biggest difference b/w Memcached is its built in support for advanced data structures like Streams, Bitmaps, HyperLogLogs and more. All its primary data lives in RAM which allows faster read and writes, but it also supports durability optionally. You can persist data to disk in two ways:</p> <ul> <li>Journaling using an append only log (AOL) file, where logs are added for every insert/update.    These logs can be later replayed to restore the data of Redis upto the latest state of logs.   This requires another thread to append this logs.</li> <li>Snapshots, where data is flushed to disk periodically. This could risk data loss but the process is    much faster and the backup file is much compact and smaller.</li> </ul> <p>For communication, it uses its own wire protocol (known as RESP) build on top of TCP request/response model.</p> <p>Other popular features supported by Redis includes:</p> <ul> <li>built-in publish\u2013subscribe messaging system, where clients can publish messages to channels and subscribers   to these channels would receive them in real time.</li> <li>supports built-in distributed clustering, across different models like </li> <li>Sharding where Redis Cluster is partitioned across multiple nodes using hash slots.</li> <li>Leader-Follower async replication</li> <li>supports module to extend custom features like RedisBloom for supporting BloomFilters.</li> </ul>"},{"location":"Courses/fode/replication/","title":"Database Replication","text":""},{"location":"Courses/fode/replication/#database-replication","title":"Database Replication","text":"<p>Replication in DB system involves sharing data between redundant database instances in order to improve accessibility, reliability and fault tolerance. There are different kind of architecture for this</p> <ol> <li>Master/Standby Replication (Leader/Follower): Single database instance (node) will take all write operations    and distribute to Standby nodes. Standby nodes are only used for read operations. The consistency of reads for    standby nodes depends upon how long it take to propagate writes from master to standby, essentially ranging     the consistency from synchronous (strong) consistency to asynchronous (eventual) consistency.</li> <li>Multi-Master Replication: Similarly, to improve write you can have multiple master nodes. But this    strategy is more complicated and needs to handle conflicts when writing. </li> </ol> <p>The mode of replication can be synchronous or asynchronous depending on how data propagation is handled. Synchronous replication makes client wait till the transaction is completed on master as well as standby nodes. Cassandra uses this mode where the client accepts writes as successful until quorum (n+1/2) nodes have replicated the data. Asynchronous replication shows writes as successful write after it's committed on master. The replication is handed over to some backend process which moves data to standby nodes periodically.</p> <p>Advantage of using replication is you get horizontal scaling, and you can split standby database into  regions so that queries closer to certain group of user can use their region specific database instance.</p> <p>Disadvantage are your system might not like eventual consistency model and when going for strong  consistency the writes are slower. And if you want to go multi-master architecture, it's much more complex.</p>"},{"location":"Courses/foos/","title":"Fundamentals of Operating System","text":""},{"location":"Courses/foos/#fundamentals-of-operating-system","title":"Fundamentals of Operating System","text":"<ul> <li> Need of OS</li> <li> Process</li> <li> Memory</li> <li> CPU</li> <li> Process Management</li> <li> Storage</li> <li> Socket</li> <li> Extra</li> </ul>"},{"location":"Courses/foos/cpu/","title":"CPU","text":""},{"location":"Courses/foos/cpu/#cpu","title":"CPU","text":"<p>CPU (Central Processing Unit) as understood from its name, is a central component of computers responsible for processing. This processing can be executing instructions for software or performing arithmetics and logical calculations or controlling other components in computer to operate as a single unit(1) essentially acting as brain for computers.</p> <ol> <li>Like a brain in human body controlling different parts of body.</li> </ol>"},{"location":"Courses/foos/cpu/#cpu-architecture","title":"CPU Architecture","text":"<p> CPUs are packaged as a single integrated chip as shown on right, which is connected via motherboard. Internally, this single chip is divided into different components like cores, shared caches and more. </p> <p>CPU core is an individual processing unit which can independently execute instructions, allowing parallel processing in multicore CPUs. All these instructions which are understood by CPU are provided using an interface called ISA  (Instruction Set Architecture) which defines essentials(1) required by software to execute their work. There are many  different types of ISAs (2) which determines kinds of programs you can run and how efficiently they run. When compiling a software into program, it's compiled for a specific ISA, and it only runs natively on CPU which  implements respective ISA. Most of these implementation can be generally categorized into either RISC or CISC based. </p> <ol> <li>like the instructions, data types, registers, addressing modes for main memory, virtual memory,etc.</li> <li>like x86-64, ARM, RISC-V</li> </ol>"},{"location":"Courses/foos/cpu/#risc-vs-cisc","title":"RISC vs CISC","text":"<p>RISC or Reduced Instruction Set ISAs (like ARM) provide fewer simple instructions to perform tasks. This keep execution predictable since each instruction is executed in single clock cycle, but the programs would need to provide multiple instructions for executing a simple task. For example, you've to add two numbers, you've the following instructions:  save value to register, add value in two registers, load value from register.</p> <ul> <li>save first number in a register</li> <li>save second number in other register</li> <li>add values in both register</li> <li>save result in register. </li> </ul> <p>CISC or Complex Instruction Set ISAs (like x86-64) provides complex instruction which can perform multiple steps using single instruction. As such the numbers of instructions are more than RISC ISAs, since you need to provide permutation of all these complex instruction whereas RISC ISAs would simply compose its simple instruction. The key advantage of using CISC is program can be executed in fewer instructions reducing the overhead of instruction  translation but the execution is unpredictable because different instruction can take different number of clock cycles. For example, to add two numbers CISC architecture would have single instruction for add two numbers. Only one instruction can be used, but this would still require multiple steps to execute the process behind the scene.</p>"},{"location":"Courses/foos/cpu/#instruction-cycle","title":"Instruction Cycle","text":"<p>To execute any instruction, CPU processes it through different stages. To provide an overview for general execution,  CPU executes instructions in following stages:</p> <ol> <li>Fetch: CPU retrieves the instruction from memory for execution. The address to next executing instruction is always maintained    by CPU using PC (Program Counter) register. This address is fed to IFU (Instruction Fetch Unit)    which looks up the instruction in memory and returns the instruction bits. The lookup in memory is done in following order:     L1I (L1 Instruction Cache) -&gt; L2 cache -&gt; L3 cache -&gt; RAM/memory. Finally, the PC is updated to next instruction.</li> <li>Decode: Now we've raw instruction bits loaded into the CPU, but it doesn't know where to head next or make    sense of the instruction. Instruction Decoder helps in decoding this information, like the kind of operation    (ADD, LOAD, CMP, MOV), registers required to read/write, and does it need to access memory or any other execution     unit. Additionally, for CISC CPUs instructions might also get broken into micro-operations (uOps). </li> <li>Execute: Depending on the kind of instruction, the instruction is sent to respective execution unit to perform     the operation. </li> </ol> <p>This pipeline can have additional stages based on the kind of instruction which requires a different path, or depending on the architecture of CPU which introduces new stages to optimize its performance. </p>"},{"location":"Courses/foos/cpu/#major-instruction-types","title":"Major Instruction Types","text":"<ol> <li>Arithmetic and Logical Instructions like <code>ADD</code>, <code>AND</code> uses ALU. The ALU     typically takes two inputs with a control signal and outputs the result. The type of computation operation is     determined by the opcode decoded in Decode stage.</li> <li> <p>Memory Instructions like <code>LOAD</code> to read from memory and <code>STORE</code> to write to memory involves MMU (Memory     Management Unit). The lookup in memory is done following hierarchy from Registers -&gt; LCaches -&gt; Memory -&gt; Storage.     This ordering is decided based on the speed of transferring information to/from register into respective level.    Any access from lower level of memory will also save the information in high levels, so that further access is fast.</p> Data on Memory Lookup and Size Level Size Approx Access Time Approx CPU Cycles CPU Registers ~1 KB total ~0.3 \u2013 1 ns ~1 cycle L1 Cache 32\u201364 KB ~1 ns ~4 cycles L2 Cache 256 KB \u2013 1 MB ~3\u20134 ns ~12 cycles L3 Cache 4\u201364 MB ~10\u201315 ns ~40\u201350 cycles Main Memory (RAM) 8\u2013128 GB ~70\u2013100 ns ~200\u2013300 cycles SSD (NVMe) 256 GB \u2013 4 TB ~50\u2013150 \u00b5s ~150,000\u2013400,000 cycles HDD 500 GB \u2013 10 TB ~5\u201310 ms ~15,000,000\u201330,000,000 cycles </li> <li> <p>Branch Instructions like <code>JMP</code>, <code>CALL</code>, <code>RET</code> are used to move ahead to a specific instruction due to execution of control    statements like <code>if/else</code>, <code>for-loops</code> and <code>return</code> statements. These instructions require special execution unit due    to pipelining of instruction.</p> <p>Pipelining is done to maximize the utilization of different CPU components to reduce its idle time. For example,  when executing instructions sequentially only the currently running execution unit would be actively working while  rest of the components would be idle. To avoid this, CPU pipelines multiple instruction such that while current  instruction is in later stage of execution, the next instruction has already started its processing.</p> Pipelining Visually <pre><code>Instruction Stages:   FETCH \u2192 DECODE \u2192 EXECUTE \u2192 WRITEBACK\nTime -&gt;\n\nWithout Pipelining\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 F -&gt; D -&gt; E -&gt; W   \u2502 -&gt; \u2502 F -&gt; D -&gt; E -&gt; W   \u2502 -&gt; \u2502 F -&gt; D -&gt; E -&gt; W   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n  Instr 1                      Instr 2                   Instr 3\n\nWith Pipelining\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \n\u2502 F -&gt; D -&gt; E -&gt; W   \u2502 -&gt; \n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \n  Instr 1                                         \n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \n      \u2502 F -&gt; D -&gt; E -&gt; W   \u2502 -&gt; \n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \n         Instr 2\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502 F -&gt; D -&gt; E -&gt; W   \u2502 -&gt;\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              Instr 3\n</code></pre> <p>But with control statements the execution of instructions becomes branched, as such pipelining becomes complex since    the branched code might not include the already pipelined instruction. This essential makes the pipelined instructions    after branching invalid, which the CPU have to flush out and restart the pipeline which is very inefficient. To     minimize the flushing of pipelines, CPu uses Branch Predictors which guesses if a branch will be taken or not.    At the time of execution, if the branch is mispredicted the CPU will restart the pipeline but this happens rarely as    branch predictors have almost ~95% accuracy in predicting the right branches.</p> </li> </ol>"},{"location":"Courses/foos/cpu/#optimizations","title":"Optimizations","text":"<p>Over the time, CPUs added new architectural optimizations to maximize performance and efficiency, out of which we've already discussed one above, Branch prediction which allows speculative execution and keeps pipelines full by guessing future control flow and executing ahead. Other commonly known techniques are Out of Order Execution, Register Renaming, Prefetching, using specialized hardware which are discussed below.</p>"},{"location":"Courses/foos/cpu/#out-of-order-execution","title":"Out of Order Execution","text":"<p>Programs often contain operations that depend on the results of previous ones. In a simple in-order CPU,  if one instruction stalls (for example, waiting on memory), everything behind it must also wait\u2014even independent work. To avoid wasting these cycles, Out-of-order execution allows a CPU to execute instructions as soon as their input data is ready, rather than strictly following the original program order. The final results always appear as if instructions were executed in order, but internally, the CPU rearranges them to avoid stalls.</p> <p>This introduces 3 new step in above specified Instruction Lifecycle. After Fetch and Encode,</p> <ol> <li> <p>Register Renaming: This is done to avoid false dependencies resulting from blocked registers. Actual     dependency would be when an instruction needs a result produced earlier (also known as Read-After-Write). While other    two dependencies, </p> <ul> <li>Write After Read, when instruction writes to register that an earlier instruction will read from.</li> <li>Write After Write, when two instructions write the same register.</li> </ul> <p>aren't really dependency if we can just rename the registers to used different ones. This renaming is done to     temporary registers also known as physical registers. These are different from architectural registers specified     by ISA, and they're present in larger pool (180 in x86-64).</p> </li> <li> <p>Instruction Scheduling: , Instructions whose operands are ready goes into a     pool where a scheduler selects ready instructions and issues them to execution units\u2014regardless of original order.</p> </li> <li>Reorder Buffer (ROB) &amp; In-Order Retirement: Even though execution is out of order, results are written in order.    This is ensured by a new hardware component ROB which holds results temporarily and update them in     registers/memory only when all older instructions have finished cleanly. Additionally, it ensures results from     branch misprediction are discarded. </li> </ol> <p>The benefit of using Out of Order Execution are improved throughput and performance. While the tradeoff being, increase CPU complexity from components like ROB, dependency tracking, physical registers, higher power consumption  and heat production which is why simpler CPUs (e.g., microcontrollers) use in-order execution.</p>"},{"location":"Courses/foos/cpu/#prefetching","title":"Prefetching","text":"<p>A CPU can execute billions of instructions per second, but a memory access can take hundreds of cycles. Memory access is the most common reason for stalling pipeline. Prefetching reduces these stalls by predicting what data will be needed soon, and it\u2019s already waiting in a cache instead of main memory. Data needed is predicted using  programs access behaviour. Prefetching can be either implemented automatically using hardware built in CPU or using  software where programmer or compiler can use special instructions to hint CPU to load data. This is very useful in  manually optimizing code and high-performance computing (HPC) workloads.</p>"},{"location":"Courses/foos/cpu/#accelerators","title":"Accelerators","text":"<p>CPUs are extremely flexible but aren't efficient at highly parallel tasks, repetitive math operations ,large matrix computations, graphics and image processing and machine learning workloads. To optimize these tasks, we can use  Accelerators which are specialized hardware designed to speed up specific computations like </p> <ul> <li>GPUs (Graphics Processing Units) for rendering, machine learning, and video encoding.</li> <li>TPUs (Tensor Processing Units) for matrix multiplications, neural network training.</li> <li>DSPs (Digital Signal Processors) for signal-processing tasks like, audio, radar, telecommunications, compression. </li> <li>ASICs (Application-Specific Integrated Circuits) for fixed tasks like video encoders/decoders (H.264, HEVC),   crypto accelerators (AES, SHA), SSD controllers. </li> </ul> <p>They usually work alongside the CPU in a system, connected through either PCIe (GPUs, NICs), on-chip interconnects (NPUs, image processors) or memory-mapped interfaces. Typical CPU sets up the task for accelerator which processes data and returns the results while CPU continues main program flow. This offloading frees the CPU to handle logic, branching, and system tasks. This allows CPU to achieve more parallelism and higher throughput making accelerators essential for workloads like AI, graphics, data analytics, and networking demand.</p>"},{"location":"Courses/foos/cpu/#conclusion","title":"Conclusion","text":"<p>To conclude, let's look at the execution of following C program which will put all above topics together.</p> <p></p><pre><code>for (i = 0; i &lt; N; i++) {\n    A[i] = B[i] * C + D;\n}\n</code></pre> This is compiled into following assembly code<p></p> <pre><code>L1: LOAD R1, [B + i*4]        ; load B[i]\n    LOAD R2, C                ; load constant C\n    MUL  R3, R1, R2           ; R3 = B[i] * C\n    ADD  R4, R3, D            ; R4 = R3 + D\n    STORE [A + i*4], R4       ; A[i] = R4\n    ADD  i, i, 1              ; loop counter\n    CMP  i, N                 \n    JLT  L1                   ; loop branch\n</code></pre> <p>This code is executed in CPU as follows:</p> <ol> <li>Prefetching &amp; Instruction Fetch: As the loop runs repeatedly, the CPU\u2019s instruction prefetcher notices a sequential     pattern and fetches multiple future instructions before they\u2019re needed. Prefetcher reads ahead into the I-cache.     Branch predictor predicts the loop branch as taken. This keeps instructions flowing without waiting for memory.</li> <li>Decode + Micro-Op Translation: Instructions move to the decode stage, CISC instructions (like x86) are broken into     micro-ops. CPU identifies loads, stores, ALU ops, and branch ops. The branch predictor supplies the predicted next    PC to keep the pipeline full.</li> <li>Register Renaming: Architectural registers (R1, R2, R3, etc.) are mapped to physical registers for example, R1 -&gt;P7,     R2 -&gt; P12, R3 -&gt; P9, R4 -&gt; P14. Every new instruction that writes a register gets a new physical register.     This removes WAW/WAR dependencies so instructions can run in parallel.</li> <li> <p>Dispatch into Reservation Stations &amp; ROB: Each instruction allocates a ROB entry (for in-order retirement) and     a reservation station entry (to wait until operands are ready). For example, ROB after decoding an iteration</p> Entry Instruction Physical Dest Ready? 0 LOAD B[i] P7 no 1 LOAD C P12 yes 2 MUL P9 no 3 ADD P14 no 4 STORE \u2014 no </li> <li> <p>Out-of-Order Execution (OOO): The CPU checks which instructions are ready,</p> <pre><code> LOAD C        ;(executes immediately)\n ADD i, i, 1   ;(independent; executes early)\n CMP i, N      ;(also executes early)\n LOAD B[i]     ;(waiting on memory)\n MUL           ;(waits for B[i])\n ADD R4        ;(waits on MUL result)\n STORE         ;(waits on ADD)\n</code></pre> </li> <li> <p>Cache System &amp; Prefetchers: Because the loop accesses arrays B and A sequentially, the hardware data    prefetcher recognizes a streaming pattern and starts fetching future <code>B[i+1]</code>, <code>B[i+2]</code> and <code>A[i+1]</code> cache lines.     So future iterations hit in L1 or L2, reducing stalls.</p> </li> <li>Results Become Ready in Reservation Stations: When the <code>LOAD B[i]</code> returns from memory, it broadcasts its result     via the Common Data Bus (CDB): Waiting MUL is awakened, MUL runs, ADD depends on MUL \u2192 executes afterward.    Everything continues smoothly.</li> <li> <p>In-Order Retirement Through ROB: Even though execution was out-of-order, results become architecturally visible    in program order. The ROB commits entries:</p> <ul> <li>Commit <code>LOAD B[i]</code></li> <li>Commit <code>LOAD C</code></li> <li>Commit <code>MUL</code> </li> <li>Commit <code>ADD</code> </li> <li>Commit <code>STORE</code></li> </ul> <p>If a branch misprediction occurred, the CPU flushes the ROB, rolls back register mappings  and restarts from     correct path</p> </li> </ol> Analyzing CPU usage in Linux using top <p>Analyzing CPU usage for a programs give you better insight on its execution performance, and decide whether its CPU bound or I/O bound. Running <code>top</code> in terminal gives you information similar to following fields at top, </p><pre><code>%Cpu(s):  5.0 us,  2.0 sy,  0.0 ni, 90.0 id,  3.0 wa,  0.0 hi,  0.0 si,  0.0 st\n</code></pre><p></p> <ul> <li>us -&gt; Time spent running user processes</li> <li>sy -&gt;Time spent in kernel/system processes</li> <li>id -&gt; Idle time</li> <li>wa -&gt; Time spent waiting for I/O, tells if a process is CPU bound or I/O bound</li> <li>hi/si -&gt; Hardware/software interrupts</li> <li>st -&gt; Steal time (virtualized environments)</li> </ul> <p>In the per process stats, we get following fields: </p><pre><code>PID USER  PR NI  VIRT  RES  SHR S %CPU %MEM  TIME+ COMMAND\n</code></pre><p></p> <ul> <li>%CPU tell % of CPU the process is consuming</li> <li>S is state of process, like Running (R), Sleeping (S), Uninterruptible IO sleep (D), etc.</li> <li>TIME+ tells total CPU time used</li> <li>COMMAND is the name of process</li> </ul>"},{"location":"Courses/foos/extra/","title":"Extra Topics","text":""},{"location":"Courses/foos/extra/#extra-topics","title":"Extra Topics","text":""},{"location":"Courses/foos/extra/#compiled-vs-interpreted","title":"Compiled vs Interpreted","text":"<p>Programs runs on Machine code which are specific to CPU architecture, like ARM, x86, M series of apple each of them  operate on machine code instructions specific to their CPU. But developing programs in machine code wasn't efficient and reliable for huge programs as it isn\u2019t easily readable by humans. To solve this, high level languages were invented which could be understood by humans and also translated into machine code for CPU to understand. </p> <p>One of the earlier and popular higher level language is Assembly. It's the closest language to machine code, in a  way that each assembly instruction can be mapped to a specific machine code. But even some instructions in Assembly can be specific to CPU. However, as programs became larger and more complex, the amount of assembly instructions required also grew exponentially making assembly inconvenient for writing such programs. </p> <p>Higher Level Languages solve this by abstracting multiple low-level instructions into single high-Level  instruction. But now we\u2019ve to translate this high level instructions into machine code so that they can be executed by  CPU. This process of translation is called Compilation. But to execute these translated machine code, we\u2019ve to add more stuff like EFI headers which tells metadata like the entrypoint of execution in the program, the heap, the stack, the code area, import external files, etc. All these changes are done by Linker, and this process is called Linking. </p> <p>Compilation is done using a compiler program (like (1)), where it produces machine code as an object file for each of the source files. To execute this object file, you\u2019ll need to find and link all external object files required  and create a single file containing all the code. Further, you also have  to add headers (ELF headers) required by OS for executing program. For example, Linux uses ELF which contains all the metadata about program like where is the text data, static data, heap data, stack area, entrypoint, etc. This step is also done by linker. Linking is done using another program called linker (like (2)). This linked file is usually   the final executable which has all the instruction required to be executed against CPU.</p> <ol> <li>gcc, clang, rustc</li> <li>gold linker, lld, mold</li> </ol> Bootstrapping complier <p>Compiler themselves are complex programs, so how do you create a compiler using your high level language before  even having the tool to translate your language to machine code? It's similar to \"Which came first, the Duck or the  Egg\" analogy. There are two commonly used approach:</p> <ul> <li>Translate your language into another mature language which already have an implemented compiler (for example, Python).</li> <li>Creating an initial compiler in another mature language, and then using it create compiler written in your own      language to make it Self-hosted. And the process is  called Bootstrapping a  compiler</li> </ul> <p>There are several approach to implement bootstrapping:</p> <ol> <li>The most commonly used approach is to write the first compiler in another language. Once this compiler exists,     you can rewrite the compiler in your language itself, and use the first version to compile the new one.</li> <li>Build an interpreter for your language in a host language and create the compiler program in your own language. Now,   your compiler program can be compiled using the interpreter which can then be used to compile instruction in your   langauge without the interpreter.</li> <li>Used historically, developers wrote the first compiler in assembly or even binary. Once you've minimal compiler,    used it to compile better versions. </li> </ol> <p><code>Go</code> wrote its host compiler in C, which was later rewritten in Go to self-host. There are several benefits for  self-hosting a language:</p> <ul> <li>further development is easier as the compiler can recompile itself.</li> <li>removes dependence on another language or external tools.</li> </ul> <p>Another key issue when developing programs was you've to maintain a different version of your program to support their execution on different OS and CPU architecture. To solve this, Interpreted Languages were created which allowed you to execute same code for every support environment. Such languages (like Python, JavaScript, Java) uses an intermediate  program called Runtime which executes their instruction instead of directly executing them on CPU. This way, we can just compile the Runtime specific to the platform (mac, window, linux) and it'll translate the  intermediate code into respective machine code. The tradeoff is performance, since we\u2019re adding an extra step for executing code. Also, programs are only executable for environment in which the Runtime is available.</p> JIT Optimization <ul> <li>JIT (Just In Time) Compilation is an optimization for interpreted languages. Interpreted Languages translates their intermediate code into machine code on the fly (when the program is running). This gives you faster startup but slower execution. </li> <li>AOT (Ahead of time) compilation turns all the code into machine code beforehand which makes   is faster in execution but slower in startup. </li> </ul> <p>JIT compilation is a hybrid of both these approaches, where it starts the program by running interpreted code  (which provides faster startup) and as the program continues, runtime monitors which path of code is getting executed  more frequently (hot path). These hot paths are then compiled into machine code on the fly and replaced which their interpreted version to avoid re-translation everytime. This gives it the performance of faster execution.  The issue however is the security risk of providing access to modify code on fly, which could be  used by malicious agents to execute hidden instructions. This is the reason AOT compiled process only loaded code on  startup and marks the area in memory as read only. JIT makes it less insecure by using static code segment for initial runtime and when it needs to switch with compiled instruction, it loads the code in heap, writes the changes and then makes it read only and executable so that program counter can execute these instructions.</p>"},{"location":"Courses/foos/extra/#garbage-collection","title":"Garbage Collection","text":"<p>While managing memory manually if done right allows program to keep their memory usage low and clean, but getting it right everytime requires developers to understand the lifetimes of data across every code path. You've to free the  exact memory at the right time,</p> <ul> <li>freeing is done too early, you'll get use-after-free bug which could crashes your program or corrupt its state    leading to security vulnerabilities.</li> <li>freeing is done too late, it leads to memory leak. If it's completely forgotten, it'll leak into system reducing its   memory capacity over time.</li> <li>and other disasters like free twice, freeing the wrong pointer, or returning early due to an error.</li> </ul> <p>You can avoid all these failures and complexity if you can make memory management automatic, as a part of the language/external program which can allocate memory as required and deallocate it when its no longer in use. This idea of automatic deallocation was termed as garbage collection, which finds programming objects unreachable by code and frees them from memory. The trick to make it feasible is \"how the GC finds the unreachable objects\". There are several strategies and each has different trade-offs.</p> Mark-and-Sweep algorithm <p>As suggested by name, it does garbage collection in two phases:</p> <ul> <li>Mark phase: Start from \"roots\" (like global variables, stack variables) and follow all pointers, marking everything  reachable. </li> <li>Sweep phase: Walk through all heap objects; any object not marked can be freed.</li> </ul> <p>This approach is used widely as it works reliably with arbitrary object graphs (like cycles) and simple to implement. The tradeoff begin it block the main program during GC to mark objects reliably. Also, it can easily fragment the heap memory, making it directly unusable for large chunks of allocation.</p> Generational GC <p>Used across Java, Go, JS - this approach relies on of a key observation in real program: \"Most objects die young\", because most objects are either created from temporary tasks (like (1)), and long-lived objects (like (2)) are quite rare. Empirically, profilers and GC log shows that 80\u201395% of objects die before a second GC cycle.</p> <p>We could exploit this information, by storing the short and long-lived object separately so that GC can focus more  on the area used for storing short-live objects and collect it frequently while collecting long-lived objects  rarely. Area for storing short-live object is termed as Young generation and long-lived as Old generation. When collecting young generation, living objects are copied into a new space making everything left behind as garbage. This uses Cheney's algorithm, and it helps in defragmenting the heap automatically.  </p> <p>This approach allows faster GC with fewer pauses and less wasted work, while causing very little fragmentation. The  tradeoff begin overhead from memory copying.</p> <ol> <li>storing intermediate results, function parameters, iteration variables</li> <li>configs, sessions, cache, object pools</li> </ol> Reference Counting <p>Used in CPython, Rust ARC - Each object internally keeps a count of how many references point to it. When a  reference is created/removed, the count is incremented/decremented respectively. When an objects reference count  reaches 0, it is immediately freed.</p> <p>This saves program from big pauses due to GC cycles. The tradeoffs being, more overhead when creating new reference,  since it also requires updating the reference count. It also doesn't handle cyclic cases where two objects points to each other can cause memory leak, as their reference count is always up by 1. To solve this, CPython uses a  secondary cycle collector which cleans up cycles periodically. Rust ARC avoid secondary cycle collector by allowing developers to use weak reference for relationships that shouldn\u2019t imply ownership. Using weak references doesn't increase the reference count of pointed object, as such can be used to avoid cycles.</p> <p>Other than freeing memory GCs also helps to reduce memory fragmentation by Compaction which allows</p> <ul> <li>faster allocations</li> <li>improves cache locality</li> <li>reduces the chances the system must ask the OS for more memory</li> </ul> <p>Without compaction, a program might have free memory but in such small chunks that new objects can't fit.</p>"},{"location":"Courses/foos/extra/#virtualization-vs-containerization","title":"Virtualization vs Containerization","text":"<p>Hardware resource where very scarce and expensive during early days of computing. Companies only had single mainframe, on which users had to take turns for running their workload. But with creation of Virtualization, multiple users could run workloads on the same physical machine without interfering with each other. </p> <p>Virtualization allows one physical computer (the host) to run multiple isolated environments (the virtual machines or VMs) at the same time. Each VM acts as a complete standalone computer, with its own  CPU/Memory/Disk/OS but none of these are physical devices, they're virtualized by software.</p> <p>How Virtualization works?</p> <p>Virtualization at core uses a software layer called Hypervisor which sits between virtual machines and  physical hardware. Whenever a VM tries to execute an instruction as usual, the hypervisor quietly translates, schedules, and manages the requests over to real hardware. With time, modern CPUs added built-in virtualization features which allow VMs to run instructions almost directly on the CPU by providing special modes where the hypervisor can intercept dangerous or privileged operations. This made virtualization fast. Before these extensions, it relied on slow software tricks. There are two types of Hypervisor at this time:</p> <ol> <li>Bare-Metal Hypervisors which are installed directly on the hardware. For example, VMware ESXi, Microsoft     Hyper-V, etc. These are often used in datacenters and clouds because they\u2019re faster and more secure.</li> <li>Hosted Hypervisors: Run inside an existing OS as a software layer. For example, VirtualBox, VMware     Workstation. These are great for developers, testing, and personal use.</li> </ol> <p>Even though hardware is cheaper and readily accessible now, virtualization still remains essential because of few reasons:</p> <ol> <li>Server Consolidation: run many servers on one machine.</li> <li>Security Isolation: VMs are very strongly isolated, making them ideal for cloud customers and regulated industries     like banking, government. Multi-tenant clouds (AWS, Azure, GCP) rely heavily on VMs to safely isolate customers.</li> <li>Legacy OS Support: run Windows XP, old Linux kernels, or any outdated system safely.</li> <li>Snapshots: snapshot whole OS state for backup, migration or cloning.</li> </ol> <p>As computing evolved, hardware become cheaper while applications grew larger and more complex. There were two commonly faced issues when developing software during this period:</p> <ul> <li>Apps could behave differently on different machine due to conflicts in underlying dependencies of system. This made    development painful, as developer would've to often manually tweak OS to successfully run their application.</li> <li>Monolith architecture of application made them larger, complex, and harder to manage. Due to this, Microservice    architecture became popular which allowed developers to break single applications into smaller services which would   operate with each other in coordination to become the fully functional application.</li> </ul> <p>Both of the above problems were solved using Containerization. It allowed developers to package an application and everything it needs (libraries, dependencies, runtime) into a single file called image. Images can be executed  independently across different OS using a container runtime program (1) solving the conflict in dependencies of  platform. The process created for running the application is termed Container. Containers have a control view of other  processes and resources, so that they can work in an isolated environment without external interference. This solved the second issue where developers can now use containers for creating their service which are isolated from other process  and can be managed independently to coordinate with other process or scale. One of the implementation of container is by using built-in Linux kernel features: Namespace and Cgroups.</p> <ol> <li>similar to how interpreted language use runtime to achieve platform independence</li> </ol> <p>Namespaces are used to isolate what a process can see. Different types of namespaces are used to isolate different   parts of  process. For example,</p> <ul> <li>PID namespace allows you to isolate process tree. This way containers can\u2019t see or kill host processes, behaving    like its own mini-OS</li> <li>NET namespace allows network isolation, giving each container its own virtual network stack. </li> <li>MNT namespace to control mount point visible to container </li> <li>IPC namespace to isolate shared memory, </li> <li>UTS namespace to provide container its own hostname and domain name.</li> <li>USER namespace for user ID isolation. </li> </ul> <p>cgroups (Control Groups) limits how much resource a process can consume, like CPU, Memory, Storage I/O and Process   limits. Without cgroups, a container could hog the machine.</p> Linux Container <p>At this time, the term containers is synonymous to Linux containers which are one implementation of containers using built-in Linux Kernel features due to their dominance over modern software development. Since other platforms like macOS and Window doesn't provide Linux namespaces or cgroups support, they're supported non-natively using a lightweight Linux virtual machine.</p> <p>Further, to make container images easy to distribute and faster to build, they're built in layers using Union Filesystem (like (1)). Tools like OverlayFS merge these layers into a single filesystem at runtime.</p> <ol> <li>base OS layer, dependency layers and application layers</li> </ol> <p>To summarize, containers are essential for software development as they're</p> <ol> <li>Portable: runs the same everywhere</li> <li>Fast: spins up in milliseconds, making them perfect for scaling microservices, CI/CD workflows and serverless backends</li> <li>Efficient: uses fewer resources. You can run dozens or hundreds of containers per host.</li> <li>Reproducible builds: Dockerfiles let you define environments as code.</li> <li>Fits perfectly with DevOps automated pipelines, Kubernetes and cloud-native tooling</li> </ol>"},{"location":"Courses/foos/intro/","title":"Need of OS","text":""},{"location":"Courses/foos/intro/#need-of-os","title":"Need of OS","text":"<p>Operating System is the layer of software which abstracts interactions of other software with the underlying hardware by providing high level APIs while hiding the complex implementation details. The complexity comes due to support for variety of hardware for same component which needs to be mapped under same layers of APIs.</p> <p>However, computing environment varies to achieve different goals with unique constraints and workload specific to them. For example, Smartphones have low power and touch screen requirements which lead to development of iOS and Android. Online servers requires higher throughput, reliability which uses Linux server. General pcs which requires flexibility in apps usage uses Windows or Linux Desktop. Each such OS would be uniquely tuned for their compatible device, where a general purpose OS wouldn't be able to support needs like lower latency and smaller memory usage due to its decision to support huge numbers of hardware devices which makes the abstraction layer larger and less efficient. Even with all these differences, there are few things which are common across all OS:</p> <ul> <li>Abstraction to hide the underlying hardware from the software over an API layer. However, hardware are always    evolving due new inventions and discoveries, and OS have to provide backward compatability so that existing system   don't fail. To do this, the abstraction is divided into different levels such that you can extend the   existing design in order to handle new design changes without discarding previous design. </li> <li>Scheduling concurrent processes as per users requirements so that you can run multiple processes concurrently   without blocking one another. There are different scheduling algorithms which vary depending on goals such as   fairness, throughput, response time, and real-time guarantees. Few of the popularly known are Round Robin,   First-Come First-Served, Shortest Job First, Priority Scheduling, etc. You can customize scheduling to    select the algorithm which works best for your workload.</li> </ul>"},{"location":"Courses/foos/intro/#core-components-and-terminologies","title":"Core components and terminologies","text":"<ul> <li>Kernel: core component of an OS which manages most of the things like drivers, memory, CPU, scheduling. Other   components are tools which communicate with Kernel to perform their operations. You can directly work with kernel,   but to avoid the hassle there are different linux distributions (distros) already build on top of same Kernel to   provide ready to use features like GUI, software managers, etc.</li> <li>CPU: Most of the computation work is executed in CPU, hence it\u2019s very important to have frequently used values   near to CPU. There\u2019s <code>L1</code> and <code>L2</code> cache which lives within the CPU core and the closest in respective order.   Then there\u2019s L3 cache which is shared between cores but still closer to CPU. Each core can execute a process    individually allowing Kernel to schedule multiple process at the same time.    Instruction executed by CPU are known as machine code, which are specific to CPUs. Programs are naturally compiled   into machine code to execute them in CPU, as such their compiled version are specific to CPU    (limitation of compiled language). Interpreted languages like Python, Javascript can run same code on different CPUs   because you\u2019ve compiled runtimes specific to the CPU which executes the code.</li> <li>Memory: (or RAM) is a fast (slower than CPU caches) but volatile storage space where CPU can store information   which it needs for the execution of instructions. RAM is called Random because in earlier day\u2019s the memory are based   on tapes which had to be sequentially skipped ahead to access the information at specific location. In case of RAM,   you can directly point to the address of information to fetch it. CPU uses RAM for lots of use case like storage of   process state, data, caching etc.  To manage memory, OS uses the concept of virtual memory where it abstracts all    the details behind the scene like allocating space to process, swapping memory to disk, etc. to efficiently use memory.</li> <li>Storage: (SSDs/HDDs) a persistent storage even after power cut to the component. It\u2019s slower than RAM and provides   sequential access to data along with some limitation like HDD have to store data in sectors (section of bytes)    even if we\u2019re updating just single byte, or like SDDs which have to write on pages and can\u2019t erase single page but   whole block to free up storage. In earlier days, controller to work with storage lived in memory as a program,   but it was later move within the disk itself. Now, the controller exposes an API to work with disk which the OS    integrates with itself and exposes API to application (kind of chaining APIs). This was done to avoid involvement   of OS to update the controller whenever there were modification required within controller due to changes in disk.</li> <li>Network: also called NIC (Network Interface Controller),is one of the way to communicate with other hosts.    The Network controllers receives data/packets from internet as electrical (Ethernet/Fiber) or analog signal which    gets translated into bits  (layer 1) -&gt; frames (layer 2) -&gt; packets (layer 3) -&gt; segments (layer 4).   OS is responsible for dealing with Layer 3 and 4 also known as the communication protocol which is responsible for   parsing these bytes into sensible data. For example, TCP protocol which is implemented within OS is responsible for   providing features like parsing and scheduling packets, dealing with acknowledgement (ACK), etc.</li> <li>File System: Storage is mostly blocks of bytes and earlier kernel accessed storage directly based on the   implementation of disk at the time. This was however bad because whenever the disk had to evolve, it\u2019d need to make    the change in OS as well.  To avoid this, OS abstracted access to disk using an API known as LBA   (Logical Block Addressing). It assumes that disks are array of blocks of fixed size and the LBA will translate any    logical address to physical address on disk. But block address weren\u2019t directly consumable by users, so it was then   abstracted using file system which used identifiable objects with name, headers, types, size to map specific blocks    in storage to their respective file such that it can be accessed conveniently. There are different kinds of file   systems like XFS, NTFS, FAT32, EXT4, etc. which are build around their specific use case or for general purpose.    With this, disk are exposed as big arrays of LBAs which can be divided into separate sections using partitions.   Each partition is formed in specific file system and the data store in it can be only access in its respective format   of file system.</li> <li>Program and Process: Programs are executable file and Process are running instance of a program.   These executable file have header layout specific to OS.</li> <li>Process Management: Kernel manages the processes like schedules/switches them in CPU, providing them access to   resource. However, for security reasons it's done in different mode. So process operates in two modes \u2014    user mode and kernel mode, user mode involves all the general stuff a process executes and kernel mode involves   access to resource via kernel. Each of these mode is allocated their own space in memory of process.    For example, your browser lives in user space of memory where it can operate on its instruction, but when it needs    to receive a request it\u2019ll ask the kernel to open a socket, for which it switches to kernel mode and then the kernel   sends over the request over designated socket.</li> <li>Device Drivers: Software in kernel which knows how to communicate with pieces of hardware. It works through   Interrupt Service Routine, for example if we press any key on keyboard, it\u2019ll interrupt the CPU to immediately   execute the code to read the keypress from keyboard buffer, and sends it over to whatever is reading it.</li> <li>System calls: bridge between user space and kernel space. Apps make system calls to jump from user to kernel    space. Some of the system calls are <code>read</code>, <code>write</code>, <code>malloc</code>. It causes a mode switch in CPU to get higher    privileges for executing instruction, that\u2019s why It's important to remain isolated for security.</li> </ul>"},{"location":"Courses/foos/memory/","title":"Memory","text":""},{"location":"Courses/foos/memory/#memory","title":"Memory","text":"<p>Memory is a core component for computers which allows it to remember information for future use. Without memory, CPUs won't be able to make sense of any opcode used for executing machine code, it won't be able to perform multistep processing which requires storing intermediate results, and lastly it won't be nearly as fast as it\u2019s now. Technology like RAM/ROM provided CPUs with faster storage and retrival operations compared to other storage, but the data is only persisted for the duration its supplied with power. This lead such a high adoption of RAM as memory such that this technology has become synonymous with memory in computers. </p>"},{"location":"Courses/foos/memory/#different-kinds-of-memory","title":"Different kinds of memory","text":"<p>The core concept of memory is to allow byte addressability over its information, such access to all available addresses are equivalent. But different requirements like speed, persistence lead to development of different kind of memories. Below we'll discuss different kinds of memory and how they evolved over time.</p> <ul> <li> <p>ROM (1) was designed so that it wouldn't lose its configuration even in absence of power, the tradeoff was it could    only store static data (2). This kind of memory is still used to store information like CPU opcodes(3), and firmware   like BIOS/UEFI (any data which isn't changed normally). </p> <ol> <li>Read Only Memory, non-volatile read only memory. </li> <li>ROM stores data by physically encoding it into the hardware during manufacturing. The patterns of bits are     \"burned in\" using fixed wiring, programmed fuses, or permanently set transistors, so the data remains even without    power and cannot be easily changed afterward.</li> <li>Opcodes are machine codes which are the lowest level of instruction understandable by a CPU. </li> </ol> </li> <li> <p>RAM (1) technology allowed to reset its configuration electrically such that same circuit can be reused to stored   different information. This allowed CPU to use this space as scratch board for its execution, where it can store   temporary information like intermediate result from execution of an operation. </p> <ol> <li>Random Access Memory, since you can access information using random addresses  </li> </ol> </li> </ul> <p>Further RAM can be divided into two categories based on their implementation and usage.</p> <ul> <li> <p>DRAM(1),  used for general memory is a slower and cheaper RAM compared SRAM. It's cheaper because it's only uses   1 Capacitor and transistor to build 1-bit memory. Information of each bit is stored as charge in capacitor, but    since capacitors leak charge over time, you need a mechanism to refresh the charge done using Sense Amplifier.    When reading a selected row, capacitor's tiny charge is shared on bitline which leads to a tiny change in voltage.    This change is detected by Sense Amplifier which amplifies it to corresponding 0 or 1. Since releasing charge would   corrupt the state of capacitor, sense amplifier writes the value back (refreshes it). When writing, sense   amplifier turns the bitline to 0 or 1, which forces the capacitor to charge or discharge for storing the new bit.    Since DRAM uses less electrical circuitry to make 1-bit, its more compact and cheap making it ideal for developing    large memory. But its less performant as it needs to refresh the charge regularly (~64ms) making it less idea for low   latency parts.</p> <ol> <li>Dynamic RAM </li> </ol> Evolution of DRAM <p>In Earlier days, DRAM was asynchronous. It didn't use the system clock due to which the CPU had to wait and insert delays, as it didn\u2019t know exactly when DRAM would finish operations. This caused timing mismatches and inefficiency.   SDRAM was developed to address this issue by providing clocked, synchronous operation. Now it'd use the system bus clock, which made data transfers occur at precisely defined clock edges, so the CPU knows exactly when data will be ready. Because it\u2019s synchronized, SDRAM can accept a new command every clock cycle  (even while previous ones are still in progress), greatly improving throughput.</p> <p>SDRAM still used only single rising edge of clock to transfer data, we\u2019d double the rate of transfer if we can also use the falling edge of clock as well. This lead to development of DDR SDRAM (1). Overtime, DDR SDRAM evolved from  version 1 to 5. In DDR4 SDRAM,  we\u2019d 8 pins where each pin could fetch 8 bits per I/O. This was designed to  maximize the caching in CPU which is a 64 bytes cache lines. So while we\u2019re fetching certain data, we\u2019d also fetch the  remaining portion in 64 bytes window and this is called burst. The issue with current RAM is that it can only be used by single core of CPU at a time, so it'd be a waste to use it on multicore CPU since all other core\u2019s would\u2019ve to  wait for currently accessing CPU. This was solved by DDR5 SDRAM, which doubled the bits fetched by  each pin to  16 bits per I/O. Now, you can divide the 64 pins into 2 sections to fetch 64 bytes (16x32) and each of these sections were called channels.</p> <ol> <li>Double Data Rate SDRAM</li> </ol> </li> <li> <p>SRAM (1), used in CPU caches  has much faster access than RAM. It stores data using flip-flops (2)   , so it doesn\u2019t require the constant refreshing that RAM does making it much faster. However, manufacturing SRAM is more    expensive and takes more space (3), so you can\u2019t build large memories with it. This makes it ideal for small,    high-speed memory like L1, L2, and often L3 cache.</p> <ol> <li>Static RAM  </li> <li>Made up of transistors</li> <li>6 transistors to build 1 bit. </li> </ol> </li> </ul>"},{"location":"Courses/foos/memory/#ram-module","title":"RAM Module","text":"<p>When we're installing RAM, we've mostly come across this kind of component.  Usually, it comes as SO-DIMM(1) which consists of 3 majors parts:</p> <ol> <li>Small Outline Dual In-line Memory Module</li> </ol> <p>The PCB (1) with gold connectors and multiple layers of copper traces which provides connectivity  b/w memory controller on PC motherboard and DRAM chips for signal routing(2), power, and ground planes.</p> <ol> <li>Printed Circuit Board</li> <li>like Address lines to specify the input, Data lines for transferring data</li> </ol> <p>Next, you can see multiple black ICs mounted on PCB, known as DRAM chips.  Each DRAM chip stores a portion of the module\u2019s total memory capacity. Inside every DRAM chip, data is stored in memory cells arranged in Banks, Rows and Columns.</p> <ul> <li>Banks are independent memory region inside a DRAM chip, which allows parallelism: one bank can be precharging   while another is reading/writing. Each bank is further divided into a table like structure with multiple rows and    columns, where each row must be opened (activated) before reading or writing. </li> <li>Rows are horizontal slice of memory inside a bank. To read data from a row, the entire row needs to be activated    and pulled into row buffer/sense amplifiers. This design allow you to only access data from 1 open row per bank at a   time, because switching to a different row requires, PRECHARGE (close the current row) and ACTIVATE (open a new row)   commands. This is where \u201crow hit\u201d and \u201crow miss\u201d performance behavior comes from where<ul> <li>If the row is already open \u2192 row hit \u2192 fast read directly from row buffer.</li> <li>If a different row is open \u2192 row miss \u2192 slow as it needs PRECHARGE and ACTIVATE operations.</li> </ul> </li> <li>Columns represent the smaller divisions inside a row. After a row is activated, the read/write command specifies   column address to pick the exact data word out of the row buffer. You can read multiple columns in a single burst-read   operation (often burst length = 8).</li> <li>Cells are the fundamental storage unit of DRAM, build using a capacitor which stores the state (0/1) and    a transistor which controls the access to capacitor.</li> </ul> <p>Lastly, a small EEPROM chip called SPD (1) found at center of module above image.  It contains a small amount of non-volatile memory that stores configuration, timing, and identification data so the motherboard knows how to correctly initialize and run the memory. </p> <ol> <li>Serial Presence Detect</li> </ol>"},{"location":"Courses/foos/memory/#data-alignment","title":"Data Alignment","text":"<p>CPUs read data in fixed byte sized (1) blocks from its cache known as cache lines (2). For example, if a CPU with 64-byte cache lines needs to load address 150, it fetches the entire 64-byte block from  address 128\u2013191. </p> <ol> <li>4-, 8-, 16-, or 32-byte</li> <li>A cache line is the smallest block of memory that the CPU\u2019s cache can load from RAM and store internally.</li> </ol> <p>With this access pattern, if a variable lies across two cache lines, the CPU would need two memory accesses which is  slower and more complex to handle. To solve this problem, data alignment was introduced.</p> <p>With Data alignment, variables in memory are stored at addresses that are multiples of their size (or their required alignment boundary).</p> Typically aligned data type Data Type Size Common Alignment <code>char</code> 1 byte 1-byte aligned <code>short</code> 2 bytes 2-byte aligned <code>int32</code> 4 bytes 4-byte aligned <code>int64</code> / <code>double</code> 8 bytes 8-byte aligned SIMD types (e.g., 128-bit) 16 bytes 16-byte aligned <p>Check following diagram to see how alignment is done when storing variables, </p> <pre><code>flowchart TD\n\n    %% Main memory layout\n    subgraph Memory[\"Physical Memory Layout\"]\n        direction TB\n        A1000[\"1000: char a (1 byte)\"]\n        PAD[\"1001\u20131003: padding (3 bytes)\"]\n        A1004[\"1004\u20131007: int b (4 bytes)\"]\n    end\n\n    %% Flow connections (visual sequence)\n    A1000 --&gt; PAD --&gt; A1004\n\n    %% Side explanation nodes\n    NOTE1[\"Step 1: Store char a \u2192 placed at address 1000\"]\n    NOTE2[\"Step 2: Next free = 1001; int requires 4-byte alignment so add 3 bytes of padding to reach 1004 boundary\"]\n    NOTE3[\"Step 3: Store int b = 1 \u2192 stored at 1004\u20131007\"]\n\n    %% Attach side notes\n    NOTE1 -.-&gt; A1000\n    NOTE2 -.-&gt; PAD\n    NOTE3 -.-&gt; A1004\n</code></pre>  Use mouse to pan and zoom  <p>You might notice that padding in cache line would lower the utilization of each cache lines.   but the performance boost provided by such alignment far outweighs this cost. However, you can minimize padding, by decision like, reordering fields inside a struct or designing data layouts  manually for cache-line friendliness. Google used this technique to improve performance of Linux TCP/IP stack by 40% by reordering internal structs (blog). </p> <p>This alignment is managed by 3 cooperating layers:</p> <ol> <li>CPU at hardware level defines rules for alignments like valid addresses for each instruction, alignment required     for each data type size, and if the CPU can handle misaligned access natively.</li> <li>Compiler enforces the alignment when organizing memory during compilation by arranging address of various    objects like variables, struct fields, stack frames and heap allocations.</li> <li> <p>When you allocate memory dynamically like using <code>malloc</code>, the C standard requires <code>malloc</code> to return a pointer    suitable for storing any data type. Meaning it must be aligned to at least 8 or 16 bytes depending on platform.     So, the OS gives pages aligned to large boundaries (e.g., 4 KB) and memory allocator (1)    returns aligned chunks (8, 16, or 32 bytes).</p> <ol> <li>glibc, dlmalloc, jemalloc, etc.</li> </ol> </li> </ol>"},{"location":"Courses/foos/memory/#memory-management-with-virtual-memory","title":"Memory Management with Virtual Memory","text":"<p>Using only physical memory directly to manage processes creates several significant limitations:</p> <ol> <li>Fragmentation: Each process during initialization is provided a continuous block of memory for its operations.     As the process continues its execution, it\u2019d allocate and deallocate memory which results in creating small gaps of    unused memory. This leads to underutilization of memory, because even though we\u2019ve enough free space, we don\u2019t    have a continuous block.</li> <li>Shared Memory: implementing shared memory could poise security risk where one process can accidentally     (or maliciously) overwriting another\u2019s memory.</li> <li>Isolation: it\u2019s difficult to isolate access to memory for different process as all data is present on same    circuit</li> <li>Large Programs: Physical RAM is finite and processes must fit entirely in RAM for their execution which limits     the number of process at a time. Worse Large applications may not run at all if there isn\u2019t enough contiguous memory.</li> </ol> <p>Virtual Memory was designed to overcome these limitations. It's simply a fake/virtual continuous address space which  is mapped to real physical memory address on RAM. With this, you can allocate each process a continuous virtual address which when accessed would be translated to physical address by CPU. This translation is done using Page Table(1) and  MMU(2). </p> <ol> <li>Data structure which maps virtual address to physical address, created by OS Kernel. Each Process has its own    Page Table private to it.</li> <li>Memory Management Unit, the component in CPU responsible for translating the virtual address to physical address.</li> </ol> <p>This would solve all of above limitation:</p> <ol> <li>Solve fragmentation by grouping discrete memory location and mapping them together to a continuous virtual memory     address.</li> <li> <p>To provide Shared Memory between two different process, we can simply map the page table such that it points to     same physical address. This has variety of use cases like </p> <ul> <li>avoid storage of duplicate data in memory, for example the code section of same program with multiple processes.</li> <li>avoid reloading same libraries again, for example loading of <code>libc</code> library once and reusing the same memory     location in rest of the processes. You can see this mapping in linux using cat <code>/proc/[pid/maps</code> command. </li> <li>faster forking of new process/thread from multiprocessing and multi-threading by simply copying memory parent    to the child process. When the child process writes to memory, it\u2019ll be added at a separate location by updating    the page table using this concept is called CoW (Copy on Write).</li> <li>simple implementation of shared buffers in databases</li> </ul> </li> <li> <p>Guaranteed Isolation of Physical memory for each process. Because the page table will only point to memory     accessible by its process and this Table lives in Kernel Space which makes it secure from tinkering by bad agents.</p> </li> <li>It also solves problem of limited memory by allowing OS to use more memory than the capacity of RAM. This is done    using called Page Swap. Following diagram showcases how Page Swap is used to allocate space for new process      <pre><code>    sequenceDiagram\n        participant Process\n        participant OS as Kernel\n        participant Mem as Physical Memory\n        participant Disk as Swap Space\n\n        Process-&gt;&gt;OS: Request memory allocation\n        OS-&gt;&gt;Mem: Check available free frames\n        Mem--&gt;&gt;OS: Not enough free memory\n\n        OS-&gt;&gt;OS: Select victim pages (replacement policy)\n\n        OS-&gt;&gt;Mem: Identify victim page frame\n        Mem-&gt;&gt;Disk: Write victim page to swap (if dirty)\n        Disk--&gt;&gt;OS: Swap write complete\n\n        OS-&gt;&gt;Mem: Free the victim frame\n        OS-&gt;&gt;Disk: Read needed page from disk\n        Disk-&gt;&gt;Mem: Load required page into freed frame\n\n        OS-&gt;&gt;Process: Memory allocated successfully\n        Process-&gt;&gt;Process: Continue execution\n</code></pre>  Use mouse to pan and zoom      But when the swapped process returns for execution, Kernel needs to reload the swapped memory into RAM to for its     continued execution. This is done using Page Fault, which is explained below       <pre><code>    sequenceDiagram\n        participant Process\n        participant CPU\n        participant MMU as MMU&lt;br/&gt;(Memory Mgmt Unit)\n        participant OS as OS&lt;br/&gt;(Page Fault Handler)\n        participant Memory as Physical Memory\n        participant Disk as Secondary Storage\n\n        Process-&gt;&gt;CPU: Execute instruction\n        CPU-&gt;&gt;MMU: Request virtual address translation\n        MMU--&gt;&gt;CPU: Page Fault (page not in RAM)\n\n        CPU-&gt;&gt;OS: Trigger Page Fault Interrupt\n        OS-&gt;&gt;OS: Determine victim page (replacement policy)\n        OS-&gt;&gt;Memory: Mark victim page for eviction\n        Memory-&gt;&gt;Disk: Write victim page to disk (if dirty)\n\n        OS-&gt;&gt;Disk: Read required page into memory\n        Disk-&gt;&gt;Memory: Load page into freed frame\n\n        OS-&gt;&gt;MMU: Update Page Table (new mapping)\n        OS--&gt;&gt;CPU: Return from interrupt\n\n        CPU-&gt;&gt;MMU: Retry memory access\n        MMU-&gt;&gt;Process: Provide physical address\n        Process-&gt;&gt;Process: Continue execution</code></pre>  Use mouse to pan and zoom  </li> </ol> <p>The tradeoff of using Virtual Memory:</p> <ul> <li> <p>Increased overhead and slower performance of memory. Virtual memory requires Page table lookups,    TLB (1) management, Page fault handling. When a page fault occurs, the OS must fetch data   from disk \u2014 which is millions of times slower than RAM. This can significantly slow system performance.</p> <ol> <li>Translation Lookaside Buffer </li> </ol> </li> <li> <p>When the system spends more time swapping pages in and out of disk than executing actual processes, it enters a state   called thrashing which drastically reduces performance. This happens when either working sets don\u2019t fit in RAM    or we're running too many processes simultaneously.</p> </li> <li>Introduces higher complexity in OS and hardware. Virtual memory requires Page tables, TLB support, Sophisticated   algorithms for replacement, allocation, protection. This makes hardware (MMU) and OS design more complex.</li> <li>Large processes require very large page tables. Maintaining and storing them consumes a lot of memory and CPU time   (updating entries, context switching). Even optimized designs (e.g., multi-level page tables) add complexity.</li> </ul>"},{"location":"Courses/foos/memory/#direct-memory-access-dma","title":"Direct Memory Access (DMA)","text":"<p>Earlier days, moving data between I/O devices(1) and memory would've to be done through CPU which involved reading a  chunk of data from one side to CPU and copying it over from CPU to another side, and this process is repeated for every piece of data. This is extremely slow and wastes CPU time, specially when moving large amount of data.  But with DMA, the CPU can delegate the transfer, freeing the CPU to do other work.</p> <ol> <li>like disk controllers, network cards, sound cards, GPUs, etc.</li> </ol> <p>DMA is a hardware feature that allows certain components of a computer (mainly I/O) to transfer data directly to or from main memory (RAM) without involving the CPU for every byte or word.</p> <p>Here\u2019s the general flow of a DMA operation: </p> <pre><code>sequenceDiagram\n    participant CPU\n    participant DMA as DMA Controller\n    participant Device as I/O Device\n    participant Mem as Main Memory\n\n    CPU-&gt;&gt;DMA: Configure DMA (source, destination, size, direction)\n    CPU-&gt;&gt;DMA: Start DMA operation\n    CPU--&gt;&gt;CPU: Continue other tasks\n\n    DMA-&gt;&gt;Device: Request data (or prepare to send)\n    Device--&gt;&gt;DMA: Provide/receive data stream\n\n    DMA-&gt;&gt;Mem: Transfer data directly to/from memory\n    Note over DMA,Mem: Bus arbitration, DMA controls the bus&lt;br/&gt;(\"cycle stealing\" or burst mode)\n\n    DMA-&gt;&gt;CPU: Interrupt \u2192 DMA transfer complete\n    CPU-&gt;&gt;CPU: Handle completion and resume tasks\n</code></pre>  Use mouse to pan and zoom  <p></p> Bus Arbitration <p>Computers have multiple components that may want to read/write memory but only one can use the bus at a time, because the memory bus is a shared resource. Bus arbitration helps determines which device gets control of the system bus when multiple components want to use it at the same time. There are various arbitration strategies  depending on factors like fairness, stability and priority, the most commonly used strategy is  Centralized Bus Arbitration(1).</p> <ol> <li>uses a dedicated chip (or circuit inside the chipset) that acts as the bus arbiter. The arbiter receives     requests from multiple devices and decides based on, priority levels, fairness algorithms and current bus load.</li> </ol> <p>Since bus arbitration affects the working of CPU, DMA provide different mode of operations for different use cases:</p> <ol> <li>Burst Mode DMA transfers an entire block at once which is fast but this would block the bus for longer duration,    which blocks CPU from accessing memory.</li> <li>Cycle Stealing DMA steals a bus cycle occasionally so the CPU can still work providing a balanced performance.</li> <li>Transparent DMA: Runs only when CPU is not using the bus which makes transfer slow, but it avoids any     interference with CPU.</li> </ol> <p>Modern systems which involves transferring huge amounts of data, like streaming high quality video, reading/writing large files/network packets would consume 100% of CPU time if done manually. Using DMA in such systems is crucial as it provides high throughput and low CPU overhead. But using DMA can be a security risk as it works directly with Physical Memory Address. To protect memory from unauthorized DMA, use IOMMU (1) which limits what memory addresses a DMA device can access.</p> <ol> <li>Input-Output Memory Management Unit</li> </ol> Analyzing memory usage in Linux using <code>top</code> <p>You can use <code>top</code> command in Linux to analyze memory usage. When executed, output at top of screen would be similar to following table. </p><pre><code>MiB Mem :  7890 total,  5320 used,  1020 free,   450 buff/cache\nMiB Swap:  2048 total,   220 used,  1828 free,    40 avail Mem\n</code></pre> The key fields are:<p></p> <ul> <li><code>total</code> -&gt; total amount of system RAM or swap.</li> <li><code>used</code> -&gt; memory currently in use by processes + OS/internal usage. Linux uses free memory for caching,     so <code>used</code> often appears high which is normal.</li> <li><code>free</code> -&gt; Memory not used for anything at the moment.</li> <li><code>buff/cache</code> -&gt; Memory used for buffers (block device metadata) and Cache (file system cache). This memory is      reclaimable, meaning Linux can free it when applications need more RAM.</li> <li><code>avail Mem</code> -&gt; estimated memory realistically available for new apps without swapping.</li> </ul> <p>Below which you'll see a list of running process with following headers </p><pre><code>PID  USER   PR  NI   VIRT   RES   SHR  S  %MEM  COMMAND\n</code></pre><p></p> <ul> <li><code>VIRT</code> -&gt; Virtual Memory Size, total virtual memory used by the process, including, code, data, shared libraries,    mapped files and swap if used. Not all of this is physically in RAM.</li> <li><code>RES</code> -&gt; Resident Set Size, amount of actual physical memory the process is using.</li> <li><code>SHR</code> -&gt; Shared Memory, is the portion of memory that is shared with other processes (libraries, shared memory segments).</li> <li><code>%MEM</code> -&gt; percentage of physical RAM this process uses.</li> </ul>"},{"location":"Courses/foos/process/","title":"Program and Process","text":""},{"location":"Courses/foos/process/#program-and-process","title":"Program and Process","text":"<p>Process are logical construct used to encapsulate a running program. It's made up of all the metadata and content required by OS to manage and run a program, like entrypoint of program, the shared libraries which needs to be loaded, user data, program instructions, etc.</p> <p>But how does OS understand where to get these information? To understand this, we'll need to look into the layout of program files.</p>"},{"location":"Courses/foos/process/#program","title":"Program","text":"<p>Program is an executable file which contains instructions for kernel about how to execute it\u2019s work.  These instructions/codes are complied (1) and linked (2) for a CPU.</p> <ol> <li>Complied means the instruction should be converted into codes understandable by CPU (which are 0s and 1s).</li> <li>Linked means combining all different libraries and source object files into a single execution file. This can be    done statically or dynamically at runtime.</li> </ol> Dynamic and Static Linking <p>You might notice this difference in linking when copying games. When we copied just the executable of a game from some friend and try to execute it on our laptop we\u2019d get an error saying some DLL files are missing. These DLL files are libraries in windows which are linked in the executable dynamically. We could also statically combine all these libraries in same executable, but that\u2019d increase the size of <code>exe</code> drastically which is why  they're kept dynamic for ease of distribution.</p> <p>This executable file has specific format, (for example Unix like OS uses ELF (1) layout for its binaries) which the  OS understands, using which it can load the process with required content and metadata to execute provided instructions.</p> <ol> <li>Executable and Linked Format</li> </ol>"},{"location":"Courses/foos/process/#elf","title":"ELF","text":"<p>ELF is the standard binary format used on Linux/Unix systems for, Executable programs, Shared libraries (<code>.so</code>), Object files (<code>.o</code>) and Core dumps.  It describes how code, data, libraries, and metadata are stored so the OS loader can load and run the program. At a high level, an ELF file has following structural layers: </p><pre><code>+-----------------------+\n| ELF Header            |  \u2190 describes the whole file\n+-----------------------+\n| Program Headers       |  \u2190 used at runtime for loading\n+-----------------------+\n| Sections (e.g., .text, .data, .bss)   |\n+-----------------------+\n| Section Headers       |  \u2190 used by linkers/debuggers\n+-----------------------+\n</code></pre><p></p> <ul> <li> <p>ELF Header contains information like the magic number <code>0x7F 45 4C 46</code> (1), CPU architecture for which the file is compiled, type of file, entrypoint   address of program, offsets for program and section header.</p> <ol> <li><code>0x7F  'E'  'L'  'F'</code> in ASCII, used to identify the file as an ELF binary and allows the kernel loader to recognize    and parse it.</li> </ol> </li> <li> <p>Program Headers describes various segments to be mapped into process memory by OS loader. Each segment specifies its    type, file offset, size in file, size in memory, and memory permission (r/w/x  operations). For example,</p> <ul> <li><code>PT_LOAD</code> -&gt; used to load data and instructions into process memory. For example, mapping code segment using    <code>.text</code> and <code>.rodata</code> sections with <code>r-x</code> permission. Similarly, data segments are loaded from <code>.data</code>, <code>.bss</code>    with <code>rw-</code> permission.</li> <li><code>PT_INTERP</code> -&gt; specifies the dynamic loader for dynamically linked executables. Like <code>/lib64/ld-linux-x86-64.so.2</code>   for x86_64 Linux.</li> <li><code>PT_DYNAMIC</code> -&gt; contains <code>.dynamic</code> section data used by dynamic loader. </li> </ul> </li> <li> <p>Sections and Section Headers consists of file content used for linking and debugging purpose only. They hold content   as raw data from various part of program, like </p> <ul> <li><code>.text</code> -&gt; executable machine code instructions</li> <li><code>.data</code> -&gt; initialized global and static variables</li> <li><code>.bss</code> -&gt; uninitialized global and static variables</li> <li><code>.rodata</code> -&gt; read-only data</li> <li><code>.symtab</code> -&gt; symbol table</li> </ul> </li> </ul> <p>Checkout below sequence diagram to understand complete flow of loading a program into process using ELF format:</p> <pre><code>sequenceDiagram\n    participant User as User / Shell\n    participant Kernel as Linux Kernel\n    participant ELF as ELF Executable\n    participant Loader as Dynamic Loader (ld.so)\n    participant Proc as New Process\n\n    User-&gt;&gt;Kernel: execve(\"program\", argv, envp)\n    Kernel-&gt;&gt;ELF: Read ELF Header&lt;br/&gt;Check magic: 0x7F 'E' 'L' 'F'\n    Kernel-&gt;&gt;ELF: Read Program Header Table\n    Kernel-&gt;&gt;Proc: Create new process&lt;br/&gt;Create address space\n\n    loop For each PT_LOAD segment\n        Kernel-&gt;&gt;Proc: Map PT_LOAD segment&lt;br/&gt;into virtual memory (R/W/X)\n    end\n\n    alt ELF contains PT_INTERP?\n        ELF-&gt;&gt;Kernel: PT_INTERP = \"/lib64/ld-linux-x86-64.so.2\"\n        Kernel-&gt;&gt;Loader: Load dynamic loader&lt;br/&gt;Map its segments\n        Kernel-&gt;&gt;Loader: Transfer control&lt;br/&gt;to loader entry point (user mode)\n        Loader-&gt;&gt;Proc: Resolve shared libs&lt;br/&gt;Relocations &amp; symbol binding\n        Loader-&gt;&gt;Proc: Jump to ELF entry point (_start)\n    end\n\n    Kernel-&gt;&gt;Proc: Set up user stack&lt;br/&gt;argc, argv, envp, auxv\n    Kernel-&gt;&gt;Proc: Jump directly to&lt;br/&gt;ELF entry point (_start)\n    Proc--&gt;&gt;User: Program runs (_start \u2192 main)\n</code></pre>  Use mouse to pan and zoom  <p>Now you have overview on how a program is loaded into memory to form a Process, let's understand how Process executes instructions in program to perform the coded work.</p>"},{"location":"Courses/foos/process/#process","title":"Process","text":"<p>A process is what a program becomes after the kernel loads its segments, sets up virtual memory (code, data, heap, stack), prepares registers &amp; CPU state, and begins executing at the entry point. It's footprint can be divided into 3 major categories:</p> <ol> <li> <p>User-Space Memory, which is a private memory space assigned to process during its creation. It's a continuous chunk of    virtual memory associated with a high and low memory address. You can check below memory layout used for typical linux process     </p><pre><code>+-------------------------------+ High address\n|        Stack (grows \u2193)        |   |\n+-------------------------------+   |\n|  Memory-mapped region (mmap)  |   |\n|  \u2190 Shared libraries live here |   |\n|  \u2190 also VDSO, ld.so, JIT code |   |\n+-------------------------------+   |\n|        Heap (grows \u2191)         |   |\n+-------------------------------+   |\n|  .bss / .data / .text         |   \u2193\n+-------------------------------+ Low address \n</code></pre><p></p> </li> <li> <p>CPU Execution Context, the state of CPU needed to resume the process after a context switch. It includes values for various    pointers like <code>pc</code>(1), <code>sp</code>(2), <code>bp</code>(3), etc. We'll discuss how these pointers are used later.</p> <ol> <li>Program Counter, points next instruction to execute</li> <li>Stack Pointer, top of user stack</li> <li>Base Pointer, points to start of currently executing function frame</li> </ol> </li> <li> <p>PCB (1) which stores information about process like ids (PID, PPID, UID, GID(3)), scheduling info,    MMU (2) structs, table of open file descriptors, process running status and threading info. This area of memory    is only accessible to Kernel for security purposes.</p> <ol> <li>Process Control Block</li> <li>Memory Management Unit, describing the virtual memory mapping to physical. </li> <li>Process ID, Parent Process ID, UserID, GroupID </li> </ol> </li> </ol> <p>To get a simple understanding how process execution happens, go through below diagram which we'll continue to explore  in depth.</p> <pre><code>sequenceDiagram\n    autonumber\n\n    participant Kernel\n    participant Memory\n    participant CPU\n\n    %% 1. Process created and text loaded\n    Kernel-&gt;&gt;Memory: Loads and map process memory\n    Kernel-&gt;&gt;CPU:  Sets initial values to CPU registers (PC -&gt; entrypoint, SP -&gt; top of stack)\n    Kernel-&gt;&gt;CPU: Schedules process on CPU\n\n    %% 3. CPU fetches instruction into IR\n    CPU-&gt;&gt;Memory: Fetch instruction at PC address\n    Memory-&gt;&gt;CPU: Load instruction set into IR  (instruction register)\n\n    %% 4. Execute and increment PC\n    CPU-&gt;&gt;CPU: Execute instruction and Increment PC to point next instruction\n\n    %% 5. Use cache for next instruction\n    CPU-&gt;&gt;CPU: Check LCaches for next instruction\n    alt Instruction cached\n        CPU-&gt;&gt;CPU: Load instruction from cache\n    else Not cached\n        CPU-&gt;&gt;Memory: Fetch instructions in burst\n        Memory-&gt;&gt;CPU: Fill cache line\n        CPU-&gt;&gt;CPU: Provide next instruction\n    end\n\n    %% 6. PC not saved unless context switch\n    CPU--&gt;&gt;Kernel: Continue executing until preemption\n    Kernel-&gt;&gt;Memory: Save registers on stack only during context switch\n    Memory-&gt;&gt;CPU: Restore registers from stack when rescheduled\n\n</code></pre>  Use mouse to pan and zoom"},{"location":"Courses/foos/process/#stack","title":"Stack","text":"<p>Stack as seen above is part of User-Space Memory of process. The primary role of stack is to keep track of function calls such that CPU can jump to previous function after completing current function. This is achieved by using various pointers like <code>sp</code>(1), <code>bp</code>(2), <code>lr</code>(3), etc. Other roles includes storing functions local variables or temporary register values onto stack.</p> <ol> <li>Stack Pointer, CPU register which points to end of current function frame.</li> <li>Base Pointer, CPU register which points start of function frame.</li> <li>Link Register, CPU register which stores the address of instruction after function call. </li> </ol> <p>To reference variables stored in stack, you can use <code>sp</code> since its dynamic and keeps changing. As such another pointer <code>bp</code> is used. Since <code>bp</code> constantly points to top of function frame, you can easily reference variable address relative  to it, for example <code>a-&gt;bp</code>, <code>b-&gt;bp-4</code>, <code>c-&gt;bp-8</code> (where size of each variable is 4 bytes). With this information, we can explain how stack is used when executing a process,</p> <ol> <li> <p>When we call a new function,     </p> <pre><code>    sequenceDiagram\n        autonumber\n\n        participant CPU as CPU\n        participant SP as SP (Stack Pointer)\n        participant BP as BP (Base Pointer)\n        participant Stack as Stack Memory\n\n        note over CPU,Stack: Function call instruction is executed\n        %% Function Call\n        CPU-&gt;&gt;Stack: push return address\n        note right of SP: SP moves DOWN&lt;br/&gt;(decrement)\n        SP--&gt;&gt;SP: SP = SP - addr_size\n\n        CPU-&gt;&gt;Stack: push old BP\n        note right of SP: SP moves DOWN again\n        SP--&gt;&gt;SP: SP = SP - addr_size\n\n        CPU-&gt;&gt;BP: BP = SP\n        note right of BP: New frame base established\n\n        %% Inside Function\n        CPU-&gt;&gt;Stack: allocate locals (SP = SP - frame_size)\n        note right of SP: SP moves DOWN for locals\n\n        note over CPU,Stack: Execution continues using locals&lt;br/&gt;until function prepares to return</code></pre>  Use mouse to pan and zoom  <p></p> <p>Some CPU architecture like ARM consists of <code>lr</code> which stores the return address for current frame. Others      like x86 don't have such registers, as such the return address is pushed onto stack. </p> </li> <li> <p>While we're in a function, we can store local function variable or temporary register values like <code>lr</code> or <code>bp</code> in stack    locally and reference them w.r.t <code>bp</code> of frame.    </p> <pre><code>   sequenceDiagram\n       autonumber\n\n       participant CPU as CPU (Executes Instructions)\n       participant SP as SP (Stack Pointer)\n       participant BP as BP (Base Pointer)\n       participant Stack as Stack Memory\n\n       %% Function has already been called\n       note over CPU,Stack: We are now INSIDE the function&lt;br/&gt;Stack frame has been created\n\n       CPU-&gt;&gt;Stack: Reserve local variable &lt;br/&gt;e.g., int x\n\n       %% Storing variables\n       CPU-&gt;&gt;Stack: Write value of local var x&lt;br/&gt;(at BP - offset)\n       Stack--&gt;&gt;CPU: Store complete\n\n       %% Using local variables\n       CPU-&gt;&gt;Stack: Read local x via (BP - offset)\n       Stack--&gt;&gt;CPU: Return value of x\n\n       %% Temporary values / spills\n       CPU-&gt;&gt;Stack: Spill register value to stack\n       SP--&gt;&gt;SP: SP moves DOWN (push)\n       Stack--&gt;&gt;CPU: Load spilled value later (pop)\n       SP--&gt;&gt;SP: SP moves UP (pop)\n\n       note over CPU,Stack: Execution continues using locals&lt;br/&gt;until function prepares to return</code></pre>  Use mouse to pan and zoom  <p></p> </li> <li> <p>When we return from a function,      </p> <pre><code>    sequenceDiagram\n        autonumber\n\n        participant CPU as CPU (Executes Instructions)\n        participant SP as SP (Stack Pointer)\n        participant BP as BP (Base Pointer)\n        participant Stack as Stack Memory\n\n        note over CPU,Stack: Function has finished executing&lt;br/&gt;Now preparing to RETURN to caller\n\n        %% Step 1 \u2014 Deallocate locals\n        CPU-&gt;&gt;SP: Move SP back to BP (SP = BP)\n        note right of SP: SP jumps UP&lt;br/&gt;removing local variables\n\n        %% Step 2 \u2014 Restore old BP\n        CPU-&gt;&gt;Stack: pop saved BP\n        Stack--&gt;&gt;BP: Write old BP into BP register\n        note right of BP: BP now points to caller's frame\n\n        %% Step 3 \u2014 Load return address\n        CPU-&gt;&gt;Stack: pop return address\n        Stack--&gt;&gt;CPU: Return address loaded\n\n        %% Step 4 \u2014 Jump back to caller\n        CPU-&gt;&gt;CPU: RET instruction&lt;br/&gt;PC = return address\n        note over CPU,Stack: CPU resumes executing caller function</code></pre>  Use mouse to pan and zoom  <p></p> </li> </ol> <p>Access to memory is very costly for CPU, but having them laid out next to each other helps a lot due to caching few next required instruction/variable with single burst. Also, Memory allocation and deallocation in stack is managed using <code>sp</code> is very fast, </p> <ul> <li>to allocate new memory, you increase the <code>sp</code> and give the new space to required variables/functions.</li> <li>to clear up memory, you can decrement the <code>sp</code> to mark the memory outside it as garbage which can then be cleared or   overwritten. </li> </ul> <p>Best coding practices</p> <p>Few takeaway from understanding this design of execution:</p> <ul> <li>Function calls are expensive, as we\u2019ve to move around data between register and stack memory.    So avoid using too many function without any cause. Compilers even optimize this by using inlining, where   it inserts machine code of a function inline to where it was called but this also bloats the code if used too much.</li> <li>Stack has limited space, which protects the process from infinite function calls in case of recursions.</li> <li>Avoid using large local variables, every step which requires fetching value from memory is considered expensive in CPU.    </li> </ul>"},{"location":"Courses/foos/process/#data-section","title":"Data Section","text":"<p>Fixed size section in memory layout of process which is responsible for storing program instructions, constants and  global variables. Its size is determined by compiler during compilation using static analyses of code. The section is further divided into following subsections:</p> <ul> <li><code>.text</code> memory section stores program instructions, function bodies, CPU opcodes. It's only given read-execute    permission for security, so that any marlware can't edit code during its execution. Program Counter (<code>pc</code>) fetches   instructions from this section, which are then decoded and executed by CPU. </li> <li><code>.rodata</code> stores read-only data like constant variables, string literals, etc. This separate memory section is created   so that any constant values isn't modified accidentally. Another benefit can be ease of sharing same data with other processes.</li> <li><code>.data</code> stores initialized global variables. Since these variables are available across all functions, CPU directly   references them with ease. </li> </ul> <p>The variables stored in different sections are addressed using offset based on start of data section . And the offset is calculated by compiler during compilation.</p>"},{"location":"Courses/foos/process/#heap","title":"Heap","text":"<p>Heap section is responsible for storing/referencing large dynamic variables in memory. However, the data needs to be removed explicitly, and if not handled properly you might have memory leaks (1). It grows from lower to higher memory address. Kernel provides you with 3 APIs to manage memory in heap: <code>malloc</code>, <code>free</code> and <code>new</code>. </p> <ol> <li>When data in memory isn\u2019t referenced by any function in stack.</li> </ol> <p>To access data stored in heap, you\u2019ve to use Pointers. Pointers are variables which stores memory address of first byte of data stored in heap. And based on the type of pointer which tells us the size of data, we can fetch the  required bytes to get complete data.</p> <p>During memory allocation, we\u2019ve to specifying the memory size required. But freeing memory doesn't require you to mention the memory size. This is done using fixed size headers attached to pointer location which holds metadata on the allocated memory. Kernel uses that information to determine how much memory to free. Also, whenever we ask Kernel to allocate some memory, it\u2019ll always return it in some multiple of memory page size and not the exact memory size asked.</p> <p>Few things to know when using heap:</p> <ul> <li>Memory leak: When memory isn\u2019t freed up, the Kernel will still keep the data in memory even if it isn\u2019t used in    any function. This leads to unwanted memory growth known as memory leak. High level programming language uses garbage    collection algorithms to avoid this, one of which is <code>refcounting</code> which stores the number of references in use to    the data within the header. If the reference becomes 0, means nothing is pointing to this data and as such garbage   collector can free up this place.</li> <li>Dangling Pointers: When the original data your pointer references is freed up (like in a downstream function    call) and when you try to access it, you\u2019ll read random headers leading to errors like segfault. For example,    when you try to free a pointer twice, it leads to the crash of process.</li> <li> <p>Performance: Heap is slower compared to stack because you\u2019ve to go to allocate memory, read headers to fetch the   data, and free memory while stack doesn\u2019t involve such tedious process. Stack also has locality of related data which   are cached when reading in burst while heap is unorganized. Stack space is limited but heap can grow.</p> Google TCP/IP Performance Boost <p>Google improved performance of TCP/IP stack in Linux Kernels by 40% just by re-ordering the variables in order    they were accessed by kernel. This significant improvement was due to the locality of data which resulted in    caching from burst. So at Kernel level, always try to cache things and not take memory for granted, going to   memory might look fast for one instruction but over millions of instruction these things add up.</p> </li> <li> <p>Escape Analysis: Some languages (like Java, Go) allocate memory within stack itself whenever possible to avoid   the cost of heap. They\u2019ll create a pointer which points to a memory location within stack itself. This is mostly   applicable in places where we don\u2019t pass a pointer outside current function.</p> </li> <li>Program Break: Older version used <code>brk</code> / <code>sbrk</code> functions to allocate/deallocate memory from heap which basically   added a break at top of heap and whenever a section isn\u2019t used this memory would be deallocated. This is inefficient   because data is placed/freed randomly in heap, as such it's very rare for block to go completely free.    It was later updated to <code>MMAP</code> which resolved this issue.</li> </ul> <p>View process layout in Linux</p> <p>In Linux, you can view the internals of process using the command <code>cat /proc/{PID}/</code>. This exposes API to view metadata of process which can be used to create tools like resource monitors.  Also <code>/proc</code> isn\u2019t a physical file system present on disk, its only present in memory.</p> <p>CPU-Context and Kernel-Space for process will be discussed in following chapters to keep them in flow with respective topic.</p>"},{"location":"Courses/foos/process_mgmt/","title":"Process Management","text":""},{"location":"Courses/foos/process_mgmt/#process-management","title":"Process Management","text":"<p>Earlier we'd discussed how programs are executed using Processes, but there we only went through how process are loaded, organized and executed in memory for execution. From last chapter we learned how CPU executes those instructions in process memory space, where single  CPU core only runs one process at a time. But in reality computers are made for multitasking,  where we can run multiple tasks concurrently as if they're executed simultaneous. This is done by rapidly switching the  CPU's attention between them, creating the illusion of continuity. While multitasking significantly boosts efficiency and user productivity, it requires sharing resources like the CPU and memory between different processes at the same time. This responsibility of sharing resources b/w multiple process is delegated to OS due to few key reason:</p> <ul> <li>Security: Each process now run securely without worrying about malicious process overwriting their resource,    since is responsible for managing resource sharing. Process can only access the hardware through Kernel, which   makes sure each process is only working in their bounds.</li> <li>Scheduling: Kernel makes sure that each process is scheduled as per defined scheduling algorithm, such   that no single process can keep hogging the resource for themselves.</li> <li>Stability: Since process are isolated by Kernel, crash in one process wouldn't impact other running process.   Additionally, Kernel can detect such crash and perform clean up of resources so that they're available for others.</li> </ul> <p>Basically, delegating resource sharing for running multiple process to kernel allows system to run programs safely, fairly, efficiently, and securely. To dive deeper, let's explain how and what key responsibilities does Kernel manage.</p>"},{"location":"Courses/foos/process_mgmt/#process-creation","title":"Process Creation","text":"<p>The kernel is responsible for creating new processes (as explain previously. In additional to  that, Kernel stores additional metadata use for management of process in a protected space only accessible by Kernel. This area of memory is called PCB (Process Control Block) and its created and maintained for each process. It stores information such as Process ID (PID), CPU Context, scheduling information, page table, open file descriptors, process state (running, ready or blocked).</p> Kernel Space <p>PCB shouldn't be confused with Kernel Space which is protected region of memory where only kernel can execute.  It's used for storing kernel code and data structure (like PCB), device drivers, system call handlers, or any  other critical instructions/information which needs secure execution. </p> <p>On Unix-like systems, a new process is usually created using <code>fork()</code>, which duplicates the parent process and then  <code>exec()</code> replaces the child\u2019s memory space with a new program. On Windows, <code>CreateProcess()</code> sets up a process directly.</p>"},{"location":"Courses/foos/process_mgmt/#scheduling","title":"Scheduling","text":"<p>To decide which process gets the CPU and for how long, kernel uses a mechanism which picks process to run next and  process to stop for switching with another. This mechanism is implemented by scheduler. Without scheduler, your computer would only be able to run as many process as the number of cores in CPU at a time.     </p> <p>This requires Kernel to manage additional metadata about process which tells its current state (like (1)). Since this information is mainly used by scheduler, it's kept in PCB.  Also, the scheduler we're talking about here is CPU  scheduler which ready process gets the CPU next, Kernel also have other kind of schedulers (2).</p> <ol> <li>Ready -&gt; can run, waiting for CPU, Running -&gt; currently on CPU, Blocked -&gt; waiting for I/O, timer, lock, etc.</li> <li>like Job scheduler used in multiprogramming to control the number of processes to admit into the system, medium-term    scheduler which can temporarily swap out processes from RAM to disk (suspend) to free memory and later swap them back in.</li> </ol>"},{"location":"Courses/foos/process_mgmt/#context-switching","title":"Context Switching","text":"<p>Once the scheduler picks a process and schedules it for execution in CPU, we need to save the CPU state of currently  running process so that its execution can be resumed from where it left CPU when scheduled again. This responsibility is handled by dispatcher which saves the CPU state of the currently running process into its PCB and then loads the saved state of the next process from its PCB. This whole operation is called a context switch. </p> <p>Additionally during a context switch, the kernel must flush or reload certain CPU structures private per-process. For example, flushing TLB and PTBR (Page Table Base Register) since TLB stores page table mapping for process  and PTBR stores address pointing to page table. Since these components are used for virtual address translation, keeping older mapping might lead to incorrect translation. As flushing and restoring TLB costs additional CPU cycles, modern CPUs introduced Address-Space Identifiers which avoid flushing whole TLB unless necessary.</p> Address-Space Identifier <p>Many CPUs now support address-space identifiers (like <code>ASID</code> -&gt; ARM and <code>PCID</code> \u2192 x86-64) which tag each TLB entry with the ID of the process\u2019s address space. So instead of flushing the TLB, kernel assigns a unique <code>ASID</code>/<code>PCID</code> to each process. On a context switch, the CPU simply switches <code>ASID</code>/<code>PCID</code>. This keep TLB entries from different processes coexist safely.</p> <p>Context switching is pure overhead as no useful work is done. If done infrequently it can lead to bad responsiveness and wasted CPU cycles. </p>"},{"location":"Courses/foos/process_mgmt/#scheduling-policies","title":"Scheduling Policies","text":"<p>Different users might have different requirements for running their workload. For example, an interactive systems will prioritize response time to get fast response for a request while a batch systems requires higher throughput, turnaround to get higher number of finished process per unit time.</p> Commonly used metrics <ul> <li>CPU Utilization \u2013 keep CPU as busy as possible.</li> <li>Throughput \u2013 number of processes finished per unit time.</li> <li>Turnaround Time \u2013 time from process submission to completion.</li> <li>Waiting Time \u2013 total time a process spends in the ready queue.</li> <li>Response Time \u2013 time from request (e.g., key press) to first response.</li> </ul> <p>To help with this, Kernel allows users to switch the scheduling algorithm. Few commonly used algorithm are:</p> <ul> <li>FCFS (First-Come, First-Served) in which processes are executed in arrival order. It's simple, but one long job   delays all smaller ones. On average, this policy leads to higher waiting time per process.</li> <li>SJF (Shortest Job First) pick the process with the shortest CPU burst next. This minimizes average waiting time   (proven) but introduces complexity for predicting burst time.</li> <li>Priority Scheduling allows each process to be associated with a priority number. Scheduler always picks the   highest-priority ready process. This may cause starvation in low priority processes, which can be avoided by    gradually increase priority of waiting processes (ageing).</li> <li>RR (Round Robin) allocates each process a fixed amount of execution time. Processes are kept in circular ready    queue in which are scheduler adds process at the end as their time expires. This ensures fairness and responsiveness,   but may require tuning in allowed time. If it's too small, we'll have too many context switches (overhead) and    too large value would reduce responsiveness.</li> <li>CFS (Completely Fair Scheduler) used by Linux tries to give each process a fair share of CPU time,    proportional to its priority. It maintains a balanced tree structure of runnable tasks, ordered by how    much CPU time they\u2019ve used and the task that\u2019s had the least CPU time runs next.</li> </ul> <p>All these algorithms can be broadly categorized into two categories, Preemptive and Non-preemptive. In Non-preemptive scheduling, once a process gets the CPU, it keeps it until it finishes or blocks. The scheduler can't forcibly take the CPU away. It's simpler, but bad for responsiveness (a long job can block everyone). It's counterpart, Preemptive scheduling allows kernel to interrupt a running process and give CPU to another.  Usually done via a timer interrupt, it provides better responsiveness for interactive tasks. Almost all modern OSes use preemptive scheduling.</p> <p>To conclude, good scheduling keeps the system responsive, maximizes its CPU utilization, prevents resource starvation and ensures fairness among processes.</p>"},{"location":"Courses/foos/process_mgmt/#memory-management","title":"Memory Management","text":"<p>As previously discussed (here), Kernel manages memory using Virtual memory, Page tables for address translation. It also handles mechanism like  Page Fault where kernels brings needed page from disk into RAM,  if swapped out memory is accessed.</p>"},{"location":"Courses/foos/process_mgmt/#copy-on-write","title":"Copy On Write","text":"<p>Another concept we hadn't discussed previous is CoW (Copy On Write). It's an optimization technique used by kernels during process creation where the child and parent process shares same physical memory pages. This dramatically reduces memory usage because most processes never modify most of their inherited pages, while also increasing  performance of child creation. To fully understand how this works, checkout below sequence diagram. </p> <pre><code>sequenceDiagram\n    participant Parent\n    participant Kernel\n    participant Child\n    participant Memory as Physical Memory\n\n    Note over Parent: Parent calls fork()\n\n    Parent-&gt;&gt;Kernel: fork()\n    Kernel-&gt;&gt;Memory: Mark parent pages as shared (read-only)\n    Kernel-&gt;&gt;Child: Create child process&lt;br/&gt;Share parent's pages (read-only)\n    Child--&gt;&gt;Parent: fork() returns in both processes\n\n    Note over Parent,Child: Both processes now share the same physical pages\n\n    Child-&gt;&gt;Kernel: Write to a shared page\n    Kernel--&gt;&gt;Child: Page Fault (Write on read-only page)\n\n    Kernel-&gt;&gt;Memory: Allocate new private physical page for Child\n    Kernel-&gt;&gt;Memory: Copy shared page contents\n    Kernel-&gt;&gt;Child: Update child's page table \u2192 writable\n    Kernel--&gt;&gt;Child: Retry instruction (write now succeeds)\n\n    Note over Child: Child now has its own private copy\n    Note over Parent: Parent still uses original page</code></pre>  Use mouse to pan and zoom"},{"location":"Courses/foos/process_mgmt/#ipc","title":"IPC","text":"<p>Many applications use multiple processes for different features, which all needs to coordinate with each other to perform their work efficiently. For example, Browsers use different process for networking and rendering which all needs to communicate with each other to load web pages seamlessly. But since processes are isolated, they can't access each other\u2019s memory/resources securely. To solve this, Kernel uses IPC which provides controlled, secure ways  for processes to share data, coordinate actions, and work together. For example,</p> <ul> <li>Processes may need to coordinate actions like accessing a shared resource or ensuring tasks run in a specific order.   For them, IPC provides synchronization tools such as Semaphores, Mutexes, Signals which helps in avoiding   inconsistency and corrupting data.</li> <li>Processes may need to exchange information like a shell pipeline (ls | grep abc) passes data from one process to   another or a microservices components exchange messages. IPC provides many mechanisms for fast and structured data   exchange like Shared memory, Pipes, Message queues and Sockets. Here, shared memory is the fastest, as   it lets processes communicate without extra copying.</li> </ul> <p>All these mechanism also enforce permissions so that communication is only done through authorized processes. This way, critical system processes can safely interact with others processes while preventing malicious  programs from injecting harmful data into others.</p>"},{"location":"Courses/foos/process_mgmt/#system-calls","title":"System calls","text":"<p>User processes can\u2019t access hardware directly, they must request it through kernel using system calls(1).  System calls act as the interface between user processes and the kernel which allow applications to safely request services that only the kernel is allowed to perform, such as accessing hardware, managing files, creating processes, or communicating over networks. </p> <ol> <li><code>open()</code> -&gt; files, <code>malloc()</code> -&gt; memory request, <code>read()</code> / <code>write()</code>, <code>socket()</code></li> </ol> <p>To protect hardware from direct access, OS runs program in two modes: User Mode and Kernel Mode. User programs run in user mode, where the CPU restricts them from accessing hardware directly or modifying/executing anything in  kernel space. But when application needs to access such restricted functionality, they can request it through available system call which internally works as follows:</p> <pre><code>sequenceDiagram\n    participant UserApp as User Application\n    participant LibC as Libc Wrapper\n    participant CPU as CPU (User \u2192 Kernel Mode)\n    participant Kernel as Kernel System Call Handler\n\n    Note over UserApp: Program calls a system function&lt;br/&gt;e.g., open(\"data.txt\")\n\n    UserApp-&gt;&gt;LibC: Call libc wrapper (open)\n    LibC-&gt;&gt;CPU: Load syscall number &amp; args&lt;br/&gt;Execute syscall instruction\n\n    Note over CPU: CPU switches to kernel mode&lt;br/&gt;and jumps to syscall entry\n\n    CPU-&gt;&gt;Kernel: Enter kernel system call handler\n    Kernel-&gt;&gt;Kernel: Look up syscall number&lt;br/&gt;Dispatch to sys_open()\n\n    Kernel-&gt;&gt;Kernel: Perform privileged operations&lt;br/&gt;e.g., check permissions, open file, allocate FD\n\n    Kernel--&gt;&gt;CPU: Return result (e.g., file descriptor)\n    Note over CPU: CPU switches back to user mode\n\n    CPU--&gt;&gt;LibC: Return from syscall\n    LibC--&gt;&gt;UserApp: Deliver return value\n\n    Note over UserApp: Program resumes execution</code></pre>  Use mouse to pan and zoom  <p>Note that system calls are slow compared to normal function calls due to mode switch from user-kernel-user during which Kernel may flush TLB, check memory barriers, cache misses are likely, which reduces their execution. To improve  performance various optimization techniques are used, like Linux introduced vDSO (virtual system calls),  technologies like io_uring, etc.</p> <p>To expose this system calls to user programs, different OS expose different interfaces. For example,  Unix/Linux based OS provides POSIX based system calls standardized across Unix-like systems, while Windows system  call are wrapped by Win32 API. Few commonly used system calls -&gt; <code>fork()</code> to create a new process , <code>exec()</code> to load and run a new program, <code>mmap()</code> to map memory regions, <code>open()</code>/<code>read()</code> for File I/O.  </p>"},{"location":"Courses/foos/process_mgmt/#process-termination","title":"Process Termination","text":"<p>During Proces Termination, Kernel needs to clean up and reclaim all resources used by the process. The termination can be voluntary, when a process intentionally ends using syscall like <code>exit()</code>, or it could be involuntary where the process is forced to exit due to unexpected errors or external kill.  Check the following sequence diagram for  complete flow of termination</p> <pre><code>sequenceDiagram\n    participant Process as Terminating Process\n    participant Kernel as Kernel\n    participant Parent as Parent Process\n\n    Note over Process: Process finishes or is killed\n    Process-&gt;&gt;Kernel: exit(status)\n\n    Kernel-&gt;&gt;Kernel: Change state \u2192 TERMINATED\n    Kernel-&gt;&gt;Kernel: Release resources&lt;br/&gt;(files, memory, mappings)\n    Kernel-&gt;&gt;Kernel: Keep PCB + exit code&lt;br/&gt;Process becomes a ZOMBIE\n\n    Kernel--&gt;&gt;Parent: Send SIGCHLD\n\n    Note over Parent: Parent is notified that child has exited\n\n    Parent-&gt;&gt;Kernel: wait()/waitpid()\n    Kernel-&gt;&gt;Parent: Return child's exit status\n\n    Kernel-&gt;&gt;Kernel: Remove zombie entry&lt;br/&gt;Free PCB and kernel structures\n\n    Note over Kernel: Process fully removed from system</code></pre>  Use mouse to pan and zoom"},{"location":"Courses/foos/process_mgmt/#zombie-process","title":"Zombie Process","text":"<p>A zombie process is a process that has terminated, but still remains in the process table because its parent  hasn't yet collected its exit status. When a process finishes, the kernel keeps basic information like the exit code, metadata (PID, status) so that parent can retrieve the child's exit code by invoking <code>wait()/waitpid()</code> syscall. This way, every parent can learn how its child died (exit code, signals) which would've lost if kernel removed the whole process immediately.</p> <p>If the parent never calls <code>wait()</code>, zombies accumulate. Example: a buggy program creates many children but never waits for them which may cause system failure as it eventually run out of PIDs.</p>"},{"location":"Courses/foos/process_mgmt/#orphan-process","title":"Orphan Process","text":"<p>An orphan process is a process whose parent has terminated while the child is still running. The child can't just be  left parentless, so the kernel reassigns the orphan to a safe parent PID 1 (<code>init</code>/<code>systemd</code>). Both <code>init</code> and <code>systemd</code> has a special job of reaping orphaned zombie children as such orphan processes aren't harmful.</p>"},{"location":"Courses/foos/socket/","title":"Socket","text":""},{"location":"Courses/foos/socket/#socket","title":"Socket","text":"History on networking standardization <p>During early days of networking, each computer vendor used their own implementation of networking system. Computers from different companies couldn't talk to each other, which became a problem when organizations wanted to communicate across mixed systems. This required setting standards which could be implemented across computer vendors so that communication b/w machines isn't limited. TCP/IP specification was one of first major steps towards this standardization, which introduced an open-source vendor-neutral protocol for communication. To describe all the major networking functions, a theoretical model called OSI Model was introduced which is considered as a reference model when implementing  newer protocols or understanding existing ones. IETF publishes RFCs describing how these protocols work, so that anyone can read or change or implement them. Along with the protocol, networking also needed standardization of hardware. For example, IEEE 802 standards used across Wi-Fi, Ethernet, Bluetooth provide standard way to authenticate and authorize a device into a network. Sockets are standardization on software side, which provides standard interface to work with  TCP/IP without worrying about underlying implementation.  Each of these standardization across internet is handled by open source organizations. </p> <ul> <li>IEEE -&gt; physical standard</li> <li>IETF -&gt; Internet protocols through RFCs</li> <li>names/numbers -&gt; ICANN/IANA</li> <li>security standards -&gt; NIST, ISO, IETF </li> <li>app level standards -&gt; W3C (web), IETF (mail, DNS)  </li> </ul> <p>Sockets are logical construct to provide a standard, reliable way for programs to communicate with other programs, either on the same machine or different machine across a network (like the internet) by providing an interface  for program to access the OS\u2019s networking stack. </p> Briefly on computer networking <p></p> <p>To understand how computers communicate over network, we can use OSI Model which provides a simple reference model for various networking functions used in computer networking. To understand socket, we can just look at few top layers:</p> <ol> <li>Application: processes sends/receives complete app request parsed using application protocols like HTTP, FTP. </li> <li>Session: Manages connection information, for example TCP is a stateful protocol which requires processes to establish       a connection b/w hosts before sending/receiving app requests.</li> <li>Transport: implements TCP/UDP protocol which handles segmentation of data in packets/datagrams as per network        bandwidth, adds metadata like ports to identify process or headers for reliability.  </li> </ol> <p>For example, TCP protocol ( Transmission Control Protocol) is a Layer 4 Protocol which allows a process to communicate with process on a remote host using IP+port combination. TCP prevent any data lose over network, as any packet which are lost during transmission are retransmitted. Due to this, it needs to maintain a state  making it a stateful protocol. The additional metadata used by this layer can range from 20-60 bytes per segment.  Also, before sending a request it needs to initialize an connection which is done by 3-way handshake. The connection is defined using an hash of SourceIP-SourcePort and DestIP-DestPort, which is mapped to a file descriptor in memory.  When receiving an segment, if we\u2019ve this file descriptor the segment would be accept otherwise it would be dropped as we hadn\u2019t formed any connection.</p> <p>The segments are sequenced and ordered to make sense of the data because on receivers end, the IP packets may  arrive out of order. It\u2019s the job of receiver to order then using the provided sequence in their headers. Also,  every received segment is acknowledged. And if any segment is lost the receiver would request for it to be  retransmitted.</p> <p>Technically they're C structs formed by combinations of sender+receiver IPAddress and port which can be used to listen or send data over network using system calls. Sender/Receiver socket also needs to store their own IP because same  computer can have multiple IP Addresses, each from different interfaces like NICs, loopback, or virtual IP Addresses. In Linux, they're implemented as a file identifiable by a file descriptor(1), which can owned by the process and shared with child process during fork. This design allows the app to read/write from the socket like a file, but the I/O is routed to the network subsystem. </p> <ol> <li>unique, non-negative integer that OS assigns to an open resource like file, socket, or other input/output (I/O) for     a process.</li> </ol>"},{"location":"Courses/foos/socket/#socket-management","title":"Socket Management","text":"<p>With advancement in computer networking, one of the key idea ((1)) was the ability to execute commands across computer  through network . Client-Server architecture utilized this idea suggesting separation of apps into two categories, Client and Server which would divide the work into user facing and service provide. Now, the server can manage all the heavy lifting required by application while providing a secure interface for client to access applications resource. Today this architecture is used across majority of application in one place or another. We can use the operations used by application build on client-server architecture to get a better understanding of socket management done by OS.</p> <ol> <li>concept is called RPC (Remote Procedural Call)</li> </ol>"},{"location":"Courses/foos/socket/#starting-a-listening-port","title":"Starting a listening port","text":"<p>To respond to a client request, server need to actively check for incoming requests. This can be done by using a  listening socket over a specific port which can be used by client to send its request. </p> <p>Steps to create a listening socket</p> <ol> <li><code>fd = socket(...)</code> -&gt; create a socket which will be used by our process for listening. </li> <li><code>bind(fd, port, ...)</code> -&gt; bind it to a specific LocalIP+port combination, so that all incoming traffic is only      forwarded to socket having given IP+port combination.</li> <li><code>listen(fd, ...)</code> -&gt; Switch the socket to <code>LISTEN</code> state, this creates two new queues for establishing connection      from external hosts.</li> </ol> <p>After creating a listening port, NIC + routing table now consider this port valid for incoming SYN packets which makes  the server ready to receive new connections.</p>"},{"location":"Courses/foos/socket/#establishing-a-connection","title":"Establishing a Connection","text":"<p>When establishing a connection in TCP protocol, hosts have to complete a 3-way handshake.</p> <pre><code>sequenceDiagram\n    participant Client\n    participant ServerNIC as Server NIC\n    participant TL as Transport Layer (Server)\n    participant ListenSock as Listening Socket\n    participant SynQ as SYN Queue\n    participant AcceptQ as Accept Queue\n    participant ChildSock as Child Connection Socket\n\n    Note over Client,TL: Client initiates connection\n\n    Client -&gt;&gt; ServerNIC: SYN\n    ServerNIC -&gt;&gt; TL: Deliver SYN packet\n\n    TL -&gt;&gt; ListenSock: Match on listening port found\n    TL -&gt;&gt; SynQ: Insert half-open connection (SYN entry)\n\n    TL -&gt;&gt; Client: SYN-ACK\n\n    Client -&gt;&gt; ServerNIC: ACK\n    ServerNIC -&gt;&gt; TL: Deliver ACK packet\n\n    TL -&gt;&gt; SynQ: Remove SYN entry\n    TL -&gt;&gt; ChildSock: Create new child connection socket (established)\n    TL -&gt;&gt; AcceptQ: Insert child socket into Accept Queue\n\n    Note over AcceptQ,ListenSock: accept() will retrieve this socket\n\n    ListenSock -&gt;&gt; ChildSock: accept() returns the established connection\n\n    Note over ChildSock,Client: Child socket now handles all data transfer</code></pre>  Use mouse to pan and zoom  SYN Flood Attack <p>Denial-of-service (DoS) attack that overwhelms a server by sending many initial connection requests (SYN packets) without completing the TCP handshake, which ends up flooding the SYN Queue. This has been now solves with timeout,  where the SYN request is removed if ACK isn\u2019t received after certain time.</p> <ul> <li>The child socket is synonymously used with connection naming convention, as it contains all the data structures required for communicating over connection, like send/receive buffers, metadata for transport protocols, etc.</li> <li>The capacity of Accept queue is specified using <code>backlog</code> parameters passed onto <code>listen</code> syscall. If the   listening process doesn\u2019t accept connection as frequently as they\u2019re getting created, the Accept Queue would be    completely filled. As result, the Kernel wouldn\u2019t be able to add more SYNed connections leading to outage for users.</li> </ul>"},{"location":"Courses/foos/socket/#connecting-to-a-remote-host","title":"Connecting to a remote host","text":"<p>To connect to a remote host on listening port, you can create a socket object (<code>socket()</code>) and connect over respective IP and listening port of remote host (<code>connect(remoteIP, remotePort)</code>). When you call <code>connect</code>, OS will randomly  select an ephemeral port and initiate a 3-way handshake to establish a connection. After the connection is established, the process resumes execution and the socket can be used to send and receive data. </p>"},{"location":"Courses/foos/socket/#sending-request","title":"Sending Request","text":"<p>Once connection is established on both client and server, either host can send data using <code>send()/write()</code> over respective socket object.</p> <pre><code>sequenceDiagram\n    participant App as Application\n    participant SockSend as Socket Send Buffer\n    participant TL as Transport Layer (TCP)\n    participant IP as IP Layer\n    participant NIC as NIC Driver / Hardware\n\n    Note over App,NIC: Send Request Flow\n\n    App -&gt;&gt; SockSend: send()/write() \u2192 copy data to send buffer\n    SockSend -&gt;&gt; TL: Data delivered to TCP layer\n    TL -&gt;&gt; TL: Segment data, assign seq#, manage retransmissions\n    TL -&gt;&gt; IP: Hand off TCP segments\n    IP -&gt;&gt; IP: Add IP header, routing decision\n    IP -&gt;&gt; NIC: Pass packets to NIC driver\n    NIC -&gt;&gt; NIC: Frame packets, transmit over network</code></pre>  Use mouse to pan and zoom  <p>During the transfer, Kernel also keep tracks various protocol based data to manages different protocol requirements. For example, TCP requires retransmission to guarantee transfer of each packet, acknowledgements for received packets,  flow control to avoid flooding receiver and congestion control to avoid flooding network.</p>"},{"location":"Courses/foos/socket/#receiving-request","title":"Receiving Request","text":"<pre><code>sequenceDiagram\n    participant NIC as NIC Driver / Hardware\n    participant IP as IP Layer\n    participant TL as Transport Layer (TCP)\n    participant SockRecv as Socket Receive Buffer\n    participant App as Application\n\n    Note over NIC,App: Receive Request Flow\n\n    NIC -&gt;&gt; IP: Receive packet from network\n    IP -&gt;&gt; IP: Validate, strip IP header\n    IP -&gt;&gt; TL: Deliver to TCP layer\n    TL -&gt;&gt; TL: Reassemble stream, ACK handling, ordering checks\n    TL -&gt;&gt; SockRecv: Place data into receive buffer\n    SockRecv -&gt;&gt; App: Wake recv() \u2192 copy data to user space\n</code></pre>  Use mouse to pan and zoom  Zero Copy <p>When working with send/receive buffer (lives in kernel space), data movement requires copying them from userspace  space to kernel space. For operations which transfer data from a file on disk, this involves multiple round trips of copying. In such cases Kernel provides few mechanism to avoid copying:</p> <ul> <li>To transfer file from disk, you can use <code>sendfile</code> syscall which directly copies data filesystem into send buffer.</li> <li>Point the send buffer directly toward the file to read the outgoing data from there on disk\u2019s page cache (<code>MSG_ZEROCOPY</code>)</li> </ul> <p>Tradeoff: Can't apply additional processing (like compression and encryption) on outgoing data as doing it requires  loading the data into userspace memory of process.</p>"},{"location":"Courses/foos/socket/#closing-connection","title":"Closing Connection","text":"<p>Since using connection occupies resources (like memory and file descriptor values), they need to be cleaned up once the application is done using them. This is done using <code>close()</code> syscall, which can be invoked by either side which initiate a 4-way handshake in case of TCP.</p> <pre><code>sequenceDiagram\n    participant Initiator\n    participant Peer\n\n    Note over Initiator,Peer: Initiator begins connection close\n\n    Initiator -&gt;&gt; Peer: FIN\n    Note over Initiator: Enters FIN-WAIT-1\n\n    Peer -&gt;&gt; Initiator: ACK\n    Note over Initiator: Enters FIN-WAIT-2\n\n    Note over Peer: Application finishes processing and closes side\n\n    Peer -&gt;&gt; Initiator: FIN\n    Note over Peer: Enters CLOSE-WAIT\n\n    Initiator -&gt;&gt; Peer: ACK\n    Note over Initiator: Enters TIME-WAIT\n\n    Note over Peer: Connection resources freed\n\n    Note over Initiator: Waits for timeout, then fully closes</code></pre>  Use mouse to pan and zoom  Why <code>TIME-WAIT</code>? <p><code>TIME-WAIT</code> state of socket ensures that all inflight segments are received to correct socket before its termination.  If we immediately terminated the socket, and the IP+Port combination is occupied by another socket - any inflight packet would end up in wrong process breaching security of system.</p>"},{"location":"Courses/foos/socket/#socket-programming-patterns","title":"Socket Programming Patterns","text":"<p>More than frequently, the server\u2019s listening process only accepts new connections and the accepted connection is provided to a child worker process/thread for processing its requests. This architecture allows you to  optimize and scale your server by organizing request handling as per required workload. Few of the commonly used patterns are:</p> <ol> <li>Single Listener - Single Worker Thread: Standard pattern, where single thread is responsible for listening,     accepting and processing the incoming request. This could flood your Accept Queue, if your server is getting new    connections more frequently that it's accepting.</li> <li>Single Listener - Multiple Worker Threads: Main Thread is responsible for listening and accepting the request        but processing is offloaded to separate pool of thread. This way, you can keep your accept queue clear and unblocked       for new connections. </li> <li>Multiple Listener on same port : Some systems like proxy, load balancers have requirement of accepting connection       in huge number. To help with this, we can use <code>SO_REUSEPORT</code> flag when creating socket to allow multiple independent       sockets to bind to the exact same IP address and port combination. With this, each listener can have its own listening       socket and queues for accepting connection and the kernel will load balance incoming traffic over listening port b/w       available sockets.</li> </ol>"},{"location":"Courses/foos/socket/#asynchronous-io","title":"Asynchronous IO","text":"<p>Most of the socket operations like read, write, accept are blocking operations, i.e. they block execution in CPU until they receive some data in their respective buffers (receive, send, accept buffers). This leads to switch in CPU context and slows down the process. </p> <p>For example, a program uses multiple connection for reading data sequentially. During execution, if any connection which haven't received data in its buffer would block the execution, which leads to CPU context switch even if we\u2019ve multiple connection after this which could read data from their buffers. To avoid such situation, Kernel provides two approaches:</p> <ol> <li>Ready approach: The kernel provides you with a function over socket to check if it\u2019s ready for desired process.    Using it, we can program our application such that we only read/write/accept when the socket is ready essentially    avoiding blocking syscalls. This approach is implemented in Linux using <code>select</code> and <code>epoll</code> syscalls and <code>kqueue</code>    in Mac. Note that these function only work over socket file descriptors.</li> <li>Completion approach: Allow process to submit some I/O task to Kernel, which completes it on their behalf without    blocking the execution. Once the task is completed the Kernel can notify the respective process, which can then    pick up completed tasks and continue its execution. This approach is used in windows <code>IOCP</code> and <code>io_uring</code>. </li> </ol>"},{"location":"Courses/foos/socket/#select","title":"Select","text":"<p><code>select()</code> takes a collection of socket file descriptors for Kernel to monitor. When called, it\u2019ll block the execution until any of the provided socket is ready. With this, you can loop over socket with <code>FD_ISSET</code> to determine which FD is ready. But this is slow since we\u2019ve to now loop over all the sockets to determine ready socket.  Few other limitation of <code>select</code>: - internally uses lots of copying data between userspace and kernel. - supports a fixed number of sockets for monitoring. </p>"},{"location":"Courses/foos/socket/#epoll","title":"Epoll","text":"<p><code>epoll()</code> is the standard API in Linux for doing async IO tasks. With <code>epoll</code> you register a list of socket with kernel once (using <code>epoll_create</code> API). Now, the Kernel would maintain a data structure to keep upto date information about which connection is ready when receiving data over connection.  The app can now call <code>epoll_wait</code> and it\u2019d return all the events of ready connection created by Kernel to userspace  which can then be processed by the app. However, this API is only available in Linux platform. </p> <p><code>epoll</code> also allows user to select its execution b/w level triggered or edge triggered. Level triggered epoll would  send you notification continuously till they\u2019re cleared while Edge triggered would only send you notification when the state change from one value to another. One of the downside of using <code>epoll</code> is involvement of many syscalls which  are expensive for cpu.</p>"},{"location":"Courses/foos/socket/#io-uring","title":"IO Uring","text":"<p><code>io_uring</code> uses shared memory between kernel and userspace so that the Kernel can directly fetch tasks from User process and User process can directly fetch results from Kernels processing without adding the overhead of copying data from one place to another and moving program counter. But this approach makes it less secure due to sharing space with Kernel in userspace.  Its usages involve using submission queue which can be provided with blocking tasks which needs to be completed. And it provides Completion queue which provides you with the result of submitted tasks.</p> Cross Platform Async IO <p>All the above-mentioned APIs are a standard way for handling Asynchronous I/O, but they're only support in their respective platform subsystem. Like Linux uses epoll, Windows uses IOCP, MacOS uses kqueue. To unify all these,  we\u2019ve a library called <code>lib_uv</code> which abstract the details of platform and provides a uniform API to perform async IO.  </p>"},{"location":"Courses/foos/storage/","title":"Storage","text":""},{"location":"Courses/foos/storage/#storage","title":"Storage","text":"<p>Computers needed a place to save data and programs permanently, even when the power is turned off. RAM is volatile where it loses data without power and ROM is too costly to store large amount of data.  To solve this problem, storage disk were invented which evolved over time from punch cards to HDD/SSD to become smaller, faster, more reliable, and able to hold far more data. Let's briefly discuss this evolution and technology.</p>"},{"location":"Courses/foos/storage/#storage-technologies","title":"Storage Technologies","text":"Key historical technologies <p>History of storing information and persist it even in absence of any external field began from Punch Cards.  Punch cards stored data as holes punched into stiff paper cards where each hole\u2019s position represents a bit or character. A card reader shines light or uses mechanical sensors to detect the holes and the pattern of holes is  translated into digital data the computer can process. But punch cards were physically bulky, slow, and held only a tiny amount of data, which made them impractical as computers grew more capable. This was solved by invention of  Magnetic tape, which offered higher capacity and lower cost.</p> <p>Magnetic tape used a long plastic tape coated with magnetic material to stores data as patterns of magnetized regions. A read/write head moves over the tape, magnetizing areas to write bits or detecting magnetization to read bits.  However, magnetic tape allowed only sequential access, the tape had to be wound to reach a specific file which was too slow for interactive computing. To overcome this, floppy disks were developed, providing faster random access  and convenient portability.</p> <p>Floopy Disk used a thin magnetic disk sealed in a flexible plastic case where data was stored as magnetic patterns in circular tracks. A drive spins the disk while a read/write head hovers extremely close to the surface which provided random access by quickly moving to different tracks. </p> <p>Another popular key technology during the period were optical media like CD/DVD/Blu-ray disc. The discs contain microscopic pits (indentations) and lands (flat areas) which could reflect light differently. This reflection pattern was interpreted as digital bits to store information. Laser light was used to read/write data, as such shorter-wavelength lasers meant higher-capacity of discs as light could be reflected from even smaller pits. This lead to development of  CD which used infrared laser, DVD using red laser and finally Blu-ray used blue-violet laser. </p>"},{"location":"Courses/foos/storage/#hdd","title":"HDD","text":"<p>HDD (Hard Disk Drive) is one of the prominent technology used at this time for storage. </p> <p>It's an electro mechanical storage device that stores data using magnetism on rapidly spinning disks called platters.  Inside the drive, there are one or more platter which spin at high speeds (1). A head is used to read/write data from platters, which is attached to physical arm (actuator) to move it across the platter. </p> <ol> <li>typically 5,400 to 7,200 RPM</li> </ol> <p>Within the platter, data is organized using tracks, sectors and cylinders to allows fast and random access  to data. When the operating system requests data to read/write data, the arm moves the head to  correct track (seek) and waits for the correct sector to spin under the head (rotational latency). This physical movement (1) is the main cause that limits HDD performance, as the drive spends time physically repositioning parts before it can read or write data. Also, moving parts cause vibration and heat which causes wear and tear over time.</p> <ol> <li>of spinning platters and moving arm</li> </ol>"},{"location":"Courses/foos/storage/#ssd","title":"SSD","text":"<p>SDDs (Solid State Drive) overcame this mechanical limitations of HDD, by using NAND flash cell technology. Each cell can hold a charge that represents bits (0s and 1s) and you can combine multiple cells in series to store multiple bits per cell (1) to increase storage density.</p> <ol> <li>SLC (single-level) -&gt; 1 bit, MLC (multi-level) -&gt; 2 bits, TLC (triple-level) -&gt; 3 bits, and QLC (quad-level) -&gt; 4 bits.    </li> </ol> Physics of NAND flash cell <p> NAND flash cell has a control gate and a floating gate, separated by a thin oxide layer. The floating gate can trap electrical charge which is used to store information as 0s and 1s. To fill charge in cell, you've to apply a high voltage at the control gate, which tunnels the electrons through the oxide layer and traps in the floating gate.  To check charge in cell, a read voltage is applied which causes current flow through the transistor. This current is measured to determine if the floating gate is charged. To remove charge from cell, you've to apply an erase voltage to release the trapped electrons.  </p> <p>However, NAND flash cells has a physical limitation where data can only be written to an empty cell and the data in  cell must be erased before being writing to it again. But operating individual cell at large scale proved to be far more complex, slow and expensive making it physically infeasible. To overcome this limitation,  cells where grouped in pages and pages into blocks. </p> <ul> <li>Pages are the smallest writable/readable units which allowed faster random access and reduce write amplification   compared to larger units. They're designed such that writing fewer bits at a time would make the SSD slower and   more expensive.</li> <li>Blocks were design to erase data for making the cells reusable. But the physics of erasing flash cells lowered their    lifespan for storing data reliably. To reduce this wear down, erase is allowed at a large region at once named block.</li> </ul> <p>With this data organization, SSDs can be efficiently used for storage purpose. To make it compatible for working with OS, we could've implemented hardware drivers for SSD within kernel to help translate simple OS commands (1) into the complex internal operations (2) needed for NAND flash. But instead of drivers, SSDs uses a hardware component for translation,  so that SSDs can evolve internally without changing how they interact with OS. This was component is called  controller, which is a processor inside the SSD that manages all its operations. To map the logical address  provided by OS to physical location, controller uses FTL (Flash Translation Layer) which hides the data layout of  SSDs over simple LBA interface. Further, there are two different implementation of standard interface: SATA and NVMe.</p> <ol> <li>like READ, WRITE, DELETE</li> <li>like Page read/write, Block erases</li> </ol> SATA and NVMe <p>SATA (1) relied on AHCI (2)  which let SSDs plug into existing systems designed for HDDs without requiring new  software. But as SSDs became faster, SATA interface became a bottleneck. To fix it, NVMe (3) was created to run  over PCIe (4), and  the commands were designed specifically for Flash storage while keeping same integration layer for OS.</p> <ol> <li>Serial Advanced Technology Attachment</li> <li>Advanced Host Controller Interface</li> <li>Non-Volatile Memory Express</li> <li>Peripheral Component Interconnect Express, high-speed interface for transferring data between the motherboard and    expansion devices like SSDs, GPUs. The \"Express\" in the name indicates improved performance over older PCI standards. </li> </ol> <p>Further to understand internal working of SSD for different OS commands for CRUD and management operations, checkout below admonition.</p> Read <pre><code>    sequenceDiagram\n        participant OS as Operating System\n        participant Ctrl as SSD Controller (FTL)\n        participant NAND as NAND Flash\n\n        OS-&gt;&gt;Ctrl: Read(LBA)\n        Ctrl-&gt;&gt;Ctrl: Translate LBA -&gt; physical page\n        Ctrl-&gt;&gt;NAND: Read(physical page)\n        NAND--&gt;&gt;Ctrl: Return data\n        Ctrl--&gt;&gt;OS: Send data</code></pre>  Use mouse to pan and zoom  Write <pre><code>    sequenceDiagram\n       participant OS as Operating System\n       participant Ctrl as SSD Controller (FTL)\n       participant NAND as NAND Flash\n\n       OS-&gt;&gt;Ctrl: Write(LBA, Data)\n       Ctrl-&gt;&gt;Ctrl: Find free (erased) page\n       Ctrl-&gt;&gt;NAND: Program new page with Data\n       NAND--&gt;&gt;Ctrl: Write complete\n       Ctrl-&gt;&gt;Ctrl: Update LBA \u2192 new page mapping\n       Ctrl--&gt;&gt;OS: Write success</code></pre>  Use mouse to pan and zoom  Update <pre><code>    sequenceDiagram\n       participant OS as Operating System\n       participant Ctrl as SSD Controller (FTL)\n       participant NAND as NAND Flash\n\n       OS-&gt;&gt;Ctrl: Write(LBA, New Data)\n       Ctrl-&gt;&gt;Ctrl: Lookup old physical page\n       Ctrl-&gt;&gt;Ctrl: Allocate new free page\n       Ctrl-&gt;&gt;NAND: Program new page with New Data\n       NAND--&gt;&gt;Ctrl: Write complete\n       Ctrl-&gt;&gt;Ctrl: Mark old page as invalid\n       Ctrl-&gt;&gt;Ctrl: Update LBA \u2192 new page mapping\n       Ctrl--&gt;&gt;OS: Update success</code></pre>  Use mouse to pan and zoom  Delete/TRIM <pre><code>    sequenceDiagram\n        participant OS as Operating System\n        participant Ctrl as SSD Controller (FTL)\n        participant NAND as NAND Flash\n\n        OS-&gt;&gt;Ctrl: TRIM(LBA Range)\n        Ctrl-&gt;&gt;Ctrl: Lookup physical pages for LBA range\n        Ctrl-&gt;&gt;Ctrl: Mark pages as \"invalid\" (no longer needed)\n\n        Note over Ctrl: TRIM does NOT erase data immediately.&lt;br&gt;It only marks space as reusable.\n        Ctrl-&gt;&gt;Ctrl: Add invalid pages to GC candidate list to &lt;br&gt;erase in future Garbage Collection Cycle\n        Ctrl--&gt;&gt;OS: TRIM acknowledged</code></pre>  Use mouse to pan and zoom  Garbage Collection <pre><code>    sequenceDiagram\n        participant Ctrl as SSD Controller\n        participant NAND as NAND Flash\n\n        Ctrl-&gt;&gt;Ctrl: Identify block with invalid pages\n        Ctrl-&gt;&gt;NAND: Read valid pages from old block\n        NAND--&gt;&gt;Ctrl: Valid data\n        Ctrl-&gt;&gt;NAND: Write valid pages to new block\n        NAND--&gt;&gt;Ctrl: Data written\n        Ctrl-&gt;&gt;NAND: Erase old block\n        NAND--&gt;&gt;Ctrl: Block erased (free)</code></pre>  Use mouse to pan and zoom  Wear-Leveling <p>Wear leveling is an optimization technique to prevents any single block from reaching its erase-cycle limit too soon by moving hot-data blocks (data which is frequently updated) to cold-data blocks (data which is rarely updated). </p> <pre><code>    sequenceDiagram\n        participant Ctrl as SSD Controller (FTL)\n        participant NAND as NAND Flash\n        participant WL as Wear-Leveling Engine\n\n        Ctrl-&gt;&gt;WL: Request block for new write\n        WL-&gt;&gt;WL: Check erase counts of all blocks\n        WL-&gt;&gt;Ctrl: Provide least-worn block\n\n        Ctrl-&gt;&gt;NAND: Write data to selected block\n        NAND--&gt;&gt;Ctrl: Write complete\n\n        Note over WL: Background Wear-Leveling (Static)\n\n        WL-&gt;&gt;WL: Identify cold data in low-wear blocks\n        WL-&gt;&gt;NAND: Read cold data from low-wear block\n        NAND--&gt;&gt;WL: Return cold data\n\n        WL-&gt;&gt;NAND: Write cold data to higher-wear block\n        NAND--&gt;&gt;WL: Write complete\n\n        WL-&gt;&gt;NAND: Erase original low-wear block\n        NAND--&gt;&gt;WL: Block erased\n\n        WL-&gt;&gt;Ctrl: Updated block wear distribution</code></pre>  Use mouse to pan and zoom  <p></p> <p>SSDs have huge advantage in performance over HDDs mainly due to no involvement of mechanical apparatus. However, SSDs have few of its own tradeoffs:</p> <ul> <li>SSDs architecture causes more write amplification like updating data requires writing to new pages, copying valid   pages during garbage collection, and erasing blocks. So a small write can generate multiple internal writes.</li> <li>SSDs slow down when nearly full due large written blocks which causes more garbage collection leading to lower   performance.</li> <li>Wear-Leveling is needed since each block can endure only a limited number of erase cycles, so the SSD spreads   erasures evenly.</li> </ul>"},{"location":"Courses/foos/storage/#storage-management","title":"Storage Management","text":"<p>We've already discussed how storage devices persists data internally, specially for SSDs.  But programs rarely work with storage devices directly, instead OS provides different level of abstractions to simplify communicating with storage. </p>"},{"location":"Courses/foos/storage/#lba","title":"LBA","text":"<p>From physical layer, storage devices expose themselves to the OS as a linear array of fixed-size blocks known  as LBA (Logical Block Address). As far as the OS is concerned, each LBA is a fixed size (1) unit for reading and writing to storage. This design significantly simplifies storage interaction with OS while providing a simple interface to storage devices to work with.</p> <ol> <li>typically 512 bytes or 4 kB</li> </ol>"},{"location":"Courses/foos/storage/#block-layer","title":"Block Layer","text":"<p>Block layer sits b/w the filesystem (next layer) and storage device controller to performing crucial coordination  work.</p> <ul> <li>Provides a unified interface to different block-based storage devices (like HDD, SSD, NVMe, USB drives). Each one   of them can use their own protocol, but the block layer exposes a single common API for all of them allowing    Filesystems to work with all of them without knowing the difference. </li> <li>Filesystems issue requests in filesystem blocks like <code>block 1234 (ext4 block)</code> which would correspond to multiple LBAs   like <code>LBA range 50000\u201350007</code>. The block layer transforms this high-level filesystem request into a device-level    request with appropriate size.</li> <li>Request merging when filesystems issues multiple small request touching adjacent LBAs.</li> <li>Request splitting when a single large filesystem request exceeds devices maximum transfer size.</li> <li> <p>Multiple processes may request I/O at the same time. Block layer handles this concurrency by using queues and proper   scheduling which uses various algorithms (1) to ensure fairness and high throughput. </p> <ol> <li>like Elevator/SCAN algorithm for HDDs to minimize head movement or ordering and merging writes in SSDs </li> </ol> </li> </ul>"},{"location":"Courses/foos/storage/#filesystem-layer","title":"FileSystem Layer","text":"<p>Working directly with blocks and LBAs is impossible for humans. Instead, filesystems were invented which used structures similar to the one's we use in daily life to organize information (like (1)).</p> <ol> <li>files, directories, naming, permissions, timestamps and other metadata</li> </ol> <p>It's primary role is to provide various operations to manage these structures, and translate them into block operations for underlying abstraction layer. For example, <code>read(\"notes.txt\")</code> operation would cause the filesystem:</p> <ul> <li>name lookup to find the directory containing the file</li> <li>metadata lookup to find the inode</li> <li>block mapping to find the range of LBAs storing the file.</li> </ul> <p>Filesystem organizes blocks into data structures like,</p> <ul> <li>Directories -&gt; map names to files</li> <li>Inodes -&gt; Store metadata on file</li> <li>Data blocks -&gt; Store file contents </li> <li>Free space maps -&gt; to find unused blocks </li> <li>Allocation groups / extents -&gt; to efficiently handle large files.</li> </ul> Simple Visualization of filesystem organization <pre><code>    flowchart TD\n\n        subgraph DIR[Directory Structure]\n            D1[\"Entry: fileA \u2192 inode #10\"]\n            D2[\"Entry: fileB \u2192 inode #20\"]\n            D3[\"Entry: notes \u2192 inode #30\"]\n        end\n\n        DIR --&gt; I10\n        DIR --&gt; I20\n        DIR --&gt; I30\n\n        subgraph INODES[Inode Table]\n            I10[\"Inode #10&lt;br/&gt;size, owner, timestamps,&lt;br/&gt;permissions, block pointers\"]\n            I20[\"Inode #20&lt;br/&gt;metadata + block pointers\"]\n            I30[\"Inode #30&lt;br/&gt;metadata + block pointers\"]\n        end\n\n        I10 --&gt; B1\n        I10 --&gt; B2\n        I10 --&gt; B3\n\n        I20 --&gt; DB1[\"Direct Block\"]\n        I20 --&gt; IB1[\"Indirect Block\"]\n        IB1 --&gt; IB1_1[\"Data Block A\"]\n        IB1 --&gt; IB1_2[\"Data Block B\"]\n\n        I30 --&gt; C1[\"Content Block 1\"]\n        I30 --&gt; C2[\"Content Block 2\"]\n\n        subgraph DATABLOCKS[Data Blocks on Disk]\n            B1[\"Block 100\"]\n            B2[\"Block 101\"]\n            B3[\"Block 102\"]\n            DB1[\"Block 200\"]\n            IB1[\"Block 300 (Indirect Block)\"]\n            IB1_1[\"Block 301\"]\n            IB1_2[\"Block 302\"]\n            C1[\"Block 400\"]\n            C2[\"Block 401\"]\n        end</code></pre>  Use mouse to pan and zoom  <p>This allows filesystem to allocate blocks efficiently while maintaining consistency and durability. It ensures this by using  allocation algorithms like Extents which reserves contiguous ranges of blocks for a file, using B+ trees to  keep track of free space, using Journals to recover from crashes and Copy-on-write to avoid overwriting data in-place Depending on different requirement, various filesystems (like (1)) came to as computers and their application evolved over time. </p> <ol> <li>FAT32, ext4, NTFS, XFS, btrfs</li> </ol> <p>Few popular filesystems used in OS during this time, </p> <ul> <li>ext4 (Fourth Extended File System) is most commonly used filesystem for Linux distributions, built for    performance and reliability. </li> <li>NTFS (New Technology File System) is default for Windows, offering reliability (journaling), security    (permissions, encryption), compression, and supports large files/volumes.</li> <li>APFS (Apple File System) for macOS/iOS, is optimized for SSDs with features like cloning, space sharing, and    strong encryption.</li> </ul>"},{"location":"Courses/foos/storage/#virtual-filesystem-layer","title":"Virtual FileSystem Layer","text":"<p>All different filesystems behave differently internally. To hide these differences, the kernel uses the VFS layer which  provides a common API for all filesystems. This allows you to mount any filesystem underneath and work with file and  directory in a standard way.</p>"},{"location":"Courses/foos/storage/#application-layer","title":"Application Layer","text":"<p>Finally, at the top, we have the familiar interface (like (1)), which interact with the VFS, and passes requests to the filesystem, which translates them into block operations, which are then sent to the block layer,  which sends LBAs to the device.</p> <ol> <li>files, folders, paths, open, read, write, delete, copy/paste</li> </ol> Summary of Storage Layers <pre><code>    sequenceDiagram\n        autonumber\n\n        participant APP as Application\n        participant VFS as VFS Layer\n        participant FS as Filesystem (ext4/NTFS/XFS)\n        participant BLK as Block Layer&lt;br/&gt;I/O Scheduler\n        participant DRV as Device Driver&lt;br/&gt;(SATA/NVMe/SCSI)\n        participant DEV as Storage Device&lt;br/&gt;(HDD/SSD)\n\n        APP-&gt;&gt;VFS: read(\"fileA\", offset, size)\n        VFS-&gt;&gt;FS: Lookup file path&lt;br/&gt;Resolve filename \u2192 inode\n        FS-&gt;&gt;FS: Translate file offset \u2192 filesystem block\n        FS-&gt;&gt;BLK: Submit BIO request&lt;br/&gt;(read block X)\n        BLK-&gt;&gt;BLK: Merge &amp; schedule requests&lt;br/&gt;Apply I/O scheduler\n        BLK-&gt;&gt;DRV: Dispatch request&lt;br/&gt;(READ LBA range)\n        DRV-&gt;&gt;DEV: Issue protocol command&lt;br/&gt;(READ LBA N to N+K)\n        DEV--&gt;&gt;DRV: Return data\n        DRV--&gt;&gt;BLK: Complete I/O request\n        BLK--&gt;&gt;FS: Return data buffer\n        FS--&gt;&gt;VFS: File data\n        VFS--&gt;&gt;APP: read() returns with data</code></pre>  Use mouse to pan and zoom"},{"location":"DSA/","title":"DSA","text":""},{"location":"DSA/#dsa","title":"DSA","text":"Title Description References Tags Neetcode150 Review DSA for technical interviews by solving 150 DSA problems based on commonly asked patterns. notes site"},{"location":"DSA/neetcode150/","title":"Neetcode 150","text":""},{"location":"DSA/neetcode150/#neetcode-150","title":"Neetcode 150","text":""},{"location":"DSA/neetcode150/#arrays-hashing","title":"Arrays &amp; Hashing","text":"StatusProblemsDifficultyReferences Contains Duplicate notes | site Valid Anagram notes | site Two Sum notes | site Group Anagrams notes | site Top K Frequent Elements notes | site Encode and Decode Strings notes | site Product of Array Except Self notes | site Valid Sudoku notes | site Longest Consecutive Sequence notes | site"},{"location":"DSA/neetcode150/#two-pointers","title":"Two Pointers","text":"StatusProblemsDifficultyReferences Valid Palindrome notes | site Two Sum II Input Array Is Sorted notes | site 3Sum notes | site Container With Most Water notes | site Trapping Rain Water notes | site"},{"location":"DSA/neetcode150/#sliding-window","title":"Sliding Window","text":"StatusProblemsDifficultyReferences Best Time to Buy And Sell Stock notes | site Longest Substring Without Repeating Characters notes | site Longest Repeating Character Replacement notes | site Permutation In String notes | site Minimum Window Substring notes | site Sliding Window Maximum notes | site"},{"location":"DSA/neetcode150/#stack","title":"Stack","text":"StatusProblemsDifficultyReferences Valid Parentheses notes | site Min Stack notes | site Evaluate Reverse Polish Notation notes | site Daily Temperatures notes | site Car Fleet notes | site Largest Rectangle In Histogram notes | site"},{"location":"DSA/neetcode150/#binary-search","title":"Binary Search","text":"StatusProblemsDifficultyReferences Binary Search notes | site Search a 2D Matrix notes | site Koko Eating Bananas notes | site Find Minimum In Rotated Sorted Array notes | site Search In Rotated Sorted Array notes | site Time Based Key Value Store notes | site Median of Two Sorted Arrays notes | site"},{"location":"DSA/neetcode150/#linked-list","title":"Linked List","text":"StatusProblemsDifficultyReferences Reverse Linked List notes | site Merge Two Sorted Lists notes | site Linked List Cycle notes | site Reorder List notes | site Remove Nth Node From End of List notes | site Copy List With Random Pointer notes | site Add Two Numbers notes | site Find The Duplicate Number notes | site LRU Cache notes | site Merge K Sorted Lists notes | site Reverse Nodes In K Group notes | site"},{"location":"DSA/neetcode150/#trees","title":"Trees","text":"StatusProblemsDifficultyReferences Invert Binary Tree notes | site Maximum Depth of Binary Tree notes | site Diameter of Binary Tree notes | site Balanced Binary Tree notes | site Same Tree notes | site Subtree of Another Tree notes | site Lowest Common Ancestor of a Binary Search Tree notes | site Binary Tree Level Order Traversal notes | site Binary Tree Right Side View notes | site Count Good Nodes In Binary Tree notes | site Validate Binary Search Tree notes | site Kth Smallest Element In a Bst notes | site Construct Binary Tree From Preorder And Inorder Traversal notes | site Binary Tree Maximum Path Sum notes | site Serialize And Deserialize Binary Tree notes | site"},{"location":"DSA/neetcode150/#heap-priority-queue","title":"Heap / Priority Queue","text":"StatusProblemsDifficultyReferences Kth Largest Element In a Stream notes | site Last Stone Weight notes | site K Closest Points to Origin notes | site Kth Largest Element In An Array notes | site Task Scheduler notes | site Design Twitter notes | site Find Median From Data Stream notes | site"},{"location":"DSA/neetcode150/#backtracking","title":"Backtracking","text":"StatusProblemsDifficultyReferences Subsets notes | site Combination Sum notes | site Combination Sum II notes | site Permutations notes | site Subsets II notes | site Generate Parentheses notes | site Word Search notes | site Palindrome Partitioning notes | site Letter Combinations of a Phone Number notes | site N Queens notes | site"},{"location":"DSA/neetcode150/#tries","title":"Tries","text":"StatusProblemsDifficultyReferences Implement Trie Prefix Tree notes | site Design Add And Search Words Data Structure notes | site Word Search II notes | site"},{"location":"DSA/neetcode150/#graphs","title":"Graphs","text":"StatusProblemsDifficultyReferences Number of Islands notes | site Max Area of Island notes | site Clone Graph notes | site Walls And Gates notes | site Rotting Oranges notes | site Pacific Atlantic Water Flow notes | site Surrounded Regions notes | site Course Schedule notes | site Course Schedule II notes | site Graph Valid Tree notes | site Number of Connected Components In An Undirected Graph notes | site Redundant Connection notes | site Word Ladder notes | site"},{"location":"DSA/neetcode150/#advanced-graphs","title":"Advanced Graphs","text":"StatusProblemsDifficultyReferences Network Delay Time notes | site Reconstruct Itinerary notes | site Min Cost to Connect All Points notes | site Swim In Rising Water notes | site Alien Dictionary notes | site Cheapest Flights Within K Stops notes | site"},{"location":"DSA/neetcode150/#1-d-dynamic-programming","title":"1-D Dynamic Programming","text":"StatusProblemsDifficultyReferences Climbing Stairs notes | site Min Cost Climbing Stairs notes | site House Robber notes | site House Robber II notes | site Longest Palindromic Substring notes | site Palindromic Substrings notes | site Decode Ways notes | site Coin Change notes | site Maximum Product Subarray notes | site Word Break notes | site Longest Increasing Subsequence notes | site Partition Equal Subset Sum notes | site"},{"location":"DSA/neetcode150/#2-d-dynamic-programming","title":"2-D Dynamic Programming","text":"StatusProblemsDifficultyReferences Unique Paths notes | site Longest Common Subsequence notes | site Best Time to Buy And Sell Stock With Cooldown notes | site Coin Change II notes | site Target Sum notes | site Interleaving String notes | site Longest Increasing Path In a Matrix notes | site Distinct Subsequences notes | site Edit Distance notes | site Burst Balloons notes | site Regular Expression Matching notes | site"},{"location":"DSA/neetcode150/#greedy","title":"Greedy","text":"StatusProblemsDifficultyReferences Maximum Subarray notes | site Jump Game notes | site Jump Game II notes | site Gas Station notes | site Hand of Straights notes | site Merge Triplets to Form Target Triplet notes | site Partition Labels notes | site Valid Parenthesis String notes | site"},{"location":"DSA/neetcode150/#intervals","title":"Intervals","text":"StatusProblemsDifficultyReferences Insert Interval notes | site Merge Intervals notes | site Non Overlapping Intervals notes | site Meeting Rooms notes | site Meeting Rooms II notes | site Minimum Interval to Include Each Query notes | site"},{"location":"DSA/neetcode150/#math-geometry","title":"Math &amp; Geometry","text":"StatusProblemsDifficultyReferences Rotate Image notes | site Spiral Matrix notes | site Set Matrix Zeroes notes | site Happy Number notes | site Plus One notes | site Pow(x, n) notes | site Multiply Strings notes | site Detect Squares notes | site"},{"location":"DSA/neetcode150/#bit-manipulation","title":"Bit Manipulation","text":"StatusProblemsDifficultyReferences Single Number notes | site Number of 1 Bits notes | site Counting Bits notes | site Reverse Bits notes | site Missing Number notes | site Sum of Two Integers notes | site Reverse Integer notes | site"},{"location":"DSA/neetcode150/3sum/","title":"15. 3Sum","text":"","tags":["Two Pointers","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/3sum/#15-3sum","title":"15. 3Sum","text":"<p>Problem Link</p> <p>This problem is similar to Two Sum, we can just use an outer iteration to reduce it to a two sum. But core part would be, how you'd avoid duplicate triplets in result?</p> <p>One approach could be by sorting the array, and since duplicate values would be adjacent we could directly skip them during each iteration. Also, Sorting Array is \\(O(nlogn)\\) operation, which wouldn't impact the runtime of our \\(O(n^2)\\) solution.  </p> Pseudocode <ul> <li>Sort the input Array and start an outer loop as an indicative of first number in triplet.</li> <li>Skip the number if it's same as previous number, as we've already solved for it and don't want duplicate.</li> <li>Within inner loop since the array is sorted, you can use similar approach as Two Sum 2.</li> </ul> Runtime Complexity <p>Time: \\(O(n^2)\\), from two loops.</p> <p>Space: \\(O(1)\\)/\\(O(n)\\), depending on sorting algorithm</p> PythonGo <pre><code>class Solution:\n    def threeSum(self, nums: List[int]) -&gt; List[List[int]]:\n        result = []\n        nums.sort()\n        for i in range(len(nums)):\n            if i&gt;0 and nums[i] == nums[i-1]:\n                continue\n            j, k = i+1, len(nums)-1\n            while j &lt; k:\n                currSum = nums[i]+nums[j]+nums[k]\n                if currSum &gt; 0:\n                    k-=1\n                elif currSum &lt; 0:\n                    j+=1\n                else:\n                    result.append([nums[i], nums[j], nums[k]])\n                    j+=1\n                    while j &lt; k and nums[j] == nums[j-1]:\n                        j+=1\n        return result\n</code></pre> <pre><code>func threeSum(nums []int) [][]int {\n    sort.Ints(nums)\n    var result [][]int\n    for i := range nums {\n        if i &gt; 0 &amp;&amp; nums[i] == nums[i-1] {\n            continue\n        }\n        j, k := i+1, len(nums)-1\n        for j &lt; k {\n            currSum := nums[i] + nums[j] + nums[k]\n            if currSum &gt; 0 {\n                k--\n            } else if currSum &lt; 0 {\n                j++\n            } else {\n                result = append(result, []int{nums[i], nums[j], nums[k]})\n                j++\n                for j &lt; k &amp;&amp; nums[j] == nums[j-1] {\n                    j++\n                }\n            }\n        }\n    }\n    return result\n}\n</code></pre>","tags":["Two Pointers","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/add_two_numbers/","title":"Add Two Numbers","text":""},{"location":"DSA/neetcode150/add_two_numbers/#add-two-numbers","title":"Add Two Numbers","text":""},{"location":"DSA/neetcode150/alien_dictionary/","title":"Alien Dictionary","text":""},{"location":"DSA/neetcode150/alien_dictionary/#alien-dictionary","title":"Alien Dictionary","text":""},{"location":"DSA/neetcode150/balanced_binary_tree/","title":"Balanced Binary Tree","text":""},{"location":"DSA/neetcode150/balanced_binary_tree/#balanced-binary-tree","title":"Balanced Binary Tree","text":""},{"location":"DSA/neetcode150/best_time_to_buy_and_sell_stock/","title":"Best Time to Buy And Sell Stock","text":""},{"location":"DSA/neetcode150/best_time_to_buy_and_sell_stock/#best-time-to-buy-and-sell-stock","title":"Best Time to Buy And Sell Stock","text":""},{"location":"DSA/neetcode150/best_time_to_buy_and_sell_stock_with_cooldown/","title":"Best Time to Buy And Sell Stock With Cooldown","text":""},{"location":"DSA/neetcode150/best_time_to_buy_and_sell_stock_with_cooldown/#best-time-to-buy-and-sell-stock-with-cooldown","title":"Best Time to Buy And Sell Stock With Cooldown","text":""},{"location":"DSA/neetcode150/binary_search/","title":"Binary Search","text":""},{"location":"DSA/neetcode150/binary_search/#binary-search","title":"Binary Search","text":""},{"location":"DSA/neetcode150/binary_tree_level_order_traversal/","title":"Binary Tree Level Order Traversal","text":""},{"location":"DSA/neetcode150/binary_tree_level_order_traversal/#binary-tree-level-order-traversal","title":"Binary Tree Level Order Traversal","text":""},{"location":"DSA/neetcode150/binary_tree_maximum_path_sum/","title":"Binary Tree Maximum Path Sum","text":""},{"location":"DSA/neetcode150/binary_tree_maximum_path_sum/#binary-tree-maximum-path-sum","title":"Binary Tree Maximum Path Sum","text":""},{"location":"DSA/neetcode150/binary_tree_right_side_view/","title":"Binary Tree Right Side View","text":""},{"location":"DSA/neetcode150/binary_tree_right_side_view/#binary-tree-right-side-view","title":"Binary Tree Right Side View","text":""},{"location":"DSA/neetcode150/burst_balloons/","title":"Burst Balloons","text":""},{"location":"DSA/neetcode150/burst_balloons/#burst-balloons","title":"Burst Balloons","text":""},{"location":"DSA/neetcode150/car_fleet/","title":"Car Fleet","text":""},{"location":"DSA/neetcode150/car_fleet/#car-fleet","title":"Car Fleet","text":""},{"location":"DSA/neetcode150/cheapest_flights_within_k_stops/","title":"Cheapest Flights Within K Stops","text":""},{"location":"DSA/neetcode150/cheapest_flights_within_k_stops/#cheapest-flights-within-k-stops","title":"Cheapest Flights Within K Stops","text":""},{"location":"DSA/neetcode150/climbing_stairs/","title":"Climbing Stairs","text":""},{"location":"DSA/neetcode150/climbing_stairs/#climbing-stairs","title":"Climbing Stairs","text":""},{"location":"DSA/neetcode150/clone_graph/","title":"Clone Graph","text":""},{"location":"DSA/neetcode150/clone_graph/#clone-graph","title":"Clone Graph","text":""},{"location":"DSA/neetcode150/coin_change/","title":"Coin Change","text":""},{"location":"DSA/neetcode150/coin_change/#coin-change","title":"Coin Change","text":""},{"location":"DSA/neetcode150/coin_change_ii/","title":"Coin Change II","text":""},{"location":"DSA/neetcode150/coin_change_ii/#coin-change-ii","title":"Coin Change II","text":""},{"location":"DSA/neetcode150/combination_sum/","title":"Combination Sum","text":""},{"location":"DSA/neetcode150/combination_sum/#combination-sum","title":"Combination Sum","text":""},{"location":"DSA/neetcode150/combination_sum_ii/","title":"Combination Sum II","text":""},{"location":"DSA/neetcode150/combination_sum_ii/#combination-sum-ii","title":"Combination Sum II","text":""},{"location":"DSA/neetcode150/construct_binary_tree_from_preorder_and_inorder_traversal/","title":"Construct Binary Tree From Preorder And Inorder Traversal","text":""},{"location":"DSA/neetcode150/construct_binary_tree_from_preorder_and_inorder_traversal/#construct-binary-tree-from-preorder-and-inorder-traversal","title":"Construct Binary Tree From Preorder And Inorder Traversal","text":""},{"location":"DSA/neetcode150/container_with_most_water/","title":"11. Container With Most Water","text":"","tags":["Two Pointers","Greedy","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/container_with_most_water/#11-container-with-most-water","title":"11. Container With Most Water","text":"<p>Problem Link</p> <p>Area of container would be determined by two factors,  - width  - minimum height on either edges.</p> <p>We can use two pointers starting from the largest width available. To iterate over to next candidate, we can remove edges from either side. But we can greedily remove the shorter edge, as  this would be always the limiting factor in area calculation -&gt; \\((r-l)*min(height[l],height[r])\\). Also, the solution from shorter edges has been already considered in current iteration, so we don't have to worry about missing the current area in result.</p> Pseudocode <ul> <li>Iterate over the array using two pointers, left and right.</li> <li>During each iteration, calculate the area covered and update the global result if we've found larger value.</li> <li>Later we can just shift the left or right pointers to move onto next candidate to find global maxima.</li> </ul> Runtime Complexity <p>Time: \\(O(n)\\), since we're only iterating the nums array once.</p> <p>Space: \\(O(1)\\), constant space from pointer variables.</p> PythonGo <pre><code>class Solution:\n    def maxArea(self, height: List[int]) -&gt; int:\n        result = 0\n        l, r = 0, len(height)-1\n        while l &lt; r:\n            curr = (r-l)*min(height[l], height[r])\n            result = max(result, curr)\n            if height[r] &gt; height[l]:\n                l+=1\n            else:\n                r-=1\n        return result\n</code></pre> <pre><code>func maxArea(height []int) int {\n    l, r := 0, len(height)-1\n    var result int\n    for l &lt; r {\n        curr := (r - l) * min(height[l], height[r])\n        result = max(result, curr)\n        if height[r] &gt; height[l] {\n            l++\n        } else {\n            r--\n        }\n    }\n    return result\n}\n</code></pre>","tags":["Two Pointers","Greedy","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/contains_duplicate/","title":"217. Contains Duplicate","text":"","tags":["Hash Table","Array","LC_Easy","Neetcode150"]},{"location":"DSA/neetcode150/contains_duplicate/#217-contains-duplicate","title":"217. Contains Duplicate","text":"<p>Problem Link</p> <p>Brute Force: You can iterate twice over the Array to check if element from outer iteration is present using inner  iteration. This would yield following runtime complexity: Time -&gt; \\(O(n^2)\\), Space -&gt; \\(O(1)\\)</p> <p>We can optimize the operation of checking if an element is present from \\(O(n)\\) to \\(O(1)\\) using a data structure like  HashSet which gives us constant time querying operation. The tradeoff being increase in space complexity to \\(O(n)\\), which majority of time is acceptable as memory isn't as scare as old days.</p> Pseudocode <ul> <li>Iterate over the \\(nums\\) array. For each \\(num\\),</li> <li>check if the \\(num\\) is present in our hashset. <ul> <li>If yes, we can return immediately</li> <li>Else, store current \\(num\\) in our hashset and continue. </li> </ul> </li> </ul> Runtime Complexity <p>Time: \\(O(n)\\), since we're only iterating the nums array once and querying hashset, an \\(O(1)\\) operation.</p> <p>Space: \\(O(n)\\), due to the hashset used for storing num.</p> PythonGo <pre><code>class Solution:\n    def containsDuplicate(self, nums: List[int]) -&gt; bool:\n        visited = set()\n        for num in nums:\n            if num in visited:\n                return True\n            visited.add(num)\n        return False\n</code></pre> <pre><code>func containsDuplicate(nums []int) bool {\n    set := make(map[int]interface{})\n    for _, num := range nums {\n        _, ok := set[num]\n        if ok {\n            return true\n        }\n        set[num] = nil\n    }\n    return false\n}\n</code></pre>","tags":["Hash Table","Array","LC_Easy","Neetcode150"]},{"location":"DSA/neetcode150/copy_list_with_random_pointer/","title":"Copy List With Random Pointer","text":""},{"location":"DSA/neetcode150/copy_list_with_random_pointer/#copy-list-with-random-pointer","title":"Copy List With Random Pointer","text":""},{"location":"DSA/neetcode150/count_good_nodes_in_binary_tree/","title":"Count Good Nodes In Binary Tree","text":""},{"location":"DSA/neetcode150/count_good_nodes_in_binary_tree/#count-good-nodes-in-binary-tree","title":"Count Good Nodes In Binary Tree","text":""},{"location":"DSA/neetcode150/counting_bits/","title":"Counting Bits","text":""},{"location":"DSA/neetcode150/counting_bits/#counting-bits","title":"Counting Bits","text":""},{"location":"DSA/neetcode150/course_schedule/","title":"Course Schedule","text":""},{"location":"DSA/neetcode150/course_schedule/#course-schedule","title":"Course Schedule","text":""},{"location":"DSA/neetcode150/course_schedule_ii/","title":"Course Schedule II","text":""},{"location":"DSA/neetcode150/course_schedule_ii/#course-schedule-ii","title":"Course Schedule II","text":""},{"location":"DSA/neetcode150/daily_temperatures/","title":"Daily Temperatures","text":""},{"location":"DSA/neetcode150/daily_temperatures/#daily-temperatures","title":"Daily Temperatures","text":""},{"location":"DSA/neetcode150/decode_ways/","title":"Decode Ways","text":""},{"location":"DSA/neetcode150/decode_ways/#decode-ways","title":"Decode Ways","text":""},{"location":"DSA/neetcode150/design_add_and_search_words_data_structure/","title":"Design Add And Search Words Data Structure","text":""},{"location":"DSA/neetcode150/design_add_and_search_words_data_structure/#design-add-and-search-words-data-structure","title":"Design Add And Search Words Data Structure","text":""},{"location":"DSA/neetcode150/design_twitter/","title":"Design Twitter","text":""},{"location":"DSA/neetcode150/design_twitter/#design-twitter","title":"Design Twitter","text":""},{"location":"DSA/neetcode150/detect_squares/","title":"Detect Squares","text":""},{"location":"DSA/neetcode150/detect_squares/#detect-squares","title":"Detect Squares","text":""},{"location":"DSA/neetcode150/diameter_of_binary_tree/","title":"Diameter of Binary Tree","text":""},{"location":"DSA/neetcode150/diameter_of_binary_tree/#diameter-of-binary-tree","title":"Diameter of Binary Tree","text":""},{"location":"DSA/neetcode150/distinct_subsequences/","title":"Distinct Subsequences","text":""},{"location":"DSA/neetcode150/distinct_subsequences/#distinct-subsequences","title":"Distinct Subsequences","text":""},{"location":"DSA/neetcode150/edit_distance/","title":"Edit Distance","text":""},{"location":"DSA/neetcode150/edit_distance/#edit-distance","title":"Edit Distance","text":""},{"location":"DSA/neetcode150/encode_and_decode_strings/","title":"271. Encode and Decode Strings","text":"","tags":["Hash Table","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/encode_and_decode_strings/#271-encode-and-decode-strings","title":"271. Encode and Decode Strings","text":"<p>Problem Link</p> <p>One of the most common pattern for encoding any data structure into stream of continuous data type is to use the size of data structure to denote the amount of data to read from stream for parsing single unit of data. But, since our data might also include numbers there would be no way to differentiate the size data from data structure value. To solve this, you can simply use a placeholder value between two of these values. This would lead us to create streams as follows -&gt; <code>&lt;size&gt;&lt;placeholder&gt;&lt;data&gt;...</code></p> Pseudocode <p>Encode: Since our data structure is simply list of strings, we can</p> <ul> <li>Calculate the size of each string</li> <li>Generate the encoded token for this string -&gt; <code>&lt;size&gt;&lt;placeholder&gt;&lt;string&gt;</code></li> <li>Join all the token strings to get encoded data.</li> </ul> <p>Decode: You can use two pointers to indicate the start and current position in stream.</p> <ul> <li>We need to parse the stream until we reach end of it</li> <li>Start by parsing the size of next data token, by reading data until you encounter the placeholder value.</li> <li>Next decode the size into a number and read next <code>size</code> amount of data (skipping the placeholder).</li> <li>Update the start and current pointer to end of current token and continue.</li> </ul> Runtime Complexity <p>Time: \\(O(n)\\) &lt;- encode, \\(O(n)\\) &lt;- decode</p> <p>Space: \\(O(n)\\) &lt;- encode (from storing tokens/output), \\(O(n)\\) &lt;- decode </p> PythonGo <pre><code>class Solution:\n    DELIMITER = \"#\"\n    def encode(self, strs: List[str]) -&gt; str:\n        tokens = []\n        for s in strs:\n            token = f\"{len(s)}{self.DELIMITER}{s}\"\n            tokens.append(token)\n\n        return \"\".join(tokens)\n\n    def decode(self, s: str) -&gt; List[str]:\n        start = curr = 0\n        strs = []\n        while curr &lt; len(s):\n            while s[curr] != self.DELIMITER:\n                curr+=1\n\n            size = int(s[start:curr])\n            strs.append(s[curr+1: curr+size+1])\n            start = curr+size+1\n            curr = curr+size+1\n\n        return strs\n</code></pre> <pre><code>type Solution struct{}\n\nfunc (s *Solution) Encode(strs []string) string {\n    var tokens []string\n    for _, str := range strs {\n        tokens = append(tokens, fmt.Sprintf(\"%d#%s\", len(str), str))\n    }\n    return strings.Join(tokens, \"\")\n}\n\nfunc (s *Solution) Decode(str string) []string {\n    var strs []string\n    var start, curr int\n    for curr &lt; utf8.RuneCountInString(str) {\n        for str[curr] != '#' {\n            curr++\n        }\n        size, _ := strconv.Atoi(str[start:curr])\n        strs = append(strs, str[curr+1:curr+1+size])\n        start = curr + 1 + size\n        curr = start\n    }\n    return strs\n}\n</code></pre>","tags":["Hash Table","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/evaluate_reverse_polish_notation/","title":"Evaluate Reverse Polish Notation","text":""},{"location":"DSA/neetcode150/evaluate_reverse_polish_notation/#evaluate-reverse-polish-notation","title":"Evaluate Reverse Polish Notation","text":""},{"location":"DSA/neetcode150/find_median_from_data_stream/","title":"Find Median From Data Stream","text":""},{"location":"DSA/neetcode150/find_median_from_data_stream/#find-median-from-data-stream","title":"Find Median From Data Stream","text":""},{"location":"DSA/neetcode150/find_minimum_in_rotated_sorted_array/","title":"Find Minimum In Rotated Sorted Array","text":""},{"location":"DSA/neetcode150/find_minimum_in_rotated_sorted_array/#find-minimum-in-rotated-sorted-array","title":"Find Minimum In Rotated Sorted Array","text":""},{"location":"DSA/neetcode150/find_the_duplicate_number/","title":"Find The Duplicate Number","text":""},{"location":"DSA/neetcode150/find_the_duplicate_number/#find-the-duplicate-number","title":"Find The Duplicate Number","text":""},{"location":"DSA/neetcode150/gas_station/","title":"Gas Station","text":""},{"location":"DSA/neetcode150/gas_station/#gas-station","title":"Gas Station","text":""},{"location":"DSA/neetcode150/generate_parentheses/","title":"Generate Parentheses","text":""},{"location":"DSA/neetcode150/generate_parentheses/#generate-parentheses","title":"Generate Parentheses","text":""},{"location":"DSA/neetcode150/graph_valid_tree/","title":"Graph Valid Tree","text":""},{"location":"DSA/neetcode150/graph_valid_tree/#graph-valid-tree","title":"Graph Valid Tree","text":""},{"location":"DSA/neetcode150/group_anagrams/","title":"49. Group Anagrams","text":"","tags":["Hash Table","Array","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/group_anagrams/#49-group-anagrams","title":"49. Group Anagrams","text":"<p>Problem Link</p> <p>You can check this problem on how to find if two strings are anagrams.</p> <p>If we map a string into an Array of size 26, where each element represents the occurrence a character in a string. The associated character at any index in the Array is found by adding the ASCII code of 'a' to the index -&gt; \\(index+ascii(a)\\). And we're using size 26, because the string only have lowercase english characters which can be mapped to 26 positions in our array.</p> <p>You can think of the generated Array as a kind of bitmap and strings which are anagram would've same bitmap.  We can use this information to generate the group of Anagram string. </p> Pseudocode <ul> <li>Iterate over each string. During each iteration, </li> <li>Generate the bitmap of the string.</li> <li>Store the string in a Hashmap where key is bitmap and value is list of string with same bitmap.</li> <li>Finally returns the values of Hashmap, which would be grouped Anagram strings.</li> </ul> Runtime Complexity <p><code>strs</code> -&gt; length \\(n\\), with string of maximum size \\(k\\). </p> <p>Time: \\(O(nk)\\) &lt;- \\(O(n)\\) from iterating <code>strs</code>, \\(O(k)\\) for generating bitmap of each string.</p> <p>Space: \\(O(n)\\) &lt;- from Hashmap for storing the groups.</p> PythonGo <pre><code>class Solution:\n    def _get_bitmap(self, s: str) -&gt; tuple:\n        l = [0] * 26\n        for ch in s:\n            l[ord(ch) - ord('a')] += 1\n        return tuple(l)\n\n    def groupAnagrams(self, strs: List[str]) -&gt; List[List[str]]:\n        freq = {}  # bitmap -&gt; str\n\n        for s in strs:\n            bitmap = self._get_bitmap(s)\n            if bitmap in freq:\n                freq[bitmap].append(s)\n            else:\n                freq[bitmap] = [s]\n        return list(freq.values())\n</code></pre> <pre><code>func getBitmap(s string) [26]int {\n    var bitmap [26]int\n    for _, ch := range s {\n        bitmap[ch-'a']++\n    }\n    return bitmap\n}\n\nfunc groupAnagrams(strs []string) [][]string {\n    freq := make(map[[26]int][]string)\n    for _, str := range strs {\n        bitmap := getBitmap(str)\n        freq[bitmap] = append(freq[bitmap], str)\n    }\n\n    var groups [][]string\n    for _, group := range freq {\n        groups = append(groups, group)\n    }\n    return groups\n}\n</code></pre> Implementation Note <p>Python: We're using tuple as key, because they're hashable object. If you use strings as key, consider the number of digits in counts when generating the string key.</p> <p>Go: keys to map must be <code>comparable</code> types. Slices (<code>[]int</code>) aren't comparablable but Arrays (<code>[N]T</code>) are comparable if their element Type (<code>T</code>) is comparable. </p>","tags":["Hash Table","Array","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/hand_of_straights/","title":"Hand of Straights","text":""},{"location":"DSA/neetcode150/hand_of_straights/#hand-of-straights","title":"Hand of Straights","text":""},{"location":"DSA/neetcode150/happy_number/","title":"Happy Number","text":""},{"location":"DSA/neetcode150/happy_number/#happy-number","title":"Happy Number","text":""},{"location":"DSA/neetcode150/house_robber/","title":"House Robber","text":""},{"location":"DSA/neetcode150/house_robber/#house-robber","title":"House Robber","text":""},{"location":"DSA/neetcode150/house_robber_ii/","title":"House Robber II","text":""},{"location":"DSA/neetcode150/house_robber_ii/#house-robber-ii","title":"House Robber II","text":""},{"location":"DSA/neetcode150/implement_trie_prefix_tree/","title":"Implement Trie Prefix Tree","text":""},{"location":"DSA/neetcode150/implement_trie_prefix_tree/#implement-trie-prefix-tree","title":"Implement Trie Prefix Tree","text":""},{"location":"DSA/neetcode150/insert_interval/","title":"Insert Interval","text":""},{"location":"DSA/neetcode150/insert_interval/#insert-interval","title":"Insert Interval","text":""},{"location":"DSA/neetcode150/interleaving_string/","title":"Interleaving String","text":""},{"location":"DSA/neetcode150/interleaving_string/#interleaving-string","title":"Interleaving String","text":""},{"location":"DSA/neetcode150/invert_binary_tree/","title":"Invert Binary Tree","text":""},{"location":"DSA/neetcode150/invert_binary_tree/#invert-binary-tree","title":"Invert Binary Tree","text":""},{"location":"DSA/neetcode150/jump_game/","title":"Jump Game","text":""},{"location":"DSA/neetcode150/jump_game/#jump-game","title":"Jump Game","text":""},{"location":"DSA/neetcode150/jump_game_ii/","title":"Jump Game II","text":""},{"location":"DSA/neetcode150/jump_game_ii/#jump-game-ii","title":"Jump Game II","text":""},{"location":"DSA/neetcode150/k_closest_points_to_origin/","title":"K Closest Points to Origin","text":""},{"location":"DSA/neetcode150/k_closest_points_to_origin/#k-closest-points-to-origin","title":"K Closest Points to Origin","text":""},{"location":"DSA/neetcode150/koko_eating_bananas/","title":"Koko Eating Bananas","text":""},{"location":"DSA/neetcode150/koko_eating_bananas/#koko-eating-bananas","title":"Koko Eating Bananas","text":""},{"location":"DSA/neetcode150/kth_largest_element_in_a_stream/","title":"Kth Largest Element In a Stream","text":""},{"location":"DSA/neetcode150/kth_largest_element_in_a_stream/#kth-largest-element-in-a-stream","title":"Kth Largest Element In a Stream","text":""},{"location":"DSA/neetcode150/kth_largest_element_in_an_array/","title":"Kth Largest Element In An Array","text":""},{"location":"DSA/neetcode150/kth_largest_element_in_an_array/#kth-largest-element-in-an-array","title":"Kth Largest Element In An Array","text":""},{"location":"DSA/neetcode150/kth_smallest_element_in_a_bst/","title":"Kth Smallest Element In a Bst","text":""},{"location":"DSA/neetcode150/kth_smallest_element_in_a_bst/#kth-smallest-element-in-a-bst","title":"Kth Smallest Element In a Bst","text":""},{"location":"DSA/neetcode150/largest_rectangle_in_histogram/","title":"Largest Rectangle In Histogram","text":""},{"location":"DSA/neetcode150/largest_rectangle_in_histogram/#largest-rectangle-in-histogram","title":"Largest Rectangle In Histogram","text":""},{"location":"DSA/neetcode150/last_stone_weight/","title":"Last Stone Weight","text":""},{"location":"DSA/neetcode150/last_stone_weight/#last-stone-weight","title":"Last Stone Weight","text":""},{"location":"DSA/neetcode150/letter_combinations_of_a_phone_number/","title":"Letter Combinations of a Phone Number","text":""},{"location":"DSA/neetcode150/letter_combinations_of_a_phone_number/#letter-combinations-of-a-phone-number","title":"Letter Combinations of a Phone Number","text":""},{"location":"DSA/neetcode150/linked_list_cycle/","title":"Linked List Cycle","text":""},{"location":"DSA/neetcode150/linked_list_cycle/#linked-list-cycle","title":"Linked List Cycle","text":""},{"location":"DSA/neetcode150/longest_common_subsequence/","title":"Longest Common Subsequence","text":""},{"location":"DSA/neetcode150/longest_common_subsequence/#longest-common-subsequence","title":"Longest Common Subsequence","text":""},{"location":"DSA/neetcode150/longest_consecutive_sequence/","title":"128. Longest Consecutive Sequence","text":"","tags":["Array","Hash Table","Union Find","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/longest_consecutive_sequence/#128-longest-consecutive-sequence","title":"128. Longest Consecutive Sequence","text":"<p>Problem Link</p> <p>Brute Force: Since we only need to find the longest consecutive sequence irrespective of order in array, we can check the sequence starting each element and take the longest. This would result in \\(O(n^2)\\) time  complexity.</p> <p>To optimize this, we can just check the sequence for \\(num_i\\) whose previous number (\\(num_i-1\\)) isn't present in our  array, as these number are guaranteed to generate unique sequence . This would result in checking the sequence for elements only once. </p> Pseudocode <ul> <li>Iterate over the nums array. To keep checks constant time, we can also create a Hashset out of <code>nums</code> array.</li> <li>For each <code>num</code>, if the previous (<code>num-1</code>) isn't present in our array means we've a unique sequence starting    from this <code>num</code>.</li> <li>To generate this sequence, declare a length pointer and increment it until we exhaust the sequence numbers   present in our hashset.</li> <li>Finally use a global variable to maintain the maximum length.</li> </ul> Runtime Complexity <p>Time: \\(O(n)\\), single iteration over each element with constant time checks. It might look like we're iterating within 2 loops, but the conditional will reduce the inner iteration such that we're not repeating checks.</p> <p>Space: \\(O(n)\\) &lt;- from hashmap.</p> PythonGo <pre><code>class Solution:\n    def longestConsecutive(self, nums: List[int]) -&gt; int:\n        res = 0\n        nums = set(nums)\n        for num in nums:\n            if num-1 not in nums:\n                length = 1\n                while num+length in nums:\n                    length+=1\n                res = max(length, res)\n        return res\n</code></pre> <pre><code>func longestConsecutive(nums []int) int {\n    set := make(map[int]interface{})\n    for _, num := range nums {\n        set[num] = nil\n    }\n    res := 0\n    for num, _ := range set {\n        if _, ok := set[num-1]; !ok {\n            length := 1\n            for {\n                if _, ok := set[num+length]; ok {\n                    length++\n                } else {\n                    break\n                }\n            }\n            res = max(res, length)\n        }\n    }\n    return res\n}\n</code></pre>","tags":["Array","Hash Table","Union Find","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/longest_increasing_path_in_a_matrix/","title":"Longest Increasing Path In a Matrix","text":""},{"location":"DSA/neetcode150/longest_increasing_path_in_a_matrix/#longest-increasing-path-in-a-matrix","title":"Longest Increasing Path In a Matrix","text":""},{"location":"DSA/neetcode150/longest_increasing_subsequence/","title":"Longest Increasing Subsequence","text":""},{"location":"DSA/neetcode150/longest_increasing_subsequence/#longest-increasing-subsequence","title":"Longest Increasing Subsequence","text":""},{"location":"DSA/neetcode150/longest_palindromic_substring/","title":"Longest Palindromic Substring","text":""},{"location":"DSA/neetcode150/longest_palindromic_substring/#longest-palindromic-substring","title":"Longest Palindromic Substring","text":""},{"location":"DSA/neetcode150/longest_repeating_character_replacement/","title":"Longest Repeating Character Replacement","text":""},{"location":"DSA/neetcode150/longest_repeating_character_replacement/#longest-repeating-character-replacement","title":"Longest Repeating Character Replacement","text":""},{"location":"DSA/neetcode150/longest_substring_without_repeating_characters/","title":"Longest Substring Without Repeating Characters","text":""},{"location":"DSA/neetcode150/longest_substring_without_repeating_characters/#longest-substring-without-repeating-characters","title":"Longest Substring Without Repeating Characters","text":""},{"location":"DSA/neetcode150/lowest_common_ancestor_of_a_binary_search_tree/","title":"Lowest Common Ancestor of a Binary Search Tree","text":""},{"location":"DSA/neetcode150/lowest_common_ancestor_of_a_binary_search_tree/#lowest-common-ancestor-of-a-binary-search-tree","title":"Lowest Common Ancestor of a Binary Search Tree","text":""},{"location":"DSA/neetcode150/lru_cache/","title":"LRU Cache","text":""},{"location":"DSA/neetcode150/lru_cache/#lru-cache","title":"LRU Cache","text":""},{"location":"DSA/neetcode150/max_area_of_island/","title":"Max Area of Island","text":""},{"location":"DSA/neetcode150/max_area_of_island/#max-area-of-island","title":"Max Area of Island","text":""},{"location":"DSA/neetcode150/maximum_depth_of_binary_tree/","title":"Maximum Depth of Binary Tree","text":""},{"location":"DSA/neetcode150/maximum_depth_of_binary_tree/#maximum-depth-of-binary-tree","title":"Maximum Depth of Binary Tree","text":""},{"location":"DSA/neetcode150/maximum_product_subarray/","title":"Maximum Product Subarray","text":""},{"location":"DSA/neetcode150/maximum_product_subarray/#maximum-product-subarray","title":"Maximum Product Subarray","text":""},{"location":"DSA/neetcode150/maximum_subarray/","title":"Maximum Subarray","text":""},{"location":"DSA/neetcode150/maximum_subarray/#maximum-subarray","title":"Maximum Subarray","text":""},{"location":"DSA/neetcode150/median_of_two_sorted_arrays/","title":"Median of Two Sorted Arrays","text":""},{"location":"DSA/neetcode150/median_of_two_sorted_arrays/#median-of-two-sorted-arrays","title":"Median of Two Sorted Arrays","text":""},{"location":"DSA/neetcode150/meeting_rooms/","title":"Meeting Rooms","text":""},{"location":"DSA/neetcode150/meeting_rooms/#meeting-rooms","title":"Meeting Rooms","text":""},{"location":"DSA/neetcode150/meeting_rooms_ii/","title":"Meeting Rooms II","text":""},{"location":"DSA/neetcode150/meeting_rooms_ii/#meeting-rooms-ii","title":"Meeting Rooms II","text":""},{"location":"DSA/neetcode150/merge_intervals/","title":"Merge Intervals","text":""},{"location":"DSA/neetcode150/merge_intervals/#merge-intervals","title":"Merge Intervals","text":""},{"location":"DSA/neetcode150/merge_k_sorted_lists/","title":"Merge K Sorted Lists","text":""},{"location":"DSA/neetcode150/merge_k_sorted_lists/#merge-k-sorted-lists","title":"Merge K Sorted Lists","text":""},{"location":"DSA/neetcode150/merge_triplets_to_form_target_triplet/","title":"Merge Triplets to Form Target Triplet","text":""},{"location":"DSA/neetcode150/merge_triplets_to_form_target_triplet/#merge-triplets-to-form-target-triplet","title":"Merge Triplets to Form Target Triplet","text":""},{"location":"DSA/neetcode150/merge_two_sorted_lists/","title":"Merge Two Sorted Lists","text":""},{"location":"DSA/neetcode150/merge_two_sorted_lists/#merge-two-sorted-lists","title":"Merge Two Sorted Lists","text":""},{"location":"DSA/neetcode150/min_cost_climbing_stairs/","title":"Min Cost Climbing Stairs","text":""},{"location":"DSA/neetcode150/min_cost_climbing_stairs/#min-cost-climbing-stairs","title":"Min Cost Climbing Stairs","text":""},{"location":"DSA/neetcode150/min_cost_to_connect_all_points/","title":"Min Cost to Connect All Points","text":""},{"location":"DSA/neetcode150/min_cost_to_connect_all_points/#min-cost-to-connect-all-points","title":"Min Cost to Connect All Points","text":""},{"location":"DSA/neetcode150/min_stack/","title":"Min Stack","text":""},{"location":"DSA/neetcode150/min_stack/#min-stack","title":"Min Stack","text":""},{"location":"DSA/neetcode150/minimum_interval_to_include_each_query/","title":"Minimum Interval to Include Each Query","text":""},{"location":"DSA/neetcode150/minimum_interval_to_include_each_query/#minimum-interval-to-include-each-query","title":"Minimum Interval to Include Each Query","text":""},{"location":"DSA/neetcode150/minimum_window_substring/","title":"Minimum Window Substring","text":""},{"location":"DSA/neetcode150/minimum_window_substring/#minimum-window-substring","title":"Minimum Window Substring","text":""},{"location":"DSA/neetcode150/missing_number/","title":"Missing Number","text":""},{"location":"DSA/neetcode150/missing_number/#missing-number","title":"Missing Number","text":""},{"location":"DSA/neetcode150/multiply_strings/","title":"Multiply Strings","text":""},{"location":"DSA/neetcode150/multiply_strings/#multiply-strings","title":"Multiply Strings","text":""},{"location":"DSA/neetcode150/n_queens/","title":"N Queens","text":""},{"location":"DSA/neetcode150/n_queens/#n-queens","title":"N Queens","text":""},{"location":"DSA/neetcode150/network_delay_time/","title":"Network Delay Time","text":""},{"location":"DSA/neetcode150/network_delay_time/#network-delay-time","title":"Network Delay Time","text":""},{"location":"DSA/neetcode150/non_overlapping_intervals/","title":"Non Overlapping Intervals","text":""},{"location":"DSA/neetcode150/non_overlapping_intervals/#non-overlapping-intervals","title":"Non Overlapping Intervals","text":""},{"location":"DSA/neetcode150/number_of_1_bits/","title":"Number of 1 Bits","text":""},{"location":"DSA/neetcode150/number_of_1_bits/#number-of-1-bits","title":"Number of 1 Bits","text":""},{"location":"DSA/neetcode150/number_of_connected_components_in_an_undirected_graph/","title":"Number of Connected Components In An Undirected Graph","text":""},{"location":"DSA/neetcode150/number_of_connected_components_in_an_undirected_graph/#number-of-connected-components-in-an-undirected-graph","title":"Number of Connected Components In An Undirected Graph","text":""},{"location":"DSA/neetcode150/number_of_islands/","title":"Number of Islands","text":""},{"location":"DSA/neetcode150/number_of_islands/#number-of-islands","title":"Number of Islands","text":""},{"location":"DSA/neetcode150/pacific_atlantic_water_flow/","title":"Pacific Atlantic Water Flow","text":""},{"location":"DSA/neetcode150/pacific_atlantic_water_flow/#pacific-atlantic-water-flow","title":"Pacific Atlantic Water Flow","text":""},{"location":"DSA/neetcode150/palindrome_partitioning/","title":"Palindrome Partitioning","text":""},{"location":"DSA/neetcode150/palindrome_partitioning/#palindrome-partitioning","title":"Palindrome Partitioning","text":""},{"location":"DSA/neetcode150/palindromic_substrings/","title":"Palindromic Substrings","text":""},{"location":"DSA/neetcode150/palindromic_substrings/#palindromic-substrings","title":"Palindromic Substrings","text":""},{"location":"DSA/neetcode150/partition_equal_subset_sum/","title":"Partition Equal Subset Sum","text":""},{"location":"DSA/neetcode150/partition_equal_subset_sum/#partition-equal-subset-sum","title":"Partition Equal Subset Sum","text":""},{"location":"DSA/neetcode150/partition_labels/","title":"Partition Labels","text":""},{"location":"DSA/neetcode150/partition_labels/#partition-labels","title":"Partition Labels","text":""},{"location":"DSA/neetcode150/permutation_in_string/","title":"Permutation In String","text":""},{"location":"DSA/neetcode150/permutation_in_string/#permutation-in-string","title":"Permutation In String","text":""},{"location":"DSA/neetcode150/plus_one/","title":"Plus One","text":""},{"location":"DSA/neetcode150/plus_one/#plus-one","title":"Plus One","text":""},{"location":"DSA/neetcode150/powx_n/","title":"Pow(x, n)","text":""},{"location":"DSA/neetcode150/powx_n/#powx-n","title":"Pow(x, n)","text":""},{"location":"DSA/neetcode150/product_of_array_except_self/","title":"238. Product of Array Except Self","text":"","tags":["Array","Prefix Sum","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/product_of_array_except_self/#238-product-of-array-except-self","title":"238. Product of Array Except Self","text":"<p>Problem Link</p> <p>The catch of problem is we can't use division operator. If that wasn't the case, we could simply have aggregated multiplication of whole array and divide each element by it get their own product without self.</p> <p>When translated to equations, this would look like: \\(\\prod_{i=1}^{n} nums_i \\over nums_i\\). By removing the division  -&gt; \\(\\prod_{i=1}^{i-1} nums_i * \\prod_{i=i+1}^{n} nums_i\\), which is basically product of prefix and postfix array to index <code>i</code>.</p> <p>One way to implement this is to store prefix and postfix multiplication in two separate arrays and combine them to generate the result. </p> <p>Follow Up</p> <p>Instead of saving the prefix and postfix multiplication within separate array, we can simply store them within output array (since multiplication is communicative) </p> Pseudocode <ul> <li>To generate prefix, iterate from start of <code>nums</code> and using a variable to aggregate the multiplication over   the iteration.</li> <li>To generate postfix, iterate from end of <code>nums</code> and repeat same.</li> </ul> Runtime Complexity <p>Time: \\(O(n)\\)</p> <p>Space: \\(O(1)\\) &lt;- output isn't considered</p> PythonGo <pre><code>class Solution:\n    def productExceptSelf(self, nums: List[int]) -&gt; List[int]:\n        result = []\n        pre, post = 1, 1\n\n        for i in range(len(nums)):\n            result.append(pre)\n            pre *= nums[i]\n\n        for i in reversed(range(len(nums))):\n            result[i] *= post\n            post *= nums[i]\n        print(result)\n        return result\n</code></pre> <pre><code>func productExceptSelf(nums []int) []int {\n    result := make([]int, len(nums))\n    var pre, post int = 1, 1\n    for idx := range nums {\n        result[idx] = pre\n        pre *= nums[idx]\n    }\n\n    for idx := len(nums) - 1; idx &gt;= 0; idx-- {\n        result[idx] *= post\n        post *= nums[idx]\n    }\n    return result\n}\n</code></pre>","tags":["Array","Prefix Sum","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/reconstruct_itinerary/","title":"Reconstruct Itinerary","text":""},{"location":"DSA/neetcode150/reconstruct_itinerary/#reconstruct-itinerary","title":"Reconstruct Itinerary","text":""},{"location":"DSA/neetcode150/redundant_connection/","title":"Redundant Connection","text":""},{"location":"DSA/neetcode150/redundant_connection/#redundant-connection","title":"Redundant Connection","text":""},{"location":"DSA/neetcode150/regular_expression_matching/","title":"Regular Expression Matching","text":""},{"location":"DSA/neetcode150/regular_expression_matching/#regular-expression-matching","title":"Regular Expression Matching","text":""},{"location":"DSA/neetcode150/remove_nth_node_from_end_of_list/","title":"Remove Nth Node From End of List","text":""},{"location":"DSA/neetcode150/remove_nth_node_from_end_of_list/#remove-nth-node-from-end-of-list","title":"Remove Nth Node From End of List","text":""},{"location":"DSA/neetcode150/reorder_list/","title":"Reorder List","text":""},{"location":"DSA/neetcode150/reorder_list/#reorder-list","title":"Reorder List","text":""},{"location":"DSA/neetcode150/reverse_bits/","title":"Reverse Bits","text":""},{"location":"DSA/neetcode150/reverse_bits/#reverse-bits","title":"Reverse Bits","text":""},{"location":"DSA/neetcode150/reverse_integer/","title":"Reverse Integer","text":""},{"location":"DSA/neetcode150/reverse_integer/#reverse-integer","title":"Reverse Integer","text":""},{"location":"DSA/neetcode150/reverse_linked_list/","title":"Reverse Linked List","text":""},{"location":"DSA/neetcode150/reverse_linked_list/#reverse-linked-list","title":"Reverse Linked List","text":""},{"location":"DSA/neetcode150/reverse_nodes_in_k_group/","title":"Reverse Nodes In K Group","text":""},{"location":"DSA/neetcode150/reverse_nodes_in_k_group/#reverse-nodes-in-k-group","title":"Reverse Nodes In K Group","text":""},{"location":"DSA/neetcode150/rotate_image/","title":"Rotate Image","text":""},{"location":"DSA/neetcode150/rotate_image/#rotate-image","title":"Rotate Image","text":""},{"location":"DSA/neetcode150/rotting_oranges/","title":"Rotting Oranges","text":""},{"location":"DSA/neetcode150/rotting_oranges/#rotting-oranges","title":"Rotting Oranges","text":""},{"location":"DSA/neetcode150/same_tree/","title":"Same Tree","text":""},{"location":"DSA/neetcode150/same_tree/#same-tree","title":"Same Tree","text":""},{"location":"DSA/neetcode150/search_a_2d_matrix/","title":"Search a 2D Matrix","text":""},{"location":"DSA/neetcode150/search_a_2d_matrix/#search-a-2d-matrix","title":"Search a 2D Matrix","text":""},{"location":"DSA/neetcode150/search_in_rotated_sorted_array/","title":"Search In Rotated Sorted Array","text":""},{"location":"DSA/neetcode150/search_in_rotated_sorted_array/#search-in-rotated-sorted-array","title":"Search In Rotated Sorted Array","text":""},{"location":"DSA/neetcode150/serialize_and_deserialize_binary_tree/","title":"Serialize And Deserialize Binary Tree","text":""},{"location":"DSA/neetcode150/serialize_and_deserialize_binary_tree/#serialize-and-deserialize-binary-tree","title":"Serialize And Deserialize Binary Tree","text":""},{"location":"DSA/neetcode150/set_matrix_zeroes/","title":"Set Matrix Zeroes","text":""},{"location":"DSA/neetcode150/set_matrix_zeroes/#set-matrix-zeroes","title":"Set Matrix Zeroes","text":""},{"location":"DSA/neetcode150/single_number/","title":"Single Number","text":""},{"location":"DSA/neetcode150/single_number/#single-number","title":"Single Number","text":""},{"location":"DSA/neetcode150/sliding_window_maximum/","title":"Sliding Window Maximum","text":""},{"location":"DSA/neetcode150/sliding_window_maximum/#sliding-window-maximum","title":"Sliding Window Maximum","text":""},{"location":"DSA/neetcode150/spiral_matrix/","title":"Spiral Matrix","text":""},{"location":"DSA/neetcode150/spiral_matrix/#spiral-matrix","title":"Spiral Matrix","text":""},{"location":"DSA/neetcode150/subsets_ii/","title":"Subsets II","text":""},{"location":"DSA/neetcode150/subsets_ii/#subsets-ii","title":"Subsets II","text":""},{"location":"DSA/neetcode150/subtree_of_another_tree/","title":"Subtree of Another Tree","text":""},{"location":"DSA/neetcode150/subtree_of_another_tree/#subtree-of-another-tree","title":"Subtree of Another Tree","text":""},{"location":"DSA/neetcode150/sum_of_two_integers/","title":"Sum of Two Integers","text":""},{"location":"DSA/neetcode150/sum_of_two_integers/#sum-of-two-integers","title":"Sum of Two Integers","text":""},{"location":"DSA/neetcode150/surrounded_regions/","title":"Surrounded Regions","text":""},{"location":"DSA/neetcode150/surrounded_regions/#surrounded-regions","title":"Surrounded Regions","text":""},{"location":"DSA/neetcode150/swim_in_rising_water/","title":"Swim In Rising Water","text":""},{"location":"DSA/neetcode150/swim_in_rising_water/#swim-in-rising-water","title":"Swim In Rising Water","text":""},{"location":"DSA/neetcode150/target_sum/","title":"Target Sum","text":""},{"location":"DSA/neetcode150/target_sum/#target-sum","title":"Target Sum","text":""},{"location":"DSA/neetcode150/task_scheduler/","title":"Task Scheduler","text":""},{"location":"DSA/neetcode150/task_scheduler/#task-scheduler","title":"Task Scheduler","text":""},{"location":"DSA/neetcode150/time_based_key_value_store/","title":"Time Based Key Value Store","text":""},{"location":"DSA/neetcode150/time_based_key_value_store/#time-based-key-value-store","title":"Time Based Key Value Store","text":""},{"location":"DSA/neetcode150/top_k_frequent_element/","title":"347. Top K Frequent Elements","text":"","tags":["Hash Table","Array","Heap/Priority Queue","LC_Medium","Neetcode150","Bucket Sort"]},{"location":"DSA/neetcode150/top_k_frequent_element/#347-top-k-frequent-elements","title":"347. Top K Frequent Elements","text":"<p>Problem Link</p> <p>Heap</p> <p>This is a general problem designed to be solved efficiently using heap data structure. For now, we'll solve it using hashmaps.</p> <p>The general idea to solve this is by generating a hashmap where the value is list of numbers and key is the frequency of those number in <code>nums</code>. Now, you can use this hashmap to generate list of nums sorted in order of their frequency in <code>nums</code>. But for this problem, we only want first \\(k\\) elements from this list.</p> Pseudocode <ul> <li>Generate hashmap where key is \\(num\\) and value is frequency of that \\(num\\) in <code>nums</code>.</li> <li>Using above hashmap, generate another hashmap where key is a number and value is the list   of numbers having \\(key\\) frequency in <code>nums</code>.</li> <li>Generate result having numbers sorted in order of highest frequency.    Since the highest frequency could be <code>len(nums)</code> while smallest -&gt; \\(0\\).<ul> <li>Iterate from the highest frequency to the smallest while adding numbers of   frequencies found in our hashmap. </li> </ul> </li> </ul> Runtime Complexity <p>Time: \\(O(n)\\), from iterating <code>nums</code>.</p> <p>Space: \\(O(n)\\) &lt;- from hashmaps.</p> PythonGo <pre><code>class Solution:\n    def topKFrequent(self, nums: List[int], k: int) -&gt; List[int]:\n        freq = Counter(nums)\n        buckets = defaultdict(list)\n        for num, count in freq.items():\n            buckets[count].append(num)\n\n        result = []\n        for count in range(len(nums), 0, -1):\n            if count in buckets:\n                result.extend(buckets[count])\n\n        return result[:k]\n</code></pre> <pre><code>func topKFrequent(nums []int, k int) []int {\n    freq := make(map[int]int)\n    for _, num := range nums {\n        freq[num]++\n    }\n    buckets := make(map[int][]int)\n    for num, count := range freq {\n        buckets[count] = append(buckets[count], num)\n    }\n\n    var result []int\n    for i := len(nums); i &gt;= 0; i-- {\n        vals, ok := buckets[i]\n        if ok {\n            result = append(result, vals...)\n        }\n    }\n    return result[:k]\n}\n</code></pre>","tags":["Hash Table","Array","Heap/Priority Queue","LC_Medium","Neetcode150","Bucket Sort"]},{"location":"DSA/neetcode150/trapping_rain_water/","title":"42. Trapping Rain Water","text":"","tags":["Two Pointers","LC_Hard","Neetcode150"]},{"location":"DSA/neetcode150/trapping_rain_water/#42-trapping-rain-water","title":"42. Trapping Rain Water","text":"<p>Problem Link</p> <p>Think of the water amount as sum of water collected at each index. How would you calculate the trappable water at any index? This water level is bounded by the minimum of maximum heights on either side of given index.</p> <p></p> <p>With this, you can create such prefix arrays for maximum left and right heights of respective index and finally calculate the water trapped at each level. This would give us \\(O(n)\\) time and space complexity.</p> <p>We can optimize this to use \\(O(1)\\) space by using a two pointers technique. For this, we'll need to initialize two pointers at left and right ends, and two variables storing the maximum left and right heights of among iterated values. For example, with <code>l</code> and <code>r</code> index pointers,  lets say maxL &lt; maxR. For this, we can say for sure that the water collected for l cell is bounded by maxL. Because any consecutive height on right of <code>l</code> would be equal or greater than maxR which wouldn't impact our computation of water -&gt; \\(min(maxL, maxR)-height[i]\\).</p> Pseudocode <ul> <li>Declare two pointers for indicating current left and right position in iteration.</li> <li>Declare two variables to store the maximum left and right heights.</li> <li>Iterate over the array until we've computed water level for all cells i.e \\(l&lt;=r\\).</li> <li>For each iteration,<ul> <li>if \\(maxL &lt; maxR\\), we'll compute water level for <code>l</code> and increment the counter to next index</li> <li>else, we'll computer water level for <code>r</code> and decrement the counter to next index.</li> </ul> </li> </ul> Runtime Complexity <p>Time: \\(O(n)\\), since we're only iterating the nums array once.</p> <p>Space: \\(O(1)\\), constant time from two pointers.</p> PythonGo <pre><code>class Solution:\n    def trap(self, height: List[int]) -&gt; int:\n        maxL, maxR = height[0], height[-1]\n        l, r = 0, len(height)-1\n        result = 0\n        while l &lt;= r:\n            if height[l] &lt; height[r]:\n                maxL = max(maxL, height[l])\n                result += maxL-height[l]\n                l+=1\n            else:\n                maxR = max(maxR, height[r])\n                result += maxR - height[r]\n                r-=1\n        return result\n</code></pre> <pre><code>func trap(height []int) int {\n    l, r := 0, len(height)-1\n    maxL, maxR := height[l], height[r]\n    var result int\n    for l &lt;= r {\n        if maxL &lt; maxR {\n            maxL = max(maxL, height[l])\n            result += maxL - height[l]\n            l++\n        } else {\n            maxR = max(maxR, height[r])\n            result += maxR - height[r]\n            r--\n        }\n    }\n    return result\n}\n</code></pre>","tags":["Two Pointers","LC_Hard","Neetcode150"]},{"location":"DSA/neetcode150/two_sum/","title":"1. Two Sum","text":"","tags":["Hash Table","Array","LC_Easy","Neetcode150"]},{"location":"DSA/neetcode150/two_sum/#1-two-sum","title":"1. Two Sum","text":"<p>Problem Link</p> <p>You can think of the problem as,  given a number \\(num\\) find the index of \\(target-num\\).</p> <p>This can be done by storing the index of each \\(num\\) in a data structure like Hashmap which can be queried to fetch index of a value in a constant time.</p> Pseudocode <ul> <li>Iterate over the \\(nums\\) array. For each \\(num\\),</li> <li>check if the \\(target-num\\) is present in our map.<ul> <li>If yes, we can directly return the current index and saved index from here</li> <li>Else, store the index of current num in our map and continue. </li> </ul> </li> </ul> Runtime Complexity <p>Time: \\(O(n)\\), since we're only iterating the nums array once and querying hashmap is an \\(O(1)\\) operation.</p> <p>Space: \\(O(n)\\), due to the map used for storing num -&gt; index mapping</p> PythonGo <pre><code>class Solution:\n    def twoSum(self, nums: List[int], target: int) -&gt; List[int]:\n        s = dict()\n        for i, num in enumerate(nums):\n            if num in s:\n                return [i, s[num]]\n            s[target - num] = i\n        return [-1, -1]\n</code></pre> <pre><code>func twoSum(nums []int, target int) []int {\n    idxMap := make(map[int]int)\n    for idx, num := range nums {\n        if val, ok := idxMap[num]; ok {\n            return []int{val, idx}\n        }\n        idxMap[target-num] = idx\n    }\n    return []int{}\n}\n</code></pre>","tags":["Hash Table","Array","LC_Easy","Neetcode150"]},{"location":"DSA/neetcode150/two_sum_2/","title":"167. Two Sum II - Input Array Is Sorted","text":"","tags":["Two Pointers","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/two_sum_2/#167-two-sum-ii-input-array-is-sorted","title":"167. Two Sum II - Input Array Is Sorted","text":"<p>Problem Link</p> <p>Similar to problem TwoSum, except the <code>nums</code> array is sorted now.  We can greedily use this information by comparing sum of left and right end. If our sum exceed target, we should reduce it by decreasing the right pointer. Else if our sum is smaller than target, we can increase our current sum by increasing left pointer.</p> Runtime Complexity <p>Time: \\(O(n)\\), since we're only iterating the nums array once.</p> <p>Space: \\(O(1)\\), constant space from two pointers.</p> PythonGo <pre><code>class Solution:\n    def twoSum(self, numbers: List[int], target: int) -&gt; List[int]:\n        l, r = 0, len(numbers)-1\n        while l &lt; r:\n            curr = numbers[l] + numbers[r]\n            if curr &gt; target:\n                r-=1\n            elif curr &lt; target:\n                l+=1\n            else:\n                return [l+1, r+1]\n        return [-1,-1]\n</code></pre> <pre><code>func twoSum2(numbers []int, target int) []int {\n    l, r := 0, len(numbers)-1\n    for l &lt; r {\n        curr := numbers[l] + numbers[r]\n        if curr &gt; target {\n            r--\n        } else if curr &lt; target {\n            l++\n        } else {\n            return []int{l + 1, r + 1}\n        }\n    }\n    return nil\n}\n</code></pre>","tags":["Two Pointers","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/unique_paths/","title":"Unique Paths","text":""},{"location":"DSA/neetcode150/unique_paths/#unique-paths","title":"Unique Paths","text":""},{"location":"DSA/neetcode150/valid_anagram/","title":"242. Valid Anagram","text":"","tags":["Hash Table","LC_Easy","Neetcode150"]},{"location":"DSA/neetcode150/valid_anagram/#242-valid-anagram","title":"242. Valid Anagram","text":"<p>Problem Link</p> <p>Given strings <code>s</code> and <code>t</code>, they're Anagram if we can rearrange the characters of one to form another. This can be reworded as, both strings should have same characters. We can check this by storing the frequency of one string and match them with the other.  </p> Pseudocode <ul> <li>If strings have different number of characters, we can return early as they'll never have same characters.</li> <li>Otherwise, we'll iterate over one string and store the frequency of each character in a hashmap.</li> <li>Iterate over other string to reduce the frequency of encountered character.</li> <li>During which, we can say the strings aren\u2019t anagrams if we encounter any character which either isn't present              in our hashmap, or the frequency of character is exhausted.</li> </ul> Runtime Complexity <p>Time: \\(O(n)\\), since we're only iterating the strings twice, with insertion and querying hashmap being \\(O(1)\\)             operation.</p> <p>Space: \\(O(n)\\), due to the hashmap used for storing characters frequency.</p> PythonGo <pre><code>class Solution:\n    def isAnagram(self, s: str, t: str) -&gt; bool:\n        if len(s) != len(t):\n            return False\n\n        freq = dict()\n        for i in s:\n            freq[i] = freq.get(i, 0) + 1\n\n        for i in t:\n            if i not in freq or freq[i] == 0:\n                return False\n\n            freq[i] -= 1\n\n        return True\n</code></pre> <pre><code>func isAnagram(s string, t string) bool {\n    if len(s) != len(t) {\n        return false\n    }\n    freq := make(map[rune]int)\n    for _, ch := range s {\n        freq[ch]++\n    }\n\n    for _, ch := range t {\n        val, ok := freq[ch]\n        if !ok || val == 0 {\n            return false\n        }\n        freq[ch]--\n    }\n    return true\n}\n</code></pre>","tags":["Hash Table","LC_Easy","Neetcode150"]},{"location":"DSA/neetcode150/valid_palindrome/","title":"125. Valid Palindrome","text":"","tags":["Two Pointers","LC_Easy","Neetcode150"]},{"location":"DSA/neetcode150/valid_palindrome/#125-valid-palindrome","title":"125. Valid Palindrome","text":"<p>Problem Link</p> <p>Check Palindrome by comparing starting and ending characters in string. You can easily do this by  taking two pointers, pointing the respective position from start and end which should be same to form palindrome. </p> Pseudocode <ul> <li>Initialize start and end pointers</li> <li>Iterate over the string using the pointers until they meet or cross each other over bound.</li> </ul> Runtime Complexity <p>Time: \\(O(n)\\), single iteration over string.</p> <p>Space: \\(O(1)\\), constant space by two pointers.</p> PythonGo <pre><code>class Solution:\n    def isPalindrome(self, s: str) -&gt; bool:\n        l, r = 0, len(s)-1\n        while l &lt; r:\n            if not s[l].isalnum():\n                l+=1\n            elif not s[r].isalnum():\n                r-=1\n            elif s[l].lower() != s[r].lower():\n                return False\n            else:\n                l+=1\n                r-=1\n        return True\n</code></pre> <pre><code>func isPalindrome(s string) bool {\n    l, r := 0, len(s)-1\n    for l &lt; r {\n        for l &lt; r &amp;&amp; !unicode.IsDigit(rune(s[l])) &amp;&amp; !unicode.IsLetter(rune(s[l])) {\n            l++\n        }\n        for l &lt; r &amp;&amp; !unicode.IsDigit(rune(s[r])) &amp;&amp; !unicode.IsLetter(rune(s[r])) {\n            r--\n        }\n        if unicode.ToLower(rune(s[l])) != unicode.ToLower(rune(s[r])) {\n            return false\n        }\n        l++\n        r--\n    }\n    return true\n}\n</code></pre> Go Implementation note <p>Strings in Go are sequence of bytes , so when you access any particular index - it'll return the respective byte. While characters are represented using <code>rune</code> which represents a Unicode code point. Byte occupies 1 byte size while Rune can use upto 4 bytes size, which is why converting a byte using <code>rune(.)</code> isn\u2019t always safe. It's safe for ASCII characters which fall within 1 byte range.</p>","tags":["Two Pointers","LC_Easy","Neetcode150"]},{"location":"DSA/neetcode150/valid_parentheses/","title":"Valid Parentheses","text":""},{"location":"DSA/neetcode150/valid_parentheses/#valid-parentheses","title":"Valid Parentheses","text":""},{"location":"DSA/neetcode150/valid_parenthesis_string/","title":"Valid Parenthesis String","text":""},{"location":"DSA/neetcode150/valid_parenthesis_string/#valid-parenthesis-string","title":"Valid Parenthesis String","text":""},{"location":"DSA/neetcode150/valid_sudoku/","title":"36. Valid Sudoku","text":"","tags":["Array","Hash Table","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/valid_sudoku/#36-valid-sudoku","title":"36. Valid Sudoku","text":"<p>Problem Link</p> <p>To check if a sudo board is valid, you've to satisfy 3 types of criteria: - each row shouldn't have repeating digit. - each column shouldn't have repeating digit. - each box (3x3 here) shouldn't have a repeating digit.</p> <p>This can be checked pretty easily using hash maps (similar to contains_dupliacte).</p> How would you map cells within a box to same key? <p>\\(key=(floor(row/3))*3 + (floor(col/3))\\)</p> Pseudocode <ul> <li>Iterate over the board using two loops. Within each iteration, if the cell isn't <code>.</code></li> <li>check if the value is present in your hashmap buckets.<ul> <li>if present, we can say the sudoku is invalid and return.</li> <li>else add the values to respective hashmap key.</li> </ul> </li> </ul> Runtime Complexity <p>number of elements in board -&gt; n</p> <p>Time: \\(O(n)\\), single iteration over each element with constant time checks.</p> <p>Space: \\(O(n)\\) &lt;- from hashmaps.</p> PythonGo <pre><code>class Solution:\n    def isValidSudoku(self, board: List[List[str]]) -&gt; bool:\n        rows = defaultdict(set)\n        cols = defaultdict(set)\n        box = defaultdict(set)\n        for r in range(len(board)):\n            for c in range(len(board[0])):\n                cell = board[r][c]\n                b = (r // 3) * 3 + c // 3\n                if cell == \".\":\n                    continue\n                elif cell in rows[r] or cell in cols[c] or cell in box[b]:\n                    return False\n\n                rows[r].add(cell)\n                cols[c].add(cell)\n                box[b].add(cell)\n        return True\n</code></pre> <pre><code>func isValidSudoku(board [][]byte) bool {\n    var rows, cols, boxes [9][9]bool\n\n    for r := range board {\n        for c := range board[0] {\n            cell := board[r][c]\n            if cell != '.' {\n                b := (r/3)*3 + (c / 3)\n                idx := int(cell - '1')\n                if rows[r][idx] || cols[c][idx] || boxes[b][idx] {\n                    return false\n                }\n                rows[r][idx] = true\n                cols[c][idx] = true\n                boxes[b][idx] = true\n            }\n        }\n    }\n    return true\n}\n</code></pre>","tags":["Array","Hash Table","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/validate_binary_search_tree/","title":"Validate Binary Search Tree","text":""},{"location":"DSA/neetcode150/validate_binary_search_tree/#validate-binary-search-tree","title":"Validate Binary Search Tree","text":""},{"location":"DSA/neetcode150/walls_and_gates/","title":"Walls And Gates","text":""},{"location":"DSA/neetcode150/walls_and_gates/#walls-and-gates","title":"Walls And Gates","text":""},{"location":"DSA/neetcode150/word_break/","title":"Word Break","text":""},{"location":"DSA/neetcode150/word_break/#word-break","title":"Word Break","text":""},{"location":"DSA/neetcode150/word_ladder/","title":"Word Ladder","text":""},{"location":"DSA/neetcode150/word_ladder/#word-ladder","title":"Word Ladder","text":""},{"location":"DSA/neetcode150/word_search/","title":"Word Search","text":""},{"location":"DSA/neetcode150/word_search/#word-search","title":"Word Search","text":""},{"location":"DSA/neetcode150/word_search_ii/","title":"Word Search II","text":""},{"location":"DSA/neetcode150/word_search_ii/#word-search-ii","title":"Word Search II","text":""}]}