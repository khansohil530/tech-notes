{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>Welcome to my personal knowledge base \u2014 a curated collection of notes, explanations, and resources I\u2019ve written while learning and revising various technical topics.</p> <p>This site is built with MkDocs Material and serves as both a reference and a learning log \u2014 a place where I organize thoughts, code snippets, and insights gathered over time.</p> <p>To run this site locally: </p><pre><code>pip install -r requirements.txt\nmkdocs serve\n</code></pre><p></p>"},{"location":"Courses/","title":"Courses","text":""},{"location":"Courses/#courses","title":"Courses","text":"<p>Notes from course work done online</p> <ul> <li> Fundamentals of Database Engineering: You'll learn about ACID properties, Database Indexes,   Database Engines, Database Internals and more. </li> </ul>"},{"location":"Courses/fode/","title":"Fundamentals of Database Engineering","text":"","tags":["Database"]},{"location":"Courses/fode/#fundamentals-of-database-engineering","title":"Fundamentals of Database Engineering","text":"<p>Course Link</p> <p>Notes:</p> <ul> <li> ACID</li> <li> Understanding Database Internals</li> <li> Database Indexing</li> <li> B-Trees vs B+-Trees</li> <li> Database Partitioning</li> <li> Database Sharding</li> <li> Concurrency Control</li> <li> Replication</li> <li> Database Engines</li> <li> Database Cursor</li> <li> NoSQL Architecture</li> <li> Database Security</li> <li> Extra</li> </ul>","tags":["Database"]},{"location":"Courses/fode/acid/","title":"ACID","text":""},{"location":"Courses/fode/acid/#acid","title":"ACID","text":"<p>ACID, which stands for Atomicity, Consistency, Isolation and Durability - are fundamental properties desirable across all database systems.</p> <p>To understand these properties individually, you should know about Transactions.</p> Transactions briefly <p>A transaction is a collection of DML(1) queries treated as one unit of work at application logic. </p> <p>For Example, money transfer between accounts requires multiple operations (check balance, debit one account, credit another) to succeed or fail together. However, this is performed using a bunch DML queries and failure in any single of them could result in bugs like money being debited even if there's no sufficient balance, money being debited without crediting, etc. That's is why all these operations should be wrapped in an Transaction which ensures they either run all or none.</p> <p>Transaction lifecycle involves keywords to start (<code>BEGIN</code>) , save changes (<code>COMMIT</code>) and discard changes (<code>ROLLBACK</code>). Each of these commands are implemented differently across different DBMS, like COMMIT either flushes changes made in memory to disk in one go, or it saves individual changes separately. This is due to the tradeoffs involved with such decision, which makes each DB unique and optimized for their  own specific use case and no general DB system can handle it all. </p> <p>Often transactions are mostly used in writing data, but you can also have read-only transactions. For example, transactions for generating consistent reports by providing a time-based snapshot of data.</p> <ol> <li>Data Manipulation Language</li> </ol>"},{"location":"Courses/fode/acid/#atomicity","title":"Atomicity","text":"<p>Every transaction should be treated as indivisible unit of work (either all queries within it succeed or fail, no partials). </p> <p>This helps DB to remain in a consistent state by guaranteeing that all failed transactions are rolled back which helps  to prevent data corruption and maintain consistency. It also simplifies error handling where developers don't need to  handle rollbacks.</p> <p>There are different ways to implement atomicity, few of which are:</p> <ul> <li>Logging: before writing changes to disk, they're written to undo/redo logs and only applied when commit is successful.</li> <li>Shadow Copy: changes are applied to a copy of original page. When the transaction is successful,    the pointers to data are updated to apply the changes.</li> <li>Two-Phase Commits: used in distributed systems, ensuring all peers commit or abort the transaction together.</li> </ul>"},{"location":"Courses/fode/acid/#isolation","title":"Isolation","text":"<p>Isolation property in transaction helps prevent concurrent operations from interfering with each other, ensuring each transaction appears to run on its own. It's managed through different isolation levels, which control how transactions interact with the concurrency anomalies encountered. </p> <p>Concurrency anomalies (or Read Phenomenas) are undesirable side effects of running multiple transactions at same  time. Few of these includes:</p> <ul> <li>Dirty Reads: when a transaction reads uncommitted changes made by another concurrent transaction,    which is rolled back. From DB point of view this change was never present in data as it was never committed    essentially making our read dirty.  </li> <li>Non-Repeatable Reads: when you read same entry more than once in a transaction, and it yields different values.    For example, you read a value directly in first query and then collect sum in second query.    If the value is changed when collecting sum, this will result in inconsistent sum w.r.t data in first query.   That\u2019s why it's called non-repeatable as in you can't read repeated value in same transaction. </li> <li> <p>Phantom Reads: when re-reading a range of rows, a new row appears due to write by other transaction.    The reason it's different from Non-Repeatable Reads is due to the way Repeatable Read (1) isolation level is implemented.</p> <ol> <li>Most DBs implement Repeatable Reads by keeping a version of rows being used in the transaction. This approach     doesn't help with Phantom Reads, as you can't version non-existent rows.</li> </ol> </li> <li> <p>Lost Updates: when two or more concurrent transactions read the same data, both make a modification based    on that data, and the second transaction's update overwrites the first one, effectively erasing its changes.    This leads to inconsistency as work updated by one transaction is lost due to overwrite from others.</p> </li> </ul> <p>To prevent these anomalies, Isolation property provide different levels of control. Below are few commonly implemented Isolation levels listed from lowest to highest Isolation.</p> <ol> <li>Read committed: transactions will only see committed changes. This solves dirty read as you\u2019re sure the changes    read are committed.</li> <li>Repeatable Read: with this isolation level you can repeat reads consistently within your transaction, solving     non-repeatable and dirty read anomalies.</li> <li>Serializable: concurrent transactions are executed as if they're being run one after another, essentially solving    all concurrency anomalies.</li> <li>Snapshot: allows transaction to read from a consistent snapshot of database without blocking writers.</li> </ol> Table Anomalies/Isolation Isolation Level Dirty Reads Non-repeatable Reads Phantom Reads Lost Updates Read Committed Repeatable Read Snapshot Serializable <p>There\u2019s mainly two different approaches for implementing isolation:</p> <ul> <li>Pessimistic approaches by using locks. These maybe on either row, table or page level. </li> <li>Optimistic approaches keep track of transactions and fails one of them when they overstep each others isolation levels.    This approaches reduces the significant lock management overhead on DB, but requires additional handling for retries.  </li> </ul> <p>NOTE</p> <p>Postgres implements Repeatable Read as a Snapshot isolation level and as such you don\u2019t get Phantom reads there but this might not be true for other DB system which implements Repeatable Read by maintaining version of rows.</p> <p>The choice of isolation level balances data consistency with performance, as higher isolation provides more consistency but can decrease performance.</p>"},{"location":"Courses/fode/acid/#consistency","title":"Consistency","text":"<p>Consistency ensures that a database remains in a valid state both before and after a transaction by guaranteeing  adherence to all predefined rules, constraints, and triggers.</p> <p>When defining Consistency across DB system, it can mean two different things:</p> <ol> <li>Consistency in Data - consistent data w.r.t to the defined data model.     For example, having integrity across defined constraints (like primary key, foreign keys, data type),     cleaning orphaned references as per defined rules and constraints.</li> <li> <p>Consistency in Read/Write - consistently reading data across different instances of DB.</p> Read More here <p>Read consistency ensures a transaction sees the most recent committed changes immediately.  This consistency challenge is introduced due to Replication, specifically when data written to primary isn't  synced yet to replicas. This is usually done to optimize for performance. For example, Eventual consistency within a system provide higher performance but the application may  temporarily show stale data before eventually reflecting correct values. While Synchronous replication  offers stronger consistency at the cost of slower performance compared to asynchronous approach.</p> </li> </ol> <p>Consistency acts as a safeguard, ensuring that data integrity is maintained and preventing the database from entering  an invalid or corrupted state due to incomplete or erroneous transactions.</p>"},{"location":"Courses/fode/acid/#durability","title":"Durability","text":"<p>Durability ensures changes from a committed transaction are permanently stored on non-volatile storage (e.g., SSD, HDD) \u2014 even if the system losses its power or crashes.</p> <p>DB systems play around with this concept to optimize their performance since writing to disk is slower, and instead you can write to memory first and then flush the changes to disk in bulk. But this may compromise the durability under uncertain conditions, so in addition DBs writes these changes in a compact format to a log file (<code>WAL</code> (1) ) on disk so that even if we  lose the data in memory - the record can be replayed to recover the lost data. This is better because the changes  written are compact and appended at the end of file.</p> <ol> <li>Write Ahead Log</li> </ol> <p>NOTE</p> <p>The standard <code>write()</code> operation in OS caches writes in file system for better performance. If the system crashes during this time, the data in the cache is lost. Instead, DBs use the <code>fsync</code> operation to immediately write to disk, ensuring durability but at a performance cost.</p> <p>For mission-critical systems, strong durability is non-negotiable; for less critical data, eventual durability may be acceptable.</p>"},{"location":"Courses/fode/concurrency_control/","title":"Concurrency Control","text":""},{"location":"Courses/fode/concurrency_control/#concurrency-control","title":"Concurrency Control","text":"<p>Consider the given scenario, you've a ticket booking platform which allows multiple users to book ticket at same time. In one particular case, two users decides to book tickets for same seat at same time. Ideally the querying logic for this would be to</p> <ul> <li>Check the availability of seat in DB -&gt; <code>SELECT is_booked FROM seats WHERE id ='xyz'</code></li> <li>If available book the seat and update booked to <code>true</code> -&gt; <code>UPDATE seats SET is_booked=1 WHERE id = 'xyz'</code></li> </ul> <p>This logic in our case would book the tickets and send the confirmation to both users which shouldn't be happening. Such scenarios exist all over applications which need to check the inventory and reserve the item for end users. You could prevent this mishap if you'd control over who could access the selected seat at start of any transaction. Such features are provided under Concurrency control category and locking is one of the most commonly used implementations. </p>"},{"location":"Courses/fode/concurrency_control/#locks","title":"Locks","text":"<p>Locks are logical constructs which could be placed over a resource to prevents access of certain operations over it from other parties. Based on access pattern, locks can be divided into 2 major category:</p> <ul> <li>Exclusive Locks: Can be acquired by 1 transaction exclusively allowing it to read-write over the data while    preventing read-writes from others. These locks are primarily used to allow a transaction modify data without external   conflict.  </li> <li>Shared Locks: Can be shared between multiple transaction for reading but prevents writes from any. This allows   multiple transaction to read same data concurrently while maintaining the integrity of data. </li> </ul> <p>Usage of locks should be managed with care as it could easily lead to issues like Dead Lock where 2 transactions are waiting indefinitely to acquire lock obtained by each other. Since neither of transaction will release the  lock before committing the changes, they\u2019ll both keep waiting forever. Such scenarios must be handled by the database and depending on implementation rollback and fail one of the transactions. One way to prevent Dead locks is by using Two Phase Locking.</p> <p>Two Phase Locking ensures transaction are ordered during execution by controlling how they acquire and release locks. It works in two phases</p> <ol> <li>Growing phase where locks are acquired.</li> <li>Shrinking phase where locks are released.</li> </ol> <p>For example, in Double Booking Problem discussed at the start, if we\u2019d acquired the exclusive lock during the reading phase we could avoid the double booking situation as other transactions can\u2019t read this value currently. Only after the first transaction commits does this lock gets released upon which it can see that the seat  is already booked, so it\u2019ll return immediately.</p>"},{"location":"Courses/fode/concurrency_control/#locking-in-postgres","title":"Locking in Postgres","text":"<p>While the concept of locks on top remains as discussed above, their implementation could vary over different DB systems, to provide more granular control. For example, we'll discuss the kinds of locks provide by Postgres. For more information, can view the original postgres docs Reference: Postgres Docs</p> <p>Postgres categorizes its locks into 5 categories</p>"},{"location":"Courses/fode/concurrency_control/#tabel-level-locks","title":"Tabel Level Locks","text":"<p>Postgres provides 8 different type of table locks and transactions can have multiple locks on same table. Some of these lock can conflict, others don\u2019t.</p> <ul> <li><code>ACCESS EXCLUSIVE</code>: conflicts will all other table locks and as such completely locks the table for other transactions.</li> <li><code>ACCESS SHARE</code>: Generally acquired by queries which only reads from table like <code>SELECT</code>.</li> <li><code>EXCLUSIVE</code>: similar to <code>ACCESS EXCLUSIVE</code> except it doesn\u2019t conflict <code>ACCESS SHARE</code> locks for reading.   It\u2019s only used by <code>REFRESH MATERIALIZED VIEW CONCURRENTLY</code> command, which seems it was added so that users   can refresh their materialized view while also reading from it.</li> <li><code>ROW SHARE</code>: designed for <code>SELECT FOR...</code> commands like <code>SELECT FOR UPDATE</code>, <code>SELECT FOR SHARE</code> which works on   row level. These commands obtain two kinds of locks \u2014 a Row lock and ROW SHARE table lock.</li> <li><code>ROW EXCLUSIVE</code>: this lock mainly impacts write latency in system since it's used by write commands like <code>UPDATE</code>,   <code>DELETE</code>, <code>INSERT</code>, <code>MERGE</code>, <code>COPY FROM</code>.</li> <li><code>SHARE ROW EXCLUSIVE</code>: prevents table against concurrent data changes, and is self-exclusive so that only one session    can hold it at a time. Acquired by some <code>ALTER TABLE</code> command and <code>CREATE TRIGGER</code>.</li> <li><code>SHARE</code>: protects a table against concurrent data changes but isn't self exclusive. Used by <code>CREATE INDEX</code> so that    you can create multiple indexes concurrently but the data isn\u2019t allowed to change.</li> <li><code>SHARE UPDATE EXCLUSIVE</code>: allows concurrent writes and reads but prevents schema changes and VACCUM runs.</li> </ul> <p>Below table will summarize which locks would conflict with one another.</p> <p></p>"},{"location":"Courses/fode/concurrency_control/#row-level-locks","title":"Row Level Locks","text":"<p>Row locks are critical to prevent lost updates. New tuples don\u2019t require locks as they\u2019re only visible to current transaction in which they were created. That\u2019s why postgres doesn\u2019t support READ UNCOMMITTED isolation.  These locks are limited to <code>DELETE</code> , <code>UPDATE (no key)</code> , <code>UPDATE (key)</code>, and all <code>SELECT FOR</code>s. (key/no key refers if the column has unique index on that column or not) </p> <ul> <li><code>FOR UPDATE</code>: highest row lock, you can\u2019t delete, update on the row when this lock is acquired by other transaction.    It\u2019s self conflicting, so you can\u2019t use two <code>FOR UPDATE</code> concurrently. You can still read it through normal <code>SELECT</code>.   It\u2019s obtained by <code>DELETE</code>, <code>UPDATE (key)</code>, <code>SELECT</code> commands.</li> <li><code>FOR NO KEY UPDATE</code>: acquired for updates to column without a unique index. It\u2019s weaker <code>FOR UPDATE</code> as it allows   <code>SELECT FOR KEY SHARE</code>.</li> <li><code>FOR SHARE</code>: true shared lock, it can be acquired by multiple transactions. When acquired, it blocks modification to row</li> <li><code>FOR KEY SHARE</code>: like <code>FOR SHARE</code> but allows update to column without unique index.</li> </ul> <p>To view conflict among above locks, refer to below table </p> <p>Postgres stores table locks in memory because they\u2019re coarse but row locks are stored alongside table in <code>xmax</code> system field, which saves memory but costs disk write. </p>"},{"location":"Courses/fode/concurrency_control/#page-level-locks","title":"Page Level Locks","text":"<p>Postgres page are of size 8KB and stores tuples for table and indexes. Since these pages loaded in shared buffer pool and Postgres being process based backend, multiple process  can access these pages in shared buffer memory which could lead to inconsistent read or conflicting writes. To avoid such cases, postgres provides page-level share/exclusive locks to control read/write access to table pages  in the shared buffer pool.</p>"},{"location":"Courses/fode/concurrency_control/#dead-locks","title":"Dead Locks","text":"<p>Using explicit locking can increase the likelihood of deadlocks among transactions in DB. Postgres detects such conditions and kills one of the transaction to avoid blocking forever. So long as no deadlock situation is detected, a transaction seeking either a table-level or row-level lock will wait indefinitely for conflicting locks to be released. This means it\u2019s a bad idea for applications to hold transactions open for long periods of time</p>"},{"location":"Courses/fode/concurrency_control/#advisory-locks","title":"Advisory Locks","text":"<p>Sometimes application requirements aren\u2019t satisfied by postgres built in locks.  To help with this, postgres provides application based locks which are managed by application. However, these locks still live in databases. These are of two types: - session lock: obtained with <code>pg_advisory_lock()</code> , are kept for the length of session - transaction lock: obtained with <code>pg_advisory_xact_lock()</code>, are kept for length of current running transaction</p>"},{"location":"Courses/fode/concurrency_control/#optimistic-concurrency-control","title":"Optimistic Concurrency Control","text":"<p>Locking as discussed above is categorized under Pessimistic Concurrency Control, as in you don\u2019t trust others (Pessimistic) while updating a row, that you lock it and when anyone comes to mess with it while you\u2019re updating  the row, you can tell them whatever you want to.</p> <p>Optimistic Concurrency Control in contrast allow transactions to freely (Optimistic) operate during transaction but validates conflicts during COMMIT. If there's any conflict found, it'll abort the transaction and return an error to user. This kind of concurrency control is mostly used in read heavy workload where you get conflicts rarely as data isn't modified as frequently.</p>"},{"location":"Courses/fode/concurrency_control/#multi-version-concurrency-control-mvcc","title":"Multi-Version Concurrency Control (MVCC)","text":"<p>Instead of locking rows for reads, the DB keeps multiple versions of rows. Readers see a consistent snapshot without blocking writers. Postgres uses MVCC which you can read more about in their  docs</p>"},{"location":"Courses/fode/db_cursor/","title":"Database Cursor","text":""},{"location":"Courses/fode/db_cursor/#database-cursor","title":"Database Cursor","text":"<p>Suppose your SQL query is fetching lots of rows, and it takes some time before getting back the result.  This is because the query needs to do plan query execution, gather all records, then move this data over to TCP and transmit it over network and finally collect all the data on client side. All these steps become increasingly slower as we fetch more and more rows. Also, sometimes few of the clients won't have the required memory to store all the results from such huge queries.</p> <p>This scenario can be avoided using server side Cursor, which allow you to encapsulate a query into a token and fetch few entries at a time from the query result using that token. This allows you to have immediate result for few queried entries allowing the UI to partially load data which wouldn't affect the user workflow. You can save  memory usage on client side by processing few rows at a time. You can also cancel the query midway, if you're satisfied with your results. And all these can be streamed through websocket allowing smooth workflow on client. But this shifts the load of managing state of cursor to backend/server, which may causes resource starvation if not managed properly. Also, long-running cursors involving transactions would continue to block writes causing bad write performance.</p> <p>The above-mentioned scenarios are applicable for Server Side Cursors, where the server is responsible for managing the state of cursor. You can also use Client-Side Cursors, which provided limited sets of queried entries by sorting and filtering data in batches (using keywords like <code>offset</code>, <code>limit</code>, <code>sort by</code> in SQL).</p>"},{"location":"Courses/fode/db_cursor/#cursors-in-sql-server","title":"Cursors in SQL Server","text":"<p>To understand more about how cursors are implemented in real production DBs, let's look at the implementation of cursors in Microsoft SQL Server. Most of the information is derived from their docs referenced  here.</p> <p>They define cursors as an extension over results sets to provide processing like positioning the cursor to  a specific row in result, retrieve/modify one or many rows from current position, allowing different visibility to changes made by others and finally allowing access to stored procedures and triggers. SQL Server supports four cursor types.</p> <ol> <li>Forward-Only: is a forward-only and read-only cursor which doesn't support scrolling. They only allow you to     read data serially from start to end. Rows are retrieved from DB as they're fetched, which allows it to read     modification made by other users even after its declaration. This simplicity allows it to fetch result quickly    while keeping low memory footprint.</li> <li>Static: Builds the complete result set in a temporary DB (<code>tempdb</code>) when this cursor is opened allowing it    to hide modification made by other users after cursor declaration.  Due to the static copy, you can scroll through    result set quickly, but storing the entire result set uses more memory.</li> <li>Keyset: controlled by a set of unique identifiers, or keys, known as the <code>keyset</code>, stores the key and respective    row in temporary DB (<code>tempdb</code>). When the result is fetches, DB will fetch the related row specified by key from table    as such it only detects modification like updates and deletes but not inserts since the key for new row wouldn't    be present in <code>tempdb</code>. Since only keys are stored in <code>tempdb</code>, it uses lesser memory than <code>STATIC</code> cursor  but    fetching respective rows requires additional lookup to underlying table making scrolling slower.</li> <li>Dynamic: reflects all committed changes made to result set when scrolling over the cursor. This allows your    cursor to view the most recent changes, beneficial displaying real-time updates. But since the ordering    of data isn't constant you can use positioning reliably. </li> </ol>"},{"location":"Courses/fode/db_engines/","title":"Database Engines","text":""},{"location":"Courses/fode/db_engines/#database-engines","title":"Database Engines","text":"<p>Database Engines or Storage Engines are binaries which provides low level disk storage operations like storing and retrieving data from disk, providing compression over stored data, handling crash recovery, indexing over data, etc. When you run an DML query, the parse and optimizer translates the query into these low level operation which are then executed by engine.</p> <p>Other benefits of designing these engine is you don't have to start from scratch to build an DB system, which  supports your specific use case. You can build new features on top of existing engines. Some DB systems (like MySQL and MariaDB) even allows you to switch engines. Below are the most popular DB engines developed over time as requirement for applications evolved.</p>"},{"location":"Courses/fode/db_engines/#myisam","title":"MyISAM","text":"<p>MyISAM (which stands for Indexed Sequential Access Method) was default storage engine for MySQL in  its earlier days (before version 5.5). It became popular during early internet days due to its  simplicity (lightweight) and faster reads. But it didn't support essential features which are required by modern application like - No transactions, ACID making data storage unsafe and inconsistent. - Write operations locked entire table resulting in poor write performance in concurrent environmnet. - Didn't provide support for foreign key</p>"},{"location":"Courses/fode/db_engines/#innodb","title":"InnoDB","text":"<p>Developed to address the problems encountered by MyISAM, InnoDB became the default storage engine for MySQL since version 5.5 by providing essential features like ACID transactions, row level locking, crash recovery, foreign key constraints and more. It stores data in a B+ Tree Clustered Index around primary key.  </p>"},{"location":"Courses/fode/db_engines/#sqlite","title":"SQLite","text":"<p>A lightweight, serverless, self-contained SQL engine which became popular for being extremely lightweight, allowing it to be embedded into applications directly. There's no separate process running the DB in background, it's just your application process which reads and writes data from the file directly. Additionally, you don't need to configure or setup any additional step, just include the SQLite file. Since whole database is stored in single file, copying, versioning, deploying is very fast and simple. It's ACID compliant, works across  different platforms (like Windows, Linux, macOS). It's use across various popular apps like browsers, mobile apps, and IoT devices. However, due to this simplicity its not ideal for large datasets or high throughput writes.</p>"},{"location":"Courses/fode/db_engines/#berkeleydb","title":"BerkeleyDB","text":"<p>An embedded key\u2013value storage engine that provides ACID transactions, multiple indexing methods, and configurable concurrency control. It's extremely fast, reliable, and flexible, making it perfect for embedded systems, but it doesn\u2019t provide SQL and requires the application to handle schema and data logic.</p>"},{"location":"Courses/fode/db_engines/#leveldb","title":"LevelDB","text":"<p>LevelDB is a fast, embedded, single-threaded, key\u2013value storage engine using an LSM-tree design which allows fast sequential writes, high compression, lower write amplification and efficient storage layout.  Range or prefix queries over sorted key are much faster. Writes are optimized using LSM design while Reads are optimized using a combination of Memtable, SSTable and Bloom Filters. It's popularly used across application as Blockchain, Caching layers, and Web Browsers (much lighter than SQLite).</p>"},{"location":"Courses/fode/db_engines/#rocksdb","title":"RocksDB","text":"<p>Built as a fork from LevelDB, RocksDB introduced enhancements to provide low latency operation, higher write throughput optimized for SSDs and large scale production workloads. It's popularly used across systems like Kafka streams, Blockchains, Cockroach DB. You can check out the summary of major enhancements over its ancestor in following table.</p> Feature LevelDB RocksDB Concurrency Single-writer, limited readers Multi-threaded, many writers, parallel compaction Performance Good for small apps Extremely high throughput for large workloads Compaction Simple, single-threaded Multi-threaded, advanced, tunable compactions Transactions No transactions Full ACID transactions via WriteBatch + WAL Column families Not supported Supported (namespaces like MySQL tables) Tuning options Very few Hundreds of tuning knobs for memory, IO, compaction Memory optimization Basic Block cache, compressed cache, rate-limiting Storage types Disk only Optimized for SSD / flash, persistent memory Backup &amp; restore Manual Built-in APIs for backup, checkpoints, replication Use cases Small embedded apps Large-scale server apps, distributed systems"},{"location":"Courses/fode/db_index/","title":"B-Tree Index","text":""},{"location":"Courses/fode/db_index/#b-tree-index","title":"B-Tree Index","text":"<p>Indexes in DBs are used to speed up your read queries.</p> <p>Without Indexes, when you query for some records on your table, the DB would to sequentially search all the table pages and gather records matching your query. This internally involves many steps like loading the page from disk into memory and filtering the tuples, which is very inefficient and the time took grows linearly with the dataset. To optimize this, you can maintain a sorted order in your table which allows you to use Binary Search to cut the time taken to \\(O(log n)\\). But this sort of algorithm operates very poorly at disk level, since it requires you to load random pages causing lots of random I/O which under utilizes page buffer pool in memory. To minimize random disk I/O, computer scientists invented a balanced tree data structure where each node (DB page) stored multiple keys along  with pointers to multiple child nodes similar to below diagram.</p> <pre><code>graph TD\n\n    %% Level 0 (root)\n    R[\"[20 | 40]\"]\n\n    %% Level 1\n    A[\"[10]\"]\n    B[\"[30]\"]\n    C[\"[50 | 60]\"]\n\n    R --&gt; A\n    R --&gt; B\n    R --&gt; C\n\n    %% Level 2\n    A1[\"[5]\"]\n    A2[\"[12 | 18]\"]\n\n    B1[\"[22 | 25]\"]\n    B2[\"[32 | 35]\"]\n\n    C1[\"[45 | 48]\"]\n    C2[\"[55]\"]\n    C3[\"[65 | 70]\"]\n\n    A --&gt; A1\n    A --&gt; A2\n\n    B --&gt; B1\n    B --&gt; B2\n\n    C --&gt; C1\n    C --&gt; C2\n    C --&gt; C3</code></pre> <p>The tree consists of root node which is the starting point for search, branch nodes consisting of  keys and pointers to next level of nodes, and leaf nodes which consists of actual keys and respective data. With this you can store the root and branch nodes in memory most of the time as they're required for navigating through the tree and only evict the leaf nodes as required, essentially reducing disk I/Os required to find a key. The data structure is named as B+ Tree or commonly known as B Tree index. Additionally, the leaf nodes also have reference to their respective neighbouring leaf node which saves you the cost of traversing the whole tree in ordered to fetch a range of key.</p> <p>Let's look into how DBs use this B-Tree structure to perform different CRUD operations.</p> <ol> <li>Search: The search begins from the root node to respective leaf node by repeatedly comparing the search key    with keys in current node and moving to appropriate child node. For example, if search key is between two keys in    a node, move to the child pointer between them. Since the keys in each node are ordered, we can use binary search    to determine the key location in \\(O(logk)\\) time(k -&gt; number of keys in each node or the degree of B-Tree).</li> <li> <p>Insert: Find the leaf node responsible for holding the new key and place the key into the leaf node. If the node    is overfilled a page split occurs.</p> <ul> <li>Divide the leaf node into two leaf nodes.</li> <li>Add the middle key from leaf node into parent node.</li> <li>Update the pointers to child nodes in parent node.</li> </ul> <p>This process is cascading as page split in low level nodes can move upto higher level nodes which result in   lots of movement of data on disk than required, causing write amplifications and spikes in disk usage.  You can minimize this effect by using sequential index keys where the index is filled from left to right.</p> </li> <li> <p>Delete: Find the node containing the key to be deleted and removing the key. If the node has too few keys after    deletion, it must be restructured by either Borrowing a key from sibling node or Merging with a sibling node     which can cascade to higher level nodes potentially reducing the height of tree. </p> </li> <li>Update: It usually involves a search for the key and then performing a delete operation, followed by an insert     for the new value.</li> </ol> <p>To test it out and visualize these operation, you can use this  website.  </p>"},{"location":"Courses/fode/db_index/#secondary-index","title":"Secondary Index","text":"<p>But sometimes the table would also be queried using other fields which aren't primary key. In such cases you can develop a Secondary Index on such field, which basically creates same B-Tree Data structure with the  leaf nodes pointing to primary key or tupleId (in case of Postgres).  Few things to keep in mind when using Secondary Index</p> <ul> <li>Some DBs (like InnoDB), Secondary Indexes use primary key internally to locate the row. In such case, the size   of <code>pk</code> needs to be kept in check and choosing large <code>pk</code> can lead to larger secondary index essentially   degrading its performance.</li> <li>Each additional secondary index causes more write amplification, as the DB now have to update all these structures   which related incoming change in table. Postgres handles this a little differently by lazily updating the pointers   with a regular cleanup process, and in the meantime it'll just mark the pointed tuple data with respective change so that   DB can make the right decision.</li> <li>While <code>pk</code> is guaranteed to be unique, secondary index keys can be duplicate. Indexes perform best when the indexed    keys are selective, i.e. they map to small set of tuples. If a non-selective key is queried using index, the DB would   have to query all the pointers which are scattered across pages which could potentially lead to more I/Os. In such   cases, DB query optimizers decides to choose a less efficient execution plan like full table scans. Writes are similarly   influenced by non-selective indexes, as DB have to maintain the large list of pointers for each such operation.</li> </ul>"},{"location":"Courses/fode/db_index/#composite-indexes","title":"Composite Indexes","text":"<p>Queries which include multiple AND filters for search can hugely benefit from  Composite Indexes. Composite Indexes allows you to include more one keys in same index which are internally sorted and stored by concatenating them together from left to right. For example, index on <code>(A, B, C)</code> would store the key as <code>A_B_C</code>. The order in which columns are provided when creating composite index matters since you can still use the index to filter tuples by selectively using left most columns in provided order but same can't be done for rest of columns.</p>"},{"location":"Courses/fode/db_index/#uuid-in-b-tree-indexes","title":"UUID in B-Tree Indexes","text":"<p>UUID4 are completely random identifiers, two UUID4 Ids generated consecutively can never be ordered one after another. Using such completely random (like UUID4) values in B-Tree index are disastrous and should be avoided as they negatively impact both reads and writes.</p> <ul> <li>Writing random keys in B-Tree would result in frequent page split and fragmentation of data. This could be avoided   if we had a sequential key for index which would fill up the index entries sequentially from left to right.</li> <li>Reading requires loading whole pages into a shared buffer pool memory. If the buffer is full, DB will eliminate the    oldest page to load up the current page. And in case of UUIDs, we\u2019ll end up loading and removing page randomly    because we\u2019ve no ordering. Instead, if we had sequential key for index, we\u2019d have related pages which loaded up   into memory, so if let\u2019s say there's a surge in read for a category of products, they\u2019ll be present in same nearby   pages because their id are sequential. </li> </ul>"},{"location":"Courses/fode/db_index/#long-running-transactions-in-postgres","title":"Long-running transactions in Postgres","text":"<p>In Postgres, any DML transaction touching a row creates a new version of that row.  If the row is referenced in indexes, those need to be updated with the new tuple id as well.  There are exceptions with optimization such as heap only tuples (HOT) where all the index doesn\u2019t need to be  updated immediately but that only happens if the page where the row lives have enough space (fill factor &lt; 100%)</p> <p>If a long transaction that\u2019s updated millions of rows rolls back, then the new row versions created by this transaction (millions in my case) are now invalid and shouldn\u2019t be read by any new transaction.  You have many ways to address this, </p> <ul> <li>do you clean all dead rows eagerly on transaction rollback?</li> <li>Or do you do it lazily as a post-process?</li> <li>Or do you lock the table and clean those up until the database fully restarts?</li> </ul> <p>Postgres does the lazy approach, using <code>VACCUM</code> command which is called periodically to remove dead rows and free up space on the page.</p> <p>What's the harm of leaving those dead rows in?  It's not really correctness issues at all, in fact, transactions know not to read those dead rows by checking the state of the transaction that created them. This is however an expensive check, the check to see if the transaction that created this row is committed or rolled back. Also, the fact that those dead rows live in disk pages with alive rows makes an IO inefficient as the database has to filter out dead rows. For example, a page may have contained 1000 rows, but only 1 live row and 999 dead rows, the database will make that IO but only will get a single row of it. Repeat that and you end up making more IOs. More IOs = slower performance.</p> <p>Other databases do the eager approach and won\u2019t let you even start the database before rolling back completely, using undo logs. Both approaches have their pros and cons and at the end it really upto your workload which approach suits you best.</p>"},{"location":"Courses/fode/db_internals/","title":"Database Internals","text":""},{"location":"Courses/fode/db_internals/#database-internals","title":"Database Internals","text":"<p>At a high level, DB systems involves storing vast amount of data and providing an API for querying over the data efficiently. To solve this DBs uses various strategies and data structure, the majority of which revolves around these two data structure: Table and Index.</p>"},{"location":"Courses/fode/db_internals/#table","title":"Table","text":"<p>Table is a logical structure which defines how data is modeled and stored on disk. From outside, table is collection of rows(1) and columns(2) - usually representing an entity in your application. Internally, table is a collection of tuples (rows) organized across pages on one or more data files.</p> <p>{ .annotate }</p> <ol> <li>unique record or instance of entity referenced by the table </li> <li>specifies the attribute or field of referenced row </li> </ol>"},{"location":"Courses/fode/db_internals/#tuple","title":"Tuple","text":"<p>Tuple is a structured block of bytes stored inside a page, the physical realization of a row. It's composed of header and data field, with following format roughly,</p> <p><code>| tuple header (visibility, length, flags, etc.) | col_1 value | col_2 value...|</code></p> <p>Within the page, it's paired with a line pointer which is generated by the byte offset of Tuple relative to the start of page, similar to following:</p> <pre><code>[Page]\n...\n \u251c\u2500\u2500 Line pointer \u2192 Tuple #1 (offset 40)\n \u251c\u2500\u2500 Line pointer \u2192 Tuple #2 (offset 120)\n |\n...\n |\n [ ... Tuple Data Area ... ]\n</code></pre> <p>With this, DB can globally identify each tuple physically within a table using following combination known as  Tuple ID -&gt; <code>(PageNumber, LinePointer)</code>. For example, <code>TID -&gt; (PageNumber=42, LinePointer=2)</code> would read the page number \\(42\\) into memory, lookup the value of line pointer \\(2\\) and fetch the data pointed by line pointer. TID allows DBs to reference rows efficiently and uniquely across various data structures like indexes.</p> <p>When designing a DB system, there are two broad choices based on how this TID mapping to physical location is utilized:</p> <ol> <li> <p>DBs (like Postgres) where table is physically stored as a heap (1) uses <code>(PageNumber, LinePointer)</code> directly    since the rows are unordered. </p> <ol> <li>Unordered pages of tuple</li> </ol> </li> <li> <p>DBs (like MySQL InnoDB) which uses clustered indexes (1), lays the physical location of tuple using primary key.    Due to this, the  TID mapping mentioned above can't be utilized directly.     You've to use primary key to get the physical location, which internally uses mapping similar to TID managed by      storage layer.</p> <ol> <li>Tuples are stored inside the index, whose key determines the physical and logical location of tuple.</li> </ol> </li> </ol> Tradeoff <p>You get the following summarized tradeoff based on this decision choice:</p> Aspect PostgreSQL (Internal TID) MySQL/InnoDB (External RowID) Row addressing Physical (page, slot) Logical (primary key) Update behavior (MVCC) New version = new TID (row moves) Row stays; old versions kept in undo log Index maintenance Indexes point to TIDs \u2192 need update when TIDs change Indexes point to PKs \u2192 stable, fewer updates Lookup cost (secondary index) One hop (index \u2192 heap) Two hops (secondary \u2192 primary \u2192 data) Insert performance Fast (append to heap) Slower (must maintain clustered order) Range scan performance Slower (heap unordered) Faster (rows ordered by PK) VACUUM / cleanup Required to reclaim old tuples Handled via purge of undo logs Storage flexibility Simple, flexible heap More rigid due to clustering"},{"location":"Courses/fode/db_internals/#page","title":"Page","text":"<p>All data structures in DBs (like Tables, collections, rows, columns, indexes, sequences, documents)  end up as bytes in a page. This model allows you to decouple the storage engine from the DB frontend which is  responsible for formating the data and provide an API over it.  A DB page is the fundamental unit of I/O and storage inside a DB engine. It\u2019s the smallest chunk of data the DB reads from or writes to disk or caches in memory.</p> <p>But why do DBs use fixed size blocks for its read/write operations? This is due to the way disk storage (HDDs/SDDs) work and how they're different from memory (RAM). </p> <p>Note</p> <p>Physical Disk operates at the level of sectors (HDD) or blocks (SDD) which allow you to persist fixed chunks of data even in absence of power. Working with disk is abstracted by OS using LBA API and then File System API, which defines fixed size chunks known as Pages (usually \\(4\\) KB) as smallest unit to read and write from disk (defined as an I/O).</p> <p>So working with disk requires you to use fixed size blocks of storage known as pages. To be not confused with OS Pages, DBs uses their own logical page definition. This approach offers several key advantages which are requirements for DB systems:</p> <ol> <li>Separation of Concern, OS is designed to manage generic files and memory, while DBs needs exact control on    how data is laid out, cached, logged, and recovered. Using its own Page abstraction would allow DB system    to use all such optimize optimization.</li> <li> <p>OS Page size defaults to \\(4\\) KB which works for many applications, but is inefficient of DBs    since the space acquired by metadata would out weigh the useful information available. Today, DBs uses    page size ranging from (\\(8\\)-\\(16\\) KB) optimized for their workload.</p> Small vs Large DB Pages <ul> <li>Small pages are faster to read and write especially if the page size is closer to the media block size.    However, the overhead cost of the page header metadata compare to useful data can get higher for smaller pages.</li> <li>Larger pages can minimize metadata overhead and page splits but at the cost of higher cold read/write.</li> </ul> </li> <li> <p>This allows DBs to implement their own page cache/buffer pool as they already know which pages are \"hot\"(1) and want to     manage them efficiently. </p> <ol> <li>When the required page is already present in memory/cache.</li> </ol> </li> <li>You can consistently port your data to different OS platform, which make replication and backup operations simpler.</li> </ol> <p>A simplified page structure on disk looks like as follows:</p> <pre><code>+----------------------------------------------------+\n| Page Header (metadata)                             |\n|----------------------------------------------------|\n| Line Pointer Array (Item IDs / Slot Directory)     |\n|----------------------------------------------------|\n|                    Free Space                      |\n|----------------------------------------------------|\n| Tuple Data Area (actual rows/tuples)               |\n|----------------------------------------------------|\n| Special Space (optional, for indexes)              |\n+----------------------------------------------------+\n</code></pre> <ul> <li>Page Header is a fixed size bytes which stores metadata like Page LSN (Log Sequence Number) -&gt; for WAL consistency,   checksums/CRC -&gt; corruption detection, and various flags and pointers.</li> <li>Line pointer array stores offset of tuple within the page along with other metadata like tuple length or whether   the tuple is dead or redirected, etc. These pointers grow downward from the header, as such the deleted/updated   tuples are marks respectively without moving other tuples immediately.</li> <li>Tuple Data Area, where actual data is stored. It consists of tuple headers (like visibility info, transaction IDs) and    column data. Tuple Data grows upward from bottom of page.</li> </ul> <p>Note</p> <p>Space occupied by dead/redirected tuples are reclaimed later by cleanup operations like <code>VACCUM</code> or compaction.  </p> Role of Page Layout in forming different DB domains <p>The internal page layout and tuple organization fundamentally define what kind of database it is:</p> <ul> <li>Row based DBs stores complete tuples (rows) in each page. This gives you access to all columns of a row in same page,   which is ideal for OLTP workloads involving frequent read/writes over entire rows.</li> <li>Columnar DBs organizes each page to store data for individual columns across many rows. Since columns are   contiguously stored, such DBs are ideal for OLAP workload which involves reading few columns across large number of rows.</li> <li>Document DBs organizes their data in self-contained object (JSON/BSON) of variable length known as documents. This   model provides flexible schema and faster document level read/writes.  </li> <li>Graph DBs models their data as nodes and relationships (edges). Each page stores either nodes, relationship or property   records, where the relationship records has direct pointers to start and end nodes, making sequence traversal fast.</li> <li>Key Value Stores (Sorted Key Pages) like RocksDB, LevelDB stores sorted key-value pairs.    The storage engine (often an LSM tree) manages multiple sorted runs which makes it ideal for efficient for range    scans and sequential writes.</li> </ul>"},{"location":"Courses/fode/db_internals/#index","title":"Index","text":"<p>Another important data structure which plays crucial role in working of DBs is Index. Logically index is a data structure which allows database to quickly locate rows without scanning the entire table. Physically, they\u2019re just files made up of fixed size pages consisting of index entries. The structure of these entries (and how pages are connected) defines the index type. Among which the most common type is B-Tree (B+Tree), which almost all general-purpose databases (Postgres, MySQL, Oracle, SQL Server, SQLite) use as standard indexes.</p> <p>Note</p> <p>To be not confused with B-Tree and B+Tree, modern DBs entirely uses B+Tree implementation, but you'll still find people using B-Tree naming convention from place to place.</p> <p>At a high level, B+Tree index is a hierarchy of pages consisting of Root, Internal and Leaf pages (or nodes). The Root node is entry point for lookup, points to Internal nodes consisting of indexed keys -&gt; child node mapping. At the bottom, we've Leaf nodes which consists of the index key -&gt; TupleID mapping. Each node stores sorted list of keys and pointer which allows you to perform Binary Search on it to find the  respective key in \\(O(logn)\\) time. Also, the leaf nodes are doubly linked to adjacent nodes, which allows you faster range queries.</p> <p>Some DBs (like InnoDB) decided to store the data tuple within index itself instead of using TID. Such indexes are known as Clustered Index, as they cluster the table within index itself. Since data is stored directly in index, the I/O cost for lookup is usually 1-page read which is faster compared other its counterpart. You also get faster range queries since the rows would live within same page if the keys are sequential. But this design is severally impacted inserts, since you must keep rows physically ordered by key which can cause page splits and  random I/O if the indexing key is random. Also, secondary Indexes point to primary key in clustered index. If you primary key is modified, all your secondary index needs to be updated. You also need to be aware of the primary key size, which can impact the secondary index size and performance.</p> <p>To optimize index performance, try to keep the index size as small as possible, since we need to load the index pages into memory before working with them. Smaller index entries would allow us to pack more information per index page, and if such information is user managed -&gt; just be mindful about the size of user defined field stored in such entries.</p>"},{"location":"Courses/fode/db_partition/","title":"Database Partitioning","text":""},{"location":"Courses/fode/db_partition/#database-partitioning","title":"Database Partitioning","text":"<p>As your table size grows, querying data from it becomes more and more slower. Even the index would grow larger,  making its navigation slower. At this point, the best way to optimize your queries is by dividing your table into smaller datasets which are operable independently. One of the way to do this is using Partitioning, where you divide your table into smaller tables (partition) based on a partitioning key, which map the rows to their respective partition.  </p> <p>For example, we to execute the following query -&gt; <code>SELECT name FROM Customer WHERE Id=655970</code> . In original table, DB have to sequentially scan the whole table to fetch this row. But if we partition our table on <code>Id</code> as following figure </p><pre><code>flowchart LR\n    A[Customer Table] --&gt; B[Partition 1&lt;br/&gt;Id: 1 - 200,000]\n    A --&gt; C[Partition 2&lt;br/&gt;Id: 200,001 - 400,000]\n    A --&gt; D[Partition 3&lt;br/&gt;Id: 400,001 - 600,000]\n    A --&gt; E[Partition 4&lt;br/&gt;Id: 600,001 - 800,000]\n    A --&gt; F[Partition 5&lt;br/&gt;Id: 800,001 - 1,000,000]\n\n    class B,C,D,E,F partition;</code></pre> The DB would directly jump to Partition 4 since it knows that <code>Id</code> -&gt; \\((600k,800k)\\) belongs to that partition and here it\u2019ll only have to scan through 200k rows at max. <p></p> <p>The above example is known as Horizontal Partitioning where we slice table along the rows. There\u2019s a lesser popular version of partition known as  Vertical Partitioning which slices table along column,  which you can use to slice column which are larger and lesser frequently accessed (like a blob). This allows you keep access to rest of the columns quicker, as same page could now fit more rows.</p> <p>However, efficiently utilizing partitions might not be as straight forward as implementing it. You need to have an understanding over you data and queries, like frequently accessed data which could cause  hot partitions, and determining queries which are crucial for performance. And depending on these factors, decide the right partitioning strategy like:</p> <ul> <li>partitioning on a range of keys, (like dates or IDs) which is used ideal for data with natural ordering</li> <li>partitioning on a list of keys, (like region, category) which is ideal for discrete predefined data.</li> <li>partitioning on a hash of keys, to evenly distribute the data, reducing hotspots and improving performance</li> </ul> <p>Along with it, you need to set up plans for maintenance and evolving partitions like automating partitioning creating in case of range partitioning, and monitoring the performance and usage of each partition to identify the imbalance and address them effectively. Following table briefly summarizes the pros and cons of using partitioning.</p> Advantage Disadvantage Improves query performance when accessing a single partition as it\u2019ll have lot less rows then original. Updates which moves row from one partition to another are slower. Improves Sequential Scan and Index Scan as both underlying data structure are a lot less in size compared to original. Inefficient Queries could scan all partitions if not used properly. This is a lot more slower Easy to bulk import data by attaching partition Schema changes can be challenging. Archieve old data into seperate partition which can use cheaper storage."},{"location":"Courses/fode/db_security/","title":"Database Security","text":""},{"location":"Courses/fode/db_security/#database-security","title":"Database Security","text":"<p>Data is a critical part of every business which should be handled responsibly for successfully running a business. Security plays a vital role in managing such data by using set of policies and controls to protect data from unauthorized access, misuse, alteration or destruction ensuring only the right user can access the right data. Key goals of security in DB involves:</p> <ul> <li>Keeping data confidential, by protecting it from unauthorized access</li> <li>Maintaining data integrity, by controlling access so that it can't be altered by just anybody.</li> <li>Ensuring data is available whenever needed, it shouldn't be lost on accidents or crashes.</li> </ul> <p>Few components which helps DB achieve these goals</p> <ul> <li>Access Control: Control who can access what data and how they can use it by using proper authentication and   authorization. For example, only admins should've access to creating and deleting tables.</li> <li>Encryption: Data should be protected from 3rd parties by using safe encryption at both rest and transit to    keep them confidential.</li> <li>Backup &amp; Recovery: Keeping regular backups and redundant copies can prevent data loss due to failure, or crashes.</li> </ul> <p>REST apps sometimes requires database tables to be present, and it\u2019s usually covered with the startup  of app to create the table if not present. This is a bad practice and should be avoided, because when your app users are interacting with your database \u2014 they\u2019ll have full privilege to your  DB which can cause serious harm like SQL Injections or XSS attack to drop the table.  Instead, you should keep separate users for creating tables, schema, etc. and have separate users for read/write permissions. You can also maintain separate connection pools in client side for each of these  read/update/delete operations.</p>"},{"location":"Courses/fode/db_security/#homomorphic-encryption","title":"Homomorphic Encryption","text":"<p>Encryption is transforming data into random text which doesn\u2019t make sense when looked at.  To make the sense out of it, you\u2019ll have to decrypt the random text.  You can do this in two different way, by using a common key (symmetric encryption) for both encryption and decryption or by using separate keys (asymmetric encryption) for encryption and decryption.</p> <p>In database systems, we can\u2019t simply encrypt the data because of few reasons - queries need plain data to perform their work. To work with encrypted data, it\u2019ll have to decrypt it first    everytime which isn\u2019t optimal. - Analysis of data, indexing, tuning the data needs plain text - application needs recognizable data to process it. - Layer 7 reverse proxies terminates TLS connections so that it can read the traffic and apply   routing rules on it.</p> <p>Homomorphic Encryption allows you to do all these operations on encrypted data by allowing  Arithmetic operations on encrypted data. You can use indexes and query on encrypted data using these Arithmetics because at low level each of these operations are simply performed by comparison, shifting bits, adding, etc. However, it\u2019s still in PoC as actual querying is too slow for production system.</p>"},{"location":"Courses/fode/db_shard/","title":"Database Sharding","text":""},{"location":"Courses/fode/db_shard/#database-sharding","title":"Database Sharding","text":"<p>Another way to divide you dataset into smaller chunks is using Shards. It divides the table into smaller tables  similar to partitioning, but the key difference is each table lives in a separate DB instance with sharding. Using separate DB instance would provide u additional benefits like more resources (CPU, memory) for each sharded table, or network advantage by geographically locating the shard closer to client, or provide specific security standards for specific group of users. </p> <p>You can create shards based on keys like, zipcode which represent an area geographically, or using range of values on some number field. But what if your have to shard on some random text? Mostly shard keys which can\u2019t be distinguished into groups are grouped using Consistent Hashing.</p> Consistent Hashing <p>Consistent hashing is an algorithm which distributes different shards as points on a ring like data structure and the range of values which falls on a pie of the ring belong to a single shard. To map values to ring, you\u2019ve different hash function which evenly distributes the shard keys so that no  single shard is overused.</p> <p>Following are few key advantages and disadvantages of sharding summarized briefly.</p> Advantage Disadvantage You get scalability in data, memory, CPU, etc Makes the client complex as it needs to be aware of each shard. You get smaller tables and as such smaller indexes Transaction across shard wouldn\u2019t be atomic anymore. You get security as data can live in separate database instances for users which require most secure storage. Rollbacks would be expensive Schema changes are hard The query must know which shard to hit, otherwise the client would\u2019ve to search every database. <p>So when should you use sharding to optimize your DB performance? Sharding should be your last option for optimizing your database performances due to the complications involved  which most of the time are unnecessary. There are many other optimization trick you can do before ultimately using sharding</p> <ul> <li>You can look into horizontal partitioning before it which allows you to divide your table into smaller   chunks and provide smaller index for all these partitions.</li> <li>Then, if your reads are slow, you can look into replication \u2014 where you can employ multiple read replicas    to distribute the load over a single server. </li> <li>If you\u2019re facing slow writes, maybe try to separate servers based on regions.</li> </ul> <p>Sharding does provide you with scaled writes and read, but you\u2019ll have to leave behind features like ACID transactions. Also, the client coupling to database is strong since client needs to be aware of each shard. And resharding or changing business logic also becomes complicated. So avoid using it until its absolutely  necessary.</p> <p>Vitess</p> <p>To avoid coupling you can transfer sharding client logic to a backend app which will handle which shard  should the query be directed to. One such backend app is Vitess.</p>"},{"location":"Courses/fode/extra/","title":"Extra Topics","text":""},{"location":"Courses/fode/extra/#extra-topics","title":"Extra Topics","text":""},{"location":"Courses/fode/extra/#index-selectivity","title":"Index Selectivity","text":"<p>You want to index column which provides as few rows when filtered through.  For example, we index a column which stores Gender which can have 3 values,  and filtering any gender would result in massive result set. So instead database would go for heap scan.  </p>"},{"location":"Courses/fode/extra/#postgres-tupleid","title":"Postgres TupleId","text":"<p>All indexes in Postgres point to a tupleId which is used as the key to cluster around the table. Whenever you make any update, a new tupleId will be generated for the same row. Due to this Postgres needs to update all the indexes pointing to this row with old tupleId to new tupleId,  which isn\u2019t optimal when we\u2019ve a lots of indexes. Still it tries to optimize this by using  Heap Only Tuple (HOT) optimization where it\u2019ll immediately update the tupleId in index of the updated column. But this could still cause issue for index pointing to old tupleId. This is resolved by storing some metadata on old tupleId page, which points to the latest tupleId of this row only if the new tupleId is on same page.  This can be used to advantage by using the fill factor configuration which tells the limit upto which a page can be filled to leave some space for updates and inserts. </p>"},{"location":"Courses/fode/extra/#wal-redo-and-undo-logs","title":"WAL, Redo and Undo Logs","text":"<p>Logs help DBMS to ensure durability and crash recoverability. Whenever you commit writes, it must be persisted  by database. It basically means the data must be present even after shutting of the DBMS.  One way to do this is to directly flush the changes to disk after each commit, but this way your commit operation will become slower as writing to disk is heavy (because writes to disk are in whole pages,  not individual bytes). Instead, databases, keep changes in memory and marks the pages as dirty to indicate the  page has been updated. This approach is fast but can compromise with the persistence of data. To be 100% sure with persistence, database instead maintains logs contains each of these changes as a tiny delta which are  appended to the end of the log file and can be replayed to update the state of database. This log is called Write Ahead Log (WAL). WAL can\u2019t grow infinitely large, so after it grew to some size we\u2019ll flush the changes in WAL to disk and clear our WAL since it no longer needs to maintain the logs. And we\u2019ll restart again. This flushing is of changes to disk is called checkpointing. Checkpointing operation are very heavy  operations as it includes a lots of IOs and compute operations which can cause spike in systems resource usage and impact its performance. To make this tolerable we can make checkpoints smaller so that we can flush to disk frequently, but not smaller enough to impact the writes operation.  </p>"},{"location":"Courses/fode/extra/#endurance-of-ssds","title":"Endurance of SSDs","text":"<p>SSDs store data within page present in fixed blocks. There\u2019s no mechanical apparatus like hard disk which makes them faster compared to them. However, SSDs can update certain page until a limit after which the bytes are no longer usable essentially reducing the size of SSD. Due to this workload involving updates aren\u2019t considered good for SSDs. For example, B-Tree indexes which restructure themselves are considered bad for SSDs but index like LSM-Tree which only appends entries are considered optimal for SSDs.</p>"},{"location":"Courses/fode/extra/#postgres-architecture","title":"Postgres Architecture","text":"<p>Postgres is a SQL row-based database which follows MVCC storage model where each row can have multiple  physical version on disk with the last version is the latest. Basically every insert/update/delete operation create a newer tupleId of row indicating the current version (lookup pros and cons of this decision). Then it uses processes instead of threads for work. Let\u2019s discuss all the process below: </p> <ul> <li>Post master: first process to spawn on startup, acts a parent process for all other processes,   works at listener to connect external application over network. Every other process is forked from this process.</li> <li>Backend Processes: each client connection receives its own backend processes to receive and process the    request. This is a bad choice as processes requires more memory and CPU context switching which impacts   the performance. But postgres avoid this by offloading most of the work outside these processes.</li> <li>Shared Memory: Or shared buffer pool where most of the data which is shared among processes are present like   WAL records, pages, etc.</li> <li>Background workers: backend processes uses these workers to outsource most of its work like querying   based on the generated plan. If a parallel plan is needed, these background workers will be picked up and   assigned respective work.</li> <li>Auxiliary Processes: </li> <li><code>bw</code> (background writer) wakes up periodically,and write pages from shared memory to disk to free up the memory.</li> <li><code>cp</code> (checkpointer) directly flushes the WAL records and pages to disk and creates a checkpoint which      indicates that data uptil this point is consistent. </li> <li><code>lg</code> (logger) is used to writes logs. </li> <li><code>avl</code> (Auto vacuum launcher) launches autovacuum workers. </li> <li><code>wa</code>(WAL archiver) responsible for backing up WAL records</li> <li><code>wr</code> (WAL receiver) runs on replica to replicate data from WAL records</li> <li><code>ww</code> (WAL writer) writes record to WAL and flush them to disk.</li> <li><code>st</code> (startup process) which is actually the first process to start whose role is to check if the pages     are consistent with WAL records, if not mark them as dirty pages. As this needs to be done before any     client connects to database, it should be the first process to start.</li> <li>AutoVacuum Workers: Periodically Vacuums the database which essentially means it frees up old tupleId   which are no longer required by any transactions. Vacuum includes much other stuff you can explore online.</li> <li>WAL Senders: responsible for sending WAL records from client to replicas.</li> </ul>"},{"location":"Courses/fode/extra/#table-joins-using-hash-tables","title":"Table Joins using Hash Tables","text":"<p>In simple terms one relation is mapped to another using key and value as respective column and foreign key. Usually the column with less value is picked up as key, as the hashtable will be smaller than. To map the value to foreign key \u2014 you fetch the foreign key row, and the row from source table with same foreign key, map it in hash table and finally u can use it to join the tables. </p>"},{"location":"Courses/fode/extra/#storage-of-null-value-in-database-systems","title":"Storage of NULL value in database systems","text":"<p>NULL in database system indicate that the given place isn\u2019t allowed to store data. Postgres uses a null bitmap in front of every row to indicate if the given column is null. The size of this bitmap starts with 8bits for 8 column each and with more columns it increments with 8 bytes  in size (so 9-64 column would use another 8 bytes). This helps you save space on disk, as you no longer have to persist the column for null value and allows you to fit more rows within same page. However, be careful when working with NULLs as they\u2019re widely inconsistent:</p> <ul> <li><code>SELECT COUNT(FIELD)</code> would ignore counts of row having null value in given field but    <code>SELECT COUNT(*)</code> would give you correct count.</li> <li>You can\u2019t compare null value, you can just check if It's null or not null. You can\u2019t use <code>T in [NULL]</code></li> <li>Not all database support NULLs in indexes, check if beforehand.</li> </ul>"},{"location":"Courses/fode/extra/#write-amplifications","title":"Write Amplifications","text":"<p>Basically when the actual work done for write is much more than the logical work required.  For example, postgres creates new row even for every update/delete (for versioning). Now this new row with new tupleId must be updated in all existing index on this table.  This is somewhat optimized by updating the index of columns whose value had been changes and rest of untouched column index can be updated in background. Those old rows will now also point to this latest version known as heap only tuple (HOT), because these version must be present in same page which is managed by using fill factor configuration. Then you\u2019ve WAL writes to achieve durability. All these amplification happens at database level where the database system is responsible. </p> <p>SSDs Disks/Storage also causes write amplifications. SSDs uses charge entrapment where electrons are trapped in different levels on a atom. The way these electrons are trapped can be used as a way to store information in them and this configuration of electrons isn\u2019t lost even in absence of electricity.  These cells are then arranged into rows which are then arranged into pages and pages into blocks. For Database Systems, we just need to be aware of page and block level. </p> <ul> <li>Writes to disk are on page level, and for SSDs you can simply write data to new pages easily.</li> <li>When you want to update the data on a already existing page, you\u2019ll write the data to a new page and mark the existing page as stale.</li> <li>Finally, you can\u2019t clean single pages in SSDs, you\u2019ve to clean the whole block to free up space.</li> </ul> <p>To clean up blocks with both stale and active pages, SSDs have a garbage collection program which moves active data to new block and then clean the block to free up storage. All these processes due to update requires additional work in SSDs causing write amplification.</p>"},{"location":"Courses/fode/indexing/","title":"Working with Indexes","text":""},{"location":"Courses/fode/indexing/#working-with-indexes","title":"Working with Indexes","text":"<p>This page will focus on how indexes works in DBs by demonstration using Postgres.  We'll be using following Employees table with given schema</p> <pre><code>\\d employees\n                            Table \"public.employees\"\n Column |  Type   | Collation | Nullable |                Default                \n--------+---------+-----------+----------+---------------------------------------\n id     | integer |           | not null | nextval('employees_id_seq'::regclass)\n name   | text    |           |          | \nIndexes:\n    \"employees_pkey\" PRIMARY KEY, btree (id)\n</code></pre> <p>By default, Postgres builds B-Tree index around <code>pk</code>. To look around how a query performs,  you also get <code>EXPLAIN ANALYZE</code> command which provides the query plan along with costs and other important information.</p> EXPLAIN ANALYZE <p>Postgres provides you two commands, <code>EXPLAIN</code> and <code>EXPLAIN ANALYZE</code> to understand how the query planner executes  (or intends to execute) a SQL query. It shows info like predicted plan steps (e.g., Seq Scan, Index Scan, Hash Join) estimated costs (<code>cost=...</code>), row counts, and row widths. </p> <ul> <li><code>EXPLAIN</code> shows the query execution plan without actually running the query.</li> <li><code>EXPLAIN ANALYZE</code> executes the query and shows the actual query plan, execution time, and other actual figures.</li> </ul> <p>Let\u2019s look around how different queries behave when working with index in Postgres. The actual figures might vary when executing same query due to optimizations in between like caching.</p> <ol> <li> <p><code>SELECT</code> \u2192 indexed field, <code>WHERE</code> \u2192 indexed field (ignore the example query, it's just for demo purpose) </p> <p></p><pre><code>explain analyze select id from employees where id = 2000;\n                                                          QUERY PLAN                                                           \n-------------------------------------------------------------------------------------------------------------------------------\n Index Only Scan using employees_pkey on employees  (cost=0.42..4.44 rows=1 width=4) (actual time=0.031..0.033 rows=1 loops=1)\n   Index Cond: (id = 2000)\n   Heap Fetches: 0\n Planning Time: 0.332 ms\n Execution Time: 0.072 ms\n(5 rows)\n</code></pre> DB decides to use Index Only Scan because we\u2019ve an index on <code>id</code> which is used for filtering.    Heap Fetch are 0 because the field we\u2019re fetching is present in index itself, so we didn\u2019t have to go to heap.<p></p> </li> <li> <p><code>SELECT</code> \u2192 non-indexed field, <code>WHERE</code> \u2192 indexed field</p> <p></p><pre><code>explain analyze select name from employees where id = 50000;\n                                                        QUERY PLAN                                                        \n--------------------------------------------------------------------------------------------------------------------------\n Index Scan using employees_pkey on employees  (cost=0.42..8.44 rows=1 width=6) (actual time=0.042..0.044 rows=1 loops=1)\n   Index Cond: (id = 50000)\n Planning Time: 0.092 ms\n Execution Time: 0.152 ms\n(4 rows)\n</code></pre> <code>Index Scan</code> \u2192 for identifying the rows, then we\u2019ve to go to heap for fetching the <code>name</code> field in <code>SELECT</code>.<p></p> </li> <li> <p><code>SELECT</code> \u2192 indexed field, <code>WHERE</code> \u2192 non-indexed field</p> <pre><code>explain analyze select id from employees where name = 'P7o';\n                                                       QUERY PLAN                                                       \n------------------------------------------------------------------------------------------------------------------------\n Gather  (cost=1000.00..11310.94 rows=6 width=4) (actual time=0.729..52.861 rows=1 loops=1)\n   Workers Planned: 2\n   Workers Launched: 2\n   -&gt;  Parallel Seq Scan on employees  (cost=0.00..10310.34 rows=2 width=4) (actual time=11.900..27.233 rows=0 loops=3)\n         Filter: (name = 'P7o'::text)\n         Rows Removed by Filter: 333333\n Planning Time: 0.105 ms\n Execution Time: 52.886 ms\n(8 rows)\n</code></pre> <p>Postgres will check if we\u2019ve an index on the <code>WHERE</code> clause, if not we\u2019ve to perform a Parallel Seq Scan    (Full Table Scan). Still it tries to optimize this by using multiple worker.</p> </li> <li> <p>Let\u2019s index our name field and filter using a pattern match</p> <p></p><pre><code>create index employees_name on employees(name);\nexplain analyze select id from employees where name like '%P7o%';\n                                                       QUERY PLAN                                                       \n------------------------------------------------------------------------------------------------------------------------\n Gather  (cost=1000.00..11319.34 rows=90 width=4) (actual time=0.350..73.237 rows=11 loops=1)\n   Workers Planned: 2\n   Workers Launched: 2\n   -&gt;  Parallel Seq Scan on employees  (cost=0.00..10310.34 rows=38 width=4) (actual time=6.337..42.018 rows=4 loops=3)\n         Filter: (name ~~ '%P7o%'::text)\n         Rows Removed by Filter: 333330\n Planning Time: 0.344 ms\n Execution Time: 73.324 ms\n(8 rows)\n</code></pre> Even though we\u2019d an index on <code>name</code> the DB couldn\u2019t use it because we\u2019re filtering for a pattern and not exact value. Since the pattern could fit multiple value, DB decides its more efficient to perform sequential scan.<p></p> </li> </ol>"},{"location":"Courses/fode/indexing/#different-execution-plans-in-postgres","title":"Different Execution Plans in Postgres","text":"<ul> <li> <p>Sequential Scan (or Full Table Scan): When Postgres goes directly to the heap to fetches the query results.    This can be normally identified when query is using no filtering or filtering using field without index.   But an unexpected scenario where this could happen is when it expects the query to bring a lots of rows even   if we\u2019ve filtering using indexed field. For example, <code>id!=10</code> will result only in single row so why go to   index to fetch <code>id!=10</code> when its more efficient to just fetch the rows from heap and discard <code>id=10</code> </p> <pre><code>explain select name from grades where id!=10; \n                        QUERY PLAN                        \n----------------------------------------------------------\n Seq Scan on grades  (cost=0.00..10.26 rows=500 width=15)\n   Filter: (id &lt;&gt; 10)\n(2 rows) \n</code></pre> </li> <li> <p>Bitmap Index Scan: Postgres first creates a bitmap (bits where the position of bit indicates the page    number) for pages in heap and then scans the index to set bits which satisfies the condition.    After completing this, all the pages with set a bit in bitmap are fetched in one go. This is usually performed   when we don\u2019t have lots of rows which proves sequential scan efficient but enough rows that requires fetching   in bulk.     </p><pre><code>explain select name from grades where g = 25;\n                              QUERY PLAN                               \n-----------------------------------------------------------------------\n Bitmap Heap Scan on grades  (cost=4.18..8.43 rows=4 width=15)\n   Recheck Cond: (g = 25)\n   -&gt;  Bitmap Index Scan on grades_g  (cost=0.00..4.18 rows=4 width=0)\n         Index Cond: (g = 25)\n(4 rows)   \n</code></pre>   Fetching rows from page is performed in Bitmap Heap Scan which discard rows which doesn\u2019t satisfy the    query condition. Similar to this, there\u2019s BitmapAnd and BitmapOr Scan which are used when using   more than 2 indexed fields for filtering. In which case, Postgres would develop Bitmap for both the   filters separately and then merge them into one using AND/OR operation depending on filtering condition.<p></p> </li> <li> <p>Index Scan: Postgres mostly uses index whenever we\u2019ve a filtering criteria which uses indexed field.   But it also depends on the amount of data we\u2019re fetching, like previously discussed in Full Table Scans.   If we\u2019ve lots of rows its usually inefficient to fetch results from Index and then fetch the rows from heap.</p> </li> <li> <p>Index Only Scan: Postgres performs this when it can fetch the information asked by query from index itself   (without going to heap). This is more efficient than index scans and can be made useful by adding   a non-key column to the index. For example,     </p><pre><code>create index grades_g on grades(g) include (name);\n</code></pre>     We\u2019ve included column <code>name</code> as non-key to index <code>grades_g</code> where <code>g</code> is the key column.   So all the filtering will be performed on key column, but we can also fetch non-key column from index directly.    However, beware of including a non-key column to index, as it\u2019ll increase the size of index and    will certainly impact on the cost for querying the index (as we\u2019ve to load more pages for same index).<p></p> </li> </ul>"},{"location":"Courses/fode/indexing/#when-does-db-use-index","title":"When does DB use Index?","text":"<p>Suppose we\u2019ve a table <code>T</code> with index on column <code>t1</code> (<code>idx_t1</code>) and index on column <code>t2</code> (<code>idx_t2</code>). How will database plan for following query: <code>select * from T where t1=1 and t2=4</code> ?</p> <ul> <li>If we\u2019ve lots and lots of rows, the optimizer will go ahead with full table scans and filter out data from there.</li> <li>If we\u2019ve very few rows, the optimizer will use a single index to fetch the intermediate rows and   filter out from them based on the other columns condition. Which index is used to fetch intermediate rows?   The one which yields lesser rows in case of AND operation.</li> <li>If we\u2019ve good enough rows (not too few or too many), we\u2019ll develop bitmaps from both the indexes and   BitmapAnd then to get final bitmap to scan the heap.</li> </ul> <p>This decision about how many rows will turn up in a query is estimated based on statistics  which are precalculated by DB for each table. So always remember to update stats on tables before performing any critical operation.</p> <p>You can also force database to use certain index by hinting it in query.  For example, <code>select * from T where t1=1 and t2=4 /*+ index[t1 idx_t1] */</code></p> <p>Above case study also hints on what kind of column you should create index on to have maximum efficiency. For example, if we\u2019ve a column <code>state</code> and most of the rows use a same value for <code>state</code> creating an index on it won\u2019t help with searching your query because you\u2019ll have so many rows with same state that its much more efficient to just perform a sequential scan. </p> <p>Create Index Concurrently</p> <p>Most databases blocks writes when creating an index and this could impact live production system.  To solve this issue, postgres provide this feature to create index concurrently without stopping write in b/w the process. Command: <code>create index concurrently &lt;index-name&gt; on &lt;table&gt;(&lt;column&gt;)</code> It\u2019d essentially create index sequentially and before exiting it\u2019d wait for all ongoing transactions to complete so that they\u2019ve been accounted for within the index.   </p>"},{"location":"Courses/fode/indexing/#bloom-filters","title":"Bloom Filters","text":"<p>Take a case where we need to query username to check if it\u2019s been already taken by another user or not.</p> <p>Directly querying the DB for presence of username is very slow if we\u2019ve a lots of users signing up. Instead, we can use intermediate cache to store username already taken up and query from these, but this approach doubles our memory footprint.</p> <p>To resolve this issue, we can use bloom filter which is essentially a fixed size bitmap on which set bit indicates the possibility presence of that bit number and unset indicates its absence  (the index of bit we need to check for a username can be found by using a hashing function % size of bitmap). With this we can easily redirect most of absent usernames but to confirm the presence of one,  we\u2019ll have to query our database since the bit might be set by other username which collided on same index of our bitmap. </p> <p>If all bits as set on our bloom filter, it'll become useless and if we\u2019re always increasing the size of our bloom filter we\u2019re moving toward more memory footprint. The actual implementation of bloom filter accounts for this pretty well, making it essentially works like above.</p>"},{"location":"Courses/fode/nosql/","title":"NoSQL Overview","text":""},{"location":"Courses/fode/nosql/#nosql-overview","title":"NoSQL Overview","text":"<p>DB Systems can be fundamentally divided into two parts based on their architecture:</p> <ul> <li>Frontend: communication layer which implements the APIs exposes internal functionality to clients and decides   the format of data used for this communication. The most popular format of communication with DB was using table,   which SQL excelled in but as internet evolved various other data structures like JSON became commonly used across   different places. This change became a catalyst to development of DBs using different data formats for communication,   like documents data which used JSON like structure for encoding its data. All these data format were designed because   of the need of performance in their specific use case. </li> <li>Storage Engine: primarily focuses on storing data on disk efficiently, which involves working with bytes so   format of data is irrelevant here. Other responsibility of storage engine involves working with indexes to    store/fetch data, managing data files, using compression to save storage space, providing crash recovery,   and other features like ACID.</li> </ul> <p>The major difference in SQL and NoSQL is between Frontend where the data format is changed to document from rows, and the API format which changed from SQL to simple getter and setter commands.</p>"},{"location":"Courses/fode/nosql/#mongodb-architecture","title":"MongoDB Architecture","text":"<p>MongoDB is a document based NoSQL database popular for its schemaless tables.</p> <p>MongoDB version &lt;4.2: This initial version used Memory Map Index on <code>_id</code> which is a B-Tree index where the leaf node contained a 64-bit pointer to the document. The 64-bit was composed of filename (32-bit) and offset (32-bit) to locate the document in that file, using which the OS can directly jump to the document location and retrieve it. The downside of this design was that any update in page size or data files could  mess up the whole offset based index. Also, it only supported collection level lock for concurrent transactions.</p> <p>MongoDB version 4.2-5.2: The storage engine was replaced with WiredTiger which solve the problem of collection level locks by allowing document level locks. It introduced compression which allowed mongo to fit more documents within a page.  The storage model now included a hidden clustered index (B+ Tree) on a field recordId. Any indexed field would reference this recordId which would then in turn point you to respective page on the clustered index. The problem now what that primary index on <code>_id</code> became slower since we\u2019ve to do two lookups (find recordId \u2192 find page).</p> <p>MongoDB version &gt; 5.3: Introduced clustered collection, which basically built a clustered index around <code>_id</code> field (avoiding recordId). The problem with this was that <code>_id</code> is 12 bytes long, due to which secondary indexes grew much larger.</p>"},{"location":"Courses/fode/nosql/#memcached-architecture","title":"MemCached Architecture","text":"<p>Memcached is a high-performance, distributed, in-memory key-value cache used to speed up dynamic web applications by reducing database load. Its architecture is intentionally simple and optimized for speed. It uses a client-server architecture where server are responsible for storing data in RAM as key-value pair, and client are libraries in apps which decides how to fetch the key within the cache.</p> <p>Distributed</p> <p>The cluster is logically distributed, but coordination is handled entirely by the clients. This makes the system highly scalable and avoids complex distributed consensus.</p> <p>Memory Management: It organizes memory allocation in slabs where each slab holds item of same size.  As new items are added, they\u2019re written to a pre-allocated page serially. The page is divided into equal fixed  size chunks whose determined by the assigned slab class. Each item uses whole chunk/s to stored their information, as such you might have unused memory within each chunk. This is minimized by using the most appropriate slab class for each item. The Slab class vary from class 1 (chunk size of 72 bytes) to class 43 (chunk size of 1MB). This is done to avoid memory fragmentation while keeping allocation fast and  predicatable to ensure high throughput for caching. </p> <p>Fragmentation</p> <p>When storing data sequentially without any strategy, freeing unused memory leaves small gaps of free memory scattered across the physical memory, this problem is known as fragmentation. Fragmentation makes it difficult to get a continuous block of memory large enough to store your  new data item even though there\u2019s more than enough memory present. OS overcomes this problem using  virtual memory, which basically gives us a continuous block but behind the scene is mapped to  multiple small area on physical memory. This still isn\u2019t optimal because to fetch a single block OS will have to fetch multiple pages and reassemble the memory fragments, which is why it's always better to avoid memory fragmentation.  </p> <ul> <li>Threading: Memcached used TCP transport as default to connect to remote clients.     The listener thread creates a TCP socket on port 11211. After accepting the connection,     it's distributed among a pool of worker threads which is responsible for the requested read/write</li> <li>LRU: Memcached use LRU eviction policy when it can\u2019t find any space for new keys.    Even if you put a TTL on a key that it can\u2019t expire before given time, the eviction policy can still   remove this key. LRU is implemented as a linked list where each node is a key-value pair in linked list,   and each slab has its own linked list. When an item is accessed, its move to the head of Linked List.   As result, unused items are pushed down to the tail and can be removed when needed.    One of the disadvantage of using Linked List LRU approach, you need to lock the entire linked list before   any update which serializes write operations and the multithreaded model wasn\u2019t effective.    This model was then updated to have LRU Linked List per slab, which reduced the locking to per slab.   Later on, LRU updates were made once every 60 second to reduce locking further. In 2018, the model was    completely redesigned by breaking LRU into subclass based on temperature but the problem of lock still   persists in keys belonging to same temperature.</li> <li>Reads and Writes: It uses hash to index the key to a memory location where its value is present.   For reads, it\u2019ll look up the key on the linked list at designated memory location and update the key\u2019s   position to head. For writes, if the memory location is free, it\u2019ll create a new pointer and a slab class   is assigned otherwise it\u2019ll handle Collision using chaining.  If the chain becomes too large to impact    read, Memcached resizes the hash and shifts everything to flattens the structure.</li> </ul>"},{"location":"Courses/fode/nosql/#redis-architecture","title":"Redis Architecture","text":"<p>Redis is an in-memory data structure store that supports caching, message queuing, real-time analytics, and more. Its architecture is more feature-rich than Memcached while still being extremely fast.</p> <p>It uses a single threaded event loop model for all its operation. To allow processing of multiple clients in  parallel, it uses I/O multiplexer (epoll/kqueue/select) over the single thread. </p> <p>One of the biggest difference b/w Memcached is its built in support for advanced data structures like Streams, Bitmaps, HyperLogLogs and more. All its primary data lives in RAM which allows faster read and writes, but it also supports durability optionally. You can persist data to disk in two ways:</p> <ul> <li>Journaling using an append only log (AOL) file, where logs are added for every insert/update.    These logs can be later replayed to restore the data of Redis upto the latest state of logs.   This requires another thread to append this logs.</li> <li>Snapshots, where data is flushed to disk periodically. This could risk data loss but the process is    much faster and the backup file is much compact and smaller.</li> </ul> <p>For communication, it uses its own wire protocol (known as RESP) build on top of TCP request/response model.</p> <p>Other popular features supported by Redis includes:</p> <ul> <li>built-in publish\u2013subscribe messaging system, where clients can publish messages to channels and subscribers   to these channels would receive them in real time.</li> <li>supports built-in distributed clustering, across different models like </li> <li>Sharding where Redis Cluster is partitioned across multiple nodes using hash slots.</li> <li>Leader-Follower async replication</li> <li>supports module to extend custom features like RedisBloom for supporting BloomFilters.</li> </ul>"},{"location":"Courses/fode/replication/","title":"Database Replication","text":""},{"location":"Courses/fode/replication/#database-replication","title":"Database Replication","text":"<p>Replication in DB system involves sharing data between redundant database instances in order to improve accessibility, reliability and fault tolerance. There are different kind of architecture for this</p> <ol> <li>Master/Standby Replication (Leader/Follower): Single database instance (node) will take all write operations    and distribute to Standby nodes. Standby nodes are only used for read operations. The consistency of reads for    standby nodes depends upon how long it take to propagate writes from master to standby, essentially ranging     the consistency from synchronous (strong) consistency to asynchronous (eventual) consistency.</li> <li>Multi-Master Replication: Similarly, to improve write you can have multiple master nodes. But this    strategy is more complicated and needs to handle conflicts when writing. </li> </ol> <p>The mode of replication can be synchronous or asynchronous depending on how data propagation is handled. Synchronous replication makes client wait till the transaction is completed on master as well as standby nodes. Cassandra uses this mode where the client accepts writes as successful until quorum (n+1/2) nodes have replicated the data. Asynchronous replication shows writes as successful write after it's committed on master. The replication is handed over to some backend process which moves data to standby nodes periodically.</p> <p>Advantage of using replication is you get horizontal scaling, and you can split standby database into  regions so that queries closer to certain group of user can use their region specific database instance.</p> <p>Disadvantage are your system might not like eventual consistency model and when going for strong  consistency the writes are slower. And if you want to go multi-master architecture, it's much more complex.</p>"},{"location":"DSA/","title":"DSA","text":""},{"location":"DSA/#dsa","title":"DSA","text":"<ul> <li> Neetcode 150</li> </ul>"},{"location":"DSA/neetcode150/","title":"Neetcode 150","text":""},{"location":"DSA/neetcode150/#neetcode-150","title":"Neetcode 150","text":"<p>Explanation and solutions to list of Leetcode problems created by Neetcode. It consists of 150 DSA problems, categorized into patterns derived from the blind 75 list. People already familiar with basic algorithms &amp; data structures can use this list for a quick review for technical interviews.</p>"},{"location":"DSA/neetcode150/Arrays_and_Hashing/contains_duplicate/","title":"\ud83d\udfe2 217. Contains Duplicate","text":"","tags":["Hash Table","Array","LC_Easy","Neetcode150"]},{"location":"DSA/neetcode150/Arrays_and_Hashing/contains_duplicate/#217-contains-duplicate","title":"217. Contains Duplicate","text":"<p>Problem Link</p> <p>Brute Force: You can iterate twice over the Array to check if element from outer iteration is present using inner  iteration. This would yield following runtime complexity: Time -&gt; \\(O(n^2)\\), Space -&gt; \\(O(1)\\)</p> <p>We can optimize the operation of checking if an element is present from \\(O(n)\\) to \\(O(1)\\) using a data structure like  HashSet which gives us constant time querying operation. The tradeoff being increase in space complexity to \\(O(n)\\), which majority of time is acceptable as memory isn't as scare as old days.</p> Pseudocode <ul> <li>Iterate over the \\(nums\\) array. For each \\(num\\),</li> <li>check if the \\(num\\) is present in our hashset. <ul> <li>If yes, we can return immediately</li> <li>Else, store current \\(num\\) in our hashset and continue. </li> </ul> </li> </ul> Runtime Complexity <p>Time: \\(O(n)\\), since we're only iterating the nums array once and querying hashset, an \\(O(1)\\) operation.</p> <p>Space: \\(O(n)\\), due to the hashset used for storing num.</p> PythonGo <pre><code>class Solution:\n    def containsDuplicate(self, nums: List[int]) -&gt; bool:\n        visited = set()\n        for num in nums:\n            if num in visited:\n                return True\n            visited.add(num)\n        return False\n</code></pre> <pre><code>func containsDuplicate(nums []int) bool {\n    set := make(map[int]interface{})\n    for _, num := range nums {\n        _, ok := set[num]\n        if ok {\n            return true\n        }\n        set[num] = nil\n    }\n    return false\n}\n</code></pre>","tags":["Hash Table","Array","LC_Easy","Neetcode150"]},{"location":"DSA/neetcode150/Arrays_and_Hashing/encode_and_decode_strings/","title":"\ud83d\udfe0 271. Encode and Decode Strings","text":"","tags":["Hash Table","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/Arrays_and_Hashing/encode_and_decode_strings/#271-encode-and-decode-strings","title":"271. Encode and Decode Strings","text":"<p>Problem Link</p> <p>One of the most common pattern for encoding any data structure into stream of continuous data type is to use the size of data structure to denote the amount of data to read from stream for parsing single unit of data. But, since our data might also include numbers there would be no way to differentiate the size data from data structure value. To solve this, you can simply use a placeholder value between two of these values. This would lead us to create streams as follows -&gt; <code>&lt;size&gt;&lt;placeholder&gt;&lt;data&gt;...</code></p> Pseudocode <p>Encode: Since our data structure is simply list of strings, we can</p> <ul> <li>Calculate the size of each string</li> <li>Generate the encoded token for this string -&gt; <code>&lt;size&gt;&lt;placeholder&gt;&lt;string&gt;</code></li> <li>Join all the token strings to get encoded data.</li> </ul> <p>Decode: You can use two pointers to indicate the start and current position in stream.</p> <ul> <li>We need to parse the stream until we reach end of it</li> <li>Start by parsing the size of next data token, by reading data until you encounter the placeholder value.</li> <li>Next decode the size into a number and read next <code>size</code> amount of data (skipping the placeholder).</li> <li>Update the start and current pointer to end of current token and continue.</li> </ul> Runtime Complexity <p>Time: \\(O(n)\\) &lt;- encode, \\(O(n)\\) &lt;- decode</p> <p>Space: \\(O(n)\\) &lt;- encode (from storing tokens/output), \\(O(n)\\) &lt;- decode </p> PythonGo <pre><code>class Solution:\n    DELIMITER = \"#\"\n    def encode(self, strs: List[str]) -&gt; str:\n        tokens = []\n        for s in strs:\n            token = f\"{len(s)}{self.DELIMITER}{s}\"\n            tokens.append(token)\n\n        return \"\".join(tokens)\n\n    def decode(self, s: str) -&gt; List[str]:\n        start = curr = 0\n        strs = []\n        while curr &lt; len(s):\n            while s[curr] != self.DELIMITER:\n                curr+=1\n\n            size = int(s[start:curr])\n            strs.append(s[curr+1: curr+size+1])\n            start = curr+size+1\n            curr = curr+size+1\n\n        return strs\n</code></pre> <pre><code>type Solution struct{}\n\nfunc (s *Solution) Encode(strs []string) string {\n    var tokens []string\n    for _, str := range strs {\n        tokens = append(tokens, fmt.Sprintf(\"%d#%s\", len(str), str))\n    }\n    return strings.Join(tokens, \"\")\n}\n\nfunc (s *Solution) Decode(str string) []string {\n    var strs []string\n    var start, curr int\n    for curr &lt; utf8.RuneCountInString(str) {\n        for str[curr] != '#' {\n            curr++\n        }\n        size, _ := strconv.Atoi(str[start:curr])\n        strs = append(strs, str[curr+1:curr+1+size])\n        start = curr + 1 + size\n        curr = start\n    }\n    return strs\n}\n</code></pre>","tags":["Hash Table","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/Arrays_and_Hashing/group_anagrams/","title":"\ud83d\udfe0 49. Group Anagrams","text":"","tags":["Hash Table","Array","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/Arrays_and_Hashing/group_anagrams/#49-group-anagrams","title":"49. Group Anagrams","text":"<p>Problem Link</p> <p>You can check this problem on how to find if two strings are anagrams.</p> <p>If we map a string into an Array of size 26, where each element represents the occurrence a character in a string. The associated character at any index in the Array is found by adding the ASCII code of 'a' to the index -&gt; \\(index+ascii(a)\\). And we're using size 26, because the string only have lowercase english characters which can be mapped to 26 positions in our array.</p> <p>You can think of the generated Array as a kind of bitmap and strings which are anagram would've same bitmap.  We can use this information to generate the group of Anagram string. </p> Pseudocode <ul> <li>Iterate over each string. During each iteration, </li> <li>Generate the bitmap of the string.</li> <li>Store the string in a Hashmap where key is bitmap and value is list of string with same bitmap.</li> <li>Finally returns the values of Hashmap, which would be grouped Anagram strings.</li> </ul> Runtime Complexity <p><code>strs</code> -&gt; length \\(n\\), with string of maximum size \\(k\\). </p> <p>Time: \\(O(nk)\\) &lt;- \\(O(n)\\) from iterating <code>strs</code>, \\(O(k)\\) for generating bitmap of each string.</p> <p>Space: \\(O(n)\\) &lt;- from Hashmap for storing the groups.</p> PythonGo <pre><code>class Solution:\n    def _get_bitmap(self, s: str) -&gt; tuple:\n        l = [0] * 26\n        for ch in s:\n            l[ord(ch) - ord('a')] += 1\n        return tuple(l)\n\n    def groupAnagrams(self, strs: List[str]) -&gt; List[List[str]]:\n        freq = {}  # bitmap -&gt; str\n\n        for s in strs:\n            bitmap = self._get_bitmap(s)\n            if bitmap in freq:\n                freq[bitmap].append(s)\n            else:\n                freq[bitmap] = [s]\n        return list(freq.values())\n</code></pre> <pre><code>func getBitmap(s string) [26]int {\n    var bitmap [26]int\n    for _, ch := range s {\n        bitmap[ch-'a']++\n    }\n    return bitmap\n}\n\nfunc groupAnagrams(strs []string) [][]string {\n    freq := make(map[[26]int][]string)\n    for _, str := range strs {\n        bitmap := getBitmap(str)\n        freq[bitmap] = append(freq[bitmap], str)\n    }\n\n    var groups [][]string\n    for _, group := range freq {\n        groups = append(groups, group)\n    }\n    return groups\n}\n</code></pre> Implementation Note <p>Python: We're using tuple as key, because they're hashable object. If you use strings as key, consider the number of digits in counts when generating the string key.</p> <p>Go: keys to map must be <code>comparable</code> types. Slices (<code>[]int</code>) aren't comparablable but Arrays (<code>[N]T</code>) are comparable if their element Type (<code>T</code>) is comparable. </p>","tags":["Hash Table","Array","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/Arrays_and_Hashing/longest_consecutive_sequence/","title":"\ud83d\udfe0 128. Longest Consecutive Sequence","text":"","tags":["Array","Hash Table","Union Find","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/Arrays_and_Hashing/longest_consecutive_sequence/#128-longest-consecutive-sequence","title":"128. Longest Consecutive Sequence","text":"<p>Problem Link</p> <p>Brute Force: Since we only need to find the longest consecutive sequence irrespective of order in array, we can check the sequence starting each element and take the longest. This would result in \\(O(n^2)\\) time  complexity.</p> <p>To optimize this, we can just check the sequence for \\(num_i\\) whose previous number (\\(num_i-1\\)) isn't present in our  array, as these number are guaranteed to generate unique sequence . This would result in checking the sequence for elements only once. </p> Pseudocode <ul> <li>Iterate over the nums array. To keep checks constant time, we can also create a Hashset out of <code>nums</code> array.</li> <li>For each <code>num</code>, if the previous (<code>num-1</code>) isn't present in our array means we've a unique sequence starting    from this <code>num</code>.</li> <li>To generate this sequence, declare a length pointer and increment it until we exhaust the sequence numbers   present in our hashset.</li> <li>Finally use a global variable to maintain the maximum length.</li> </ul> Runtime Complexity <p>Time: \\(O(n)\\), single iteration over each element with constant time checks. It might look like we're iterating within 2 loops, but the conditional will reduce the inner iteration such that we're not repeating checks.</p> <p>Space: \\(O(n)\\) &lt;- from hashmap.</p> PythonGo <pre><code>class Solution:\n    def longestConsecutive(self, nums: List[int]) -&gt; int:\n        res = 0\n        nums = set(nums)\n        for num in nums:\n            if num-1 not in nums:\n                length = 1\n                while num+length in nums:\n                    length+=1\n                res = max(length, res)\n        return res\n</code></pre> <pre><code>func longestConsecutive(nums []int) int {\n    set := make(map[int]interface{})\n    for _, num := range nums {\n        set[num] = nil\n    }\n    res := 0\n    for num, _ := range set {\n        if _, ok := set[num-1]; !ok {\n            length := 1\n            for {\n                if _, ok := set[num+length]; ok {\n                    length++\n                } else {\n                    break\n                }\n            }\n            res = max(res, length)\n        }\n    }\n    return res\n}\n</code></pre>","tags":["Array","Hash Table","Union Find","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/Arrays_and_Hashing/product_of_array_except_self/","title":"\ud83d\udfe0 238. Product of Array Except Self","text":"","tags":["Array","Prefix Sum","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/Arrays_and_Hashing/product_of_array_except_self/#238-product-of-array-except-self","title":"238. Product of Array Except Self","text":"<p>Problem Link</p> <p>The catch of problem is we can't use division operator. If that wasn't the case, we could simply have aggregated multiplication of whole array and divide each element by it get their own product without self.</p> <p>When translated to equations, this would look like: \\(\\prod_{i=1}^{n} nums_i \\over nums_i\\). By removing the division  -&gt; \\(\\prod_{i=1}^{i-1} nums_i * \\prod_{i=i+1}^{n} nums_i\\), which is basically product of prefix and postfix array to index <code>i</code>.</p> <p>One way to implement this is to store prefix and postfix multiplication in two separate arrays and combine them to generate the result. </p> <p>Follow Up</p> <p>Instead of saving the prefix and postfix multiplication within separate array, we can simply store them within output array (since multiplication is communicative) </p> Pseudocode <ul> <li>To generate prefix, iterate from start of <code>nums</code> and using a variable to aggregate the multiplication over   the iteration.</li> <li>To generate postfix, iterate from end of <code>nums</code> and repeat same.</li> </ul> Runtime Complexity <p>Time: \\(O(n)\\)</p> <p>Space: \\(O(1)\\) &lt;- output isn't considered</p> PythonGo <pre><code>class Solution:\n    def productExceptSelf(self, nums: List[int]) -&gt; List[int]:\n        result = []\n        pre, post = 1, 1\n\n        for i in range(len(nums)):\n            result.append(pre)\n            pre *= nums[i]\n\n        for i in reversed(range(len(nums))):\n            result[i] *= post\n            post *= nums[i]\n        print(result)\n        return result\n</code></pre> <pre><code>func productExceptSelf(nums []int) []int {\n    result := make([]int, len(nums))\n    var pre, post int = 1, 1\n    for idx := range nums {\n        result[idx] = pre\n        pre *= nums[idx]\n    }\n\n    for idx := len(nums) - 1; idx &gt;= 0; idx-- {\n        result[idx] *= post\n        post *= nums[idx]\n    }\n    return result\n}\n</code></pre>","tags":["Array","Prefix Sum","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/Arrays_and_Hashing/top_k_frequent_element/","title":"\ud83d\udfe0 347. Top K Frequent Elements","text":"","tags":["Hash Table","Array","Heap/Priority Queue","LC_Medium","Neetcode150","Bucket Sort"]},{"location":"DSA/neetcode150/Arrays_and_Hashing/top_k_frequent_element/#347-top-k-frequent-elements","title":"347. Top K Frequent Elements","text":"<p>Problem Link</p> <p>Heap</p> <p>This is a general problem designed to be solved efficiently using heap data structure. For now, we'll solve it using hashmaps.</p> <p>The general idea to solve this is by generating a hashmap where the value is list of numbers and key is the frequency of those number in <code>nums</code>. Now, you can use this hashmap to generate list of nums sorted in order of their frequency in <code>nums</code>. But for this problem, we only want first \\(k\\) elements from this list.</p> Pseudocode <ul> <li>Generate hashmap where key is \\(num\\) and value is frequency of that \\(num\\) in <code>nums</code>.</li> <li>Using above hashmap, generate another hashmap where key is a number and value is the list   of numbers having \\(key\\) frequency in <code>nums</code>.</li> <li>Generate result having numbers sorted in order of highest frequency.    Since the highest frequency could be <code>len(nums)</code> while smallest -&gt; \\(0\\).<ul> <li>Iterate from the highest frequency to the smallest while adding numbers of   frequencies found in our hashmap. </li> </ul> </li> </ul> Runtime Complexity <p>Time: \\(O(n)\\), from iterating <code>nums</code>.</p> <p>Space: \\(O(n)\\) &lt;- from hashmaps.</p> PythonGo <pre><code>class Solution:\n    def topKFrequent(self, nums: List[int], k: int) -&gt; List[int]:\n        freq = Counter(nums)\n        buckets = defaultdict(list)\n        for num, count in freq.items():\n            buckets[count].append(num)\n\n        result = []\n        for count in range(len(nums), 0, -1):\n            if count in buckets:\n                result.extend(buckets[count])\n\n        return result[:k]\n</code></pre> <pre><code>func topKFrequent(nums []int, k int) []int {\n    freq := make(map[int]int)\n    for _, num := range nums {\n        freq[num]++\n    }\n    buckets := make(map[int][]int)\n    for num, count := range freq {\n        buckets[count] = append(buckets[count], num)\n    }\n\n    var result []int\n    for i := len(nums); i &gt;= 0; i-- {\n        vals, ok := buckets[i]\n        if ok {\n            result = append(result, vals...)\n        }\n    }\n    return result[:k]\n}\n</code></pre>","tags":["Hash Table","Array","Heap/Priority Queue","LC_Medium","Neetcode150","Bucket Sort"]},{"location":"DSA/neetcode150/Arrays_and_Hashing/two_sum/","title":"\ud83d\udfe2 1. Two Sum","text":"","tags":["Hash Table","Array","LC_Easy","Neetcode150"]},{"location":"DSA/neetcode150/Arrays_and_Hashing/two_sum/#1-two-sum","title":"1. Two Sum","text":"<p>Problem Link</p> <p>You can think of the problem as,  given a number \\(num\\) find the index of \\(target-num\\).</p> <p>This can be done by storing the index of each \\(num\\) in a data structure like Hashmap which can be queried to fetch index of a value in a constant time.</p> Pseudocode <ul> <li>Iterate over the \\(nums\\) array. For each \\(num\\),</li> <li>check if the \\(target-num\\) is present in our map.<ul> <li>If yes, we can directly return the current index and saved index from here</li> <li>Else, store the index of current num in our map and continue. </li> </ul> </li> </ul> Runtime Complexity <p>Time: \\(O(n)\\), since we're only iterating the nums array once and querying hashmap is an \\(O(1)\\) operation.</p> <p>Space: \\(O(n)\\), due to the map used for storing num -&gt; index mapping</p> PythonGo <pre><code>class Solution:\n    def twoSum(self, nums: List[int], target: int) -&gt; List[int]:\n        s = dict()\n        for i, num in enumerate(nums):\n            if num in s:\n                return [i, s[num]]\n            s[target - num] = i\n        return [-1, -1]\n</code></pre> <pre><code>func twoSum(nums []int, target int) []int {\n    idxMap := make(map[int]int)\n    for idx, num := range nums {\n        if val, ok := idxMap[num]; ok {\n            return []int{val, idx}\n        }\n        idxMap[target-num] = idx\n    }\n    return []int{}\n}\n</code></pre>","tags":["Hash Table","Array","LC_Easy","Neetcode150"]},{"location":"DSA/neetcode150/Arrays_and_Hashing/valid_anagram/","title":"\ud83d\udfe2 242. Valid Anagram","text":"","tags":["Hash Table","LC_Easy","Neetcode150"]},{"location":"DSA/neetcode150/Arrays_and_Hashing/valid_anagram/#242-valid-anagram","title":"242. Valid Anagram","text":"<p>Problem Link</p> <p>Given strings <code>s</code> and <code>t</code>, they're Anagram if we can rearrange the characters of one to form another. This can be reworded as, both strings should have same characters. We can check this by storing the frequency of one string and match them with the other.  </p> Pseudocode <ul> <li>If strings have different number of characters, we can return early as they'll never have same characters.</li> <li>Otherwise, we'll iterate over one string and store the frequency of each character in a hashmap.</li> <li>Iterate over other string to reduce the frequency of encountered character.</li> <li>During which, we can say the strings aren\u2019t anagrams if we encounter any character which either isn't present              in our hashmap, or the frequency of character is exhausted.</li> </ul> Runtime Complexity <p>Time: \\(O(n)\\), since we're only iterating the strings twice, with insertion and querying hashmap being \\(O(1)\\)             operation.</p> <p>Space: \\(O(n)\\), due to the hashmap used for storing characters frequency.</p> PythonGo <pre><code>class Solution:\n    def isAnagram(self, s: str, t: str) -&gt; bool:\n        if len(s) != len(t):\n            return False\n\n        freq = dict()\n        for i in s:\n            freq[i] = freq.get(i, 0) + 1\n\n        for i in t:\n            if i not in freq or freq[i] == 0:\n                return False\n\n            freq[i] -= 1\n\n        return True\n</code></pre> <pre><code>func isAnagram(s string, t string) bool {\n    if len(s) != len(t) {\n        return false\n    }\n    freq := make(map[rune]int)\n    for _, ch := range s {\n        freq[ch]++\n    }\n\n    for _, ch := range t {\n        val, ok := freq[ch]\n        if !ok || val == 0 {\n            return false\n        }\n        freq[ch]--\n    }\n    return true\n}\n</code></pre>","tags":["Hash Table","LC_Easy","Neetcode150"]},{"location":"DSA/neetcode150/Arrays_and_Hashing/valid_sudoku/","title":"\ud83d\udfe0 36. Valid Sudoku","text":"","tags":["Array","Hash Table","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/Arrays_and_Hashing/valid_sudoku/#36-valid-sudoku","title":"36. Valid Sudoku","text":"<p>Problem Link</p> <p>To check if a sudo board is valid, you've to satisfy 3 types of criteria: - each row shouldn't have repeating digit. - each column shouldn't have repeating digit. - each box (3x3 here) shouldn't have a repeating digit.</p> <p>This can be checked pretty easily using hash maps (similar to contains_dupliacte).</p> How would you map cells within a box to same key? <p>\\(key=(floor(row/3))*3 + (floor(col/3))\\)</p> Pseudocode <ul> <li>Iterate over the board using two loops. Within each iteration, if the cell isn't <code>.</code></li> <li>check if the value is present in your hashmap buckets.<ul> <li>if present, we can say the sudoku is invalid and return.</li> <li>else add the values to respective hashmap key.</li> </ul> </li> </ul> Runtime Complexity <p>number of elements in board -&gt; n</p> <p>Time: \\(O(n)\\), single iteration over each element with constant time checks.</p> <p>Space: \\(O(n)\\) &lt;- from hashmaps.</p> PythonGo <pre><code>class Solution:\n    def isValidSudoku(self, board: List[List[str]]) -&gt; bool:\n        rows = defaultdict(set)\n        cols = defaultdict(set)\n        box = defaultdict(set)\n        for r in range(len(board)):\n            for c in range(len(board[0])):\n                cell = board[r][c]\n                b = (r // 3) * 3 + c // 3\n                if cell == \".\":\n                    continue\n                elif cell in rows[r] or cell in cols[c] or cell in box[b]:\n                    return False\n\n                rows[r].add(cell)\n                cols[c].add(cell)\n                box[b].add(cell)\n        return True\n</code></pre> <pre><code>func isValidSudoku(board [][]byte) bool {\n    var rows, cols, boxes [9][9]bool\n\n    for r := range board {\n        for c := range board[0] {\n            cell := board[r][c]\n            if cell != '.' {\n                b := (r/3)*3 + (c / 3)\n                idx := int(cell - '1')\n                if rows[r][idx] || cols[c][idx] || boxes[b][idx] {\n                    return false\n                }\n                rows[r][idx] = true\n                cols[c][idx] = true\n                boxes[b][idx] = true\n            }\n        }\n    }\n    return true\n}\n</code></pre>","tags":["Array","Hash Table","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/Two_Pointers/3sum/","title":"\ud83d\udfe0 15. 3Sum","text":"","tags":["Two Pointers","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/Two_Pointers/3sum/#15-3sum","title":"15. 3Sum","text":"<p>Problem Link</p> <p>This problem is similar to Two Sum, we can just use an outer iteration to reduce it to a two sum. But core part would be, how you'd avoid duplicate triplets in result?</p> <p>One approach could be by sorting the array, and since duplicate values would be adjacent we could directly skip them during each iteration. Also, Sorting Array is \\(O(nlogn)\\) operation, which wouldn't impact the runtime of our \\(O(n^2)\\) solution.  </p> Pseudocode <ul> <li>Sort the input Array and start an outer loop as an indicative of first number in triplet.</li> <li>Skip the number if it's same as previous number, as we've already solved for it and don't want duplicate.</li> <li>Within inner loop since the array is sorted, you can use similar approach as Two Sum 2.</li> </ul> Runtime Complexity <p>Time: \\(O(n^2)\\), from two loops.</p> <p>Space: \\(O(1)\\)/\\(O(n)\\), depending on sorting algorithm</p> PythonGo <pre><code>class Solution:\n    def threeSum(self, nums: List[int]) -&gt; List[List[int]]:\n        result = []\n        nums.sort()\n        for i in range(len(nums)):\n            if i&gt;0 and nums[i] == nums[i-1]:\n                continue\n            j, k = i+1, len(nums)-1\n            while j &lt; k:\n                currSum = nums[i]+nums[j]+nums[k]\n                if currSum &gt; 0:\n                    k-=1\n                elif currSum &lt; 0:\n                    j+=1\n                else:\n                    result.append([nums[i], nums[j], nums[k]])\n                    j+=1\n                    while j &lt; k and nums[j] == nums[j-1]:\n                        j+=1\n        return result\n</code></pre> <pre><code>func threeSum(nums []int) [][]int {\n    sort.Ints(nums)\n    var result [][]int\n    for i := range nums {\n        if i &gt; 0 &amp;&amp; nums[i] == nums[i-1] {\n            continue\n        }\n        j, k := i+1, len(nums)-1\n        for j &lt; k {\n            currSum := nums[i] + nums[j] + nums[k]\n            if currSum &gt; 0 {\n                k--\n            } else if currSum &lt; 0 {\n                j++\n            } else {\n                result = append(result, []int{nums[i], nums[j], nums[k]})\n                j++\n                for j &lt; k &amp;&amp; nums[j] == nums[j-1] {\n                    j++\n                }\n            }\n        }\n    }\n    return result\n}\n</code></pre>","tags":["Two Pointers","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/Two_Pointers/container_with_most_water/","title":"\ud83d\udfe0 11. Container With Most Water","text":"","tags":["Two Pointers","Greedy","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/Two_Pointers/container_with_most_water/#11-container-with-most-water","title":"11. Container With Most Water","text":"<p>Problem Link</p> <p>Area of container would be determined by two factors,  - width  - minimum height on either edges.</p> <p>We can use two pointers starting from the largest width available. To iterate over to next candidate, we can remove edges from either side. But we can greedily remove the shorter edge, as  this would be always the limiting factor in area calculation -&gt; \\((r-l)*min(height[l],height[r])\\). Also, the solution from shorter edges has been already considered in current iteration, so we don't have to worry about missing the current area in result.</p> Pseudocode <ul> <li>Iterate over the array using two pointers, left and right.</li> <li>During each iteration, calculate the area covered and update the global result if we've found larger value.</li> <li>Later we can just shift the left or right pointers to move onto next candidate to find global maxima.</li> </ul> Runtime Complexity <p>Time: \\(O(n)\\), since we're only iterating the nums array once.</p> <p>Space: \\(O(1)\\), constant space from pointer variables.</p> PythonGo <pre><code>class Solution:\n    def maxArea(self, height: List[int]) -&gt; int:\n        result = 0\n        l, r = 0, len(height)-1\n        while l &lt; r:\n            curr = (r-l)*min(height[l], height[r])\n            result = max(result, curr)\n            if height[r] &gt; height[l]:\n                l+=1\n            else:\n                r-=1\n        return result\n</code></pre> <pre><code>func maxArea(height []int) int {\n    l, r := 0, len(height)-1\n    var result int\n    for l &lt; r {\n        curr := (r - l) * min(height[l], height[r])\n        result = max(result, curr)\n        if height[r] &gt; height[l] {\n            l++\n        } else {\n            r--\n        }\n    }\n    return result\n}\n</code></pre>","tags":["Two Pointers","Greedy","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/Two_Pointers/trapping_rain_water/","title":"\ud83d\udd34 42. Trapping Rain Water","text":"","tags":["Two Pointers","LC_Hard","Neetcode150"]},{"location":"DSA/neetcode150/Two_Pointers/trapping_rain_water/#42-trapping-rain-water","title":"42. Trapping Rain Water","text":"<p>Problem Link</p> <p>Think of the water amount as sum of water collected at each index. How would you calculate the trappable water at any index? This water level is bounded by the minimum of maximum heights on either side of given index.</p> <p></p> <p>With this, you can create such prefix arrays for maximum left and right heights of respective index and finally calculate the water trapped at each level. This would give us \\(O(n)\\) time and space complexity.</p> <p>We can optimize this to use \\(O(1)\\) space by using a two pointers technique. For this, we'll need to initialize two pointers at left and right ends, and two variables storing the maximum left and right heights of among iterated values. For example, with <code>l</code> and <code>r</code> index pointers,  lets say maxL &lt; maxR. For this, we can say for sure that the water collected for l cell is bounded by maxL. Because any consecutive height on right of <code>l</code> would be equal or greater than maxR which wouldn't impact our computation of water -&gt; \\(min(maxL, maxR)-height[i]\\).</p> Pseudocode <ul> <li>Declare two pointers for indicating current left and right position in iteration.</li> <li>Declare two variables to store the maximum left and right heights.</li> <li>Iterate over the array until we've computed water level for all cells i.e \\(l&lt;=r\\).</li> <li>For each iteration,<ul> <li>if \\(maxL &lt; maxR\\), we'll compute water level for <code>l</code> and increment the counter to next index</li> <li>else, we'll computer water level for <code>r</code> and decrement the counter to next index.</li> </ul> </li> </ul> Runtime Complexity <p>Time: \\(O(n)\\), since we're only iterating the nums array once.</p> <p>Space: \\(O(1)\\), constant time from two pointers.</p> PythonGo <pre><code>class Solution:\n    def trap(self, height: List[int]) -&gt; int:\n        maxL, maxR = height[0], height[-1]\n        l, r = 0, len(height)-1\n        result = 0\n        while l &lt;= r:\n            if height[l] &lt; height[r]:\n                maxL = max(maxL, height[l])\n                result += maxL-height[l]\n                l+=1\n            else:\n                maxR = max(maxR, height[r])\n                result += maxR - height[r]\n                r-=1\n        return result\n</code></pre> <pre><code>func trap(height []int) int {\n    l, r := 0, len(height)-1\n    maxL, maxR := height[l], height[r]\n    var result int\n    for l &lt;= r {\n        if maxL &lt; maxR {\n            maxL = max(maxL, height[l])\n            result += maxL - height[l]\n            l++\n        } else {\n            maxR = max(maxR, height[r])\n            result += maxR - height[r]\n            r--\n        }\n    }\n    return result\n}\n</code></pre>","tags":["Two Pointers","LC_Hard","Neetcode150"]},{"location":"DSA/neetcode150/Two_Pointers/two_sum_2/","title":"\ud83d\udfe0 167. Two Sum II - Input Array Is Sorted","text":"","tags":["Two Pointers","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/Two_Pointers/two_sum_2/#167-two-sum-ii-input-array-is-sorted","title":"167. Two Sum II - Input Array Is Sorted","text":"<p>Problem Link</p> <p>Similar to problem TwoSum, except the <code>nums</code> array is sorted now.  We can greedily use this information by comparing sum of left and right end. If our sum exceed target, we should reduce it by decreasing the right pointer. Else if our sum is smaller than target, we can increase our current sum by increasing left pointer.</p> Runtime Complexity <p>Time: \\(O(n)\\), since we're only iterating the nums array once.</p> <p>Space: \\(O(1)\\), constant space from two pointers.</p> PythonGo <pre><code>class Solution:\n    def twoSum(self, numbers: List[int], target: int) -&gt; List[int]:\n        l, r = 0, len(numbers)-1\n        while l &lt; r:\n            curr = numbers[l] + numbers[r]\n            if curr &gt; target:\n                r-=1\n            elif curr &lt; target:\n                l+=1\n            else:\n                return [l+1, r+1]\n        return [-1,-1]\n</code></pre> <pre><code>func twoSum2(numbers []int, target int) []int {\n    l, r := 0, len(numbers)-1\n    for l &lt; r {\n        curr := numbers[l] + numbers[r]\n        if curr &gt; target {\n            r--\n        } else if curr &lt; target {\n            l++\n        } else {\n            return []int{l + 1, r + 1}\n        }\n    }\n    return nil\n}\n</code></pre>","tags":["Two Pointers","LC_Medium","Neetcode150"]},{"location":"DSA/neetcode150/Two_Pointers/valid_palindrome/","title":"\ud83d\udfe2 125. Valid Palindrome","text":"","tags":["Two Pointers","LC_Easy","Neetcode150"]},{"location":"DSA/neetcode150/Two_Pointers/valid_palindrome/#125-valid-palindrome","title":"125. Valid Palindrome","text":"<p>Problem Link</p> <p>Check Palindrome by comparing starting and ending characters in string. You can easily do this by  taking two pointers, pointing the respective position from start and end which should be same to form palindrome. </p> Pseudocode <ul> <li>Initialize start and end pointers</li> <li>Iterate over the string using the pointers until they meet or cross each other over bound.</li> </ul> Runtime Complexity <p>Time: \\(O(n)\\), single iteration over string.</p> <p>Space: \\(O(1)\\), constant space by two pointers.</p> PythonGo <pre><code>class Solution:\n    def isPalindrome(self, s: str) -&gt; bool:\n        l, r = 0, len(s)-1\n        while l &lt; r:\n            if not s[l].isalnum():\n                l+=1\n            elif not s[r].isalnum():\n                r-=1\n            elif s[l].lower() != s[r].lower():\n                return False\n            else:\n                l+=1\n                r-=1\n        return True\n</code></pre> <pre><code>func isPalindrome(s string) bool {\n    l, r := 0, len(s)-1\n    for l &lt; r {\n        for l &lt; r &amp;&amp; !unicode.IsDigit(rune(s[l])) &amp;&amp; !unicode.IsLetter(rune(s[l])) {\n            l++\n        }\n        for l &lt; r &amp;&amp; !unicode.IsDigit(rune(s[r])) &amp;&amp; !unicode.IsLetter(rune(s[r])) {\n            r--\n        }\n        if unicode.ToLower(rune(s[l])) != unicode.ToLower(rune(s[r])) {\n            return false\n        }\n        l++\n        r--\n    }\n    return true\n}\n</code></pre> Go Implementation note <p>Strings in Go are sequence of bytes , so when you access any particular index - it'll return the respective byte. While characters are represented using <code>rune</code> which represents a Unicode code point. Byte occupies 1 byte size while Rune can use upto 4 bytes size, which is why converting a byte using <code>rune(.)</code> isn\u2019t always safe. It's safe for ASCII characters which fall within 1 byte range.</p>","tags":["Two Pointers","LC_Easy","Neetcode150"]}]}